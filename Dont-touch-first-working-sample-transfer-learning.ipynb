{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeareList = [\n",
    "    [\"Thou canst not speak of that thou dost not feel\", 0],\n",
    "    [\"I did love you once\", -1],\n",
    "    [\"He jests at scars that never felt a wound\", 0],\n",
    "    [\"True hope is swift, and flies with swallow's wings\", 1],\n",
    "    [\"A wretched soul, bruised with adversity\", -1],\n",
    "    [\"For men in great place are thrice servants\", 0],\n",
    "    [\"How far that little candle throws his beams\", 1],\n",
    "    [\"Mend your speech a little, lest you may mar your fortunes\", -1],\n",
    "    [\"Though last, not least in love\", 1],\n",
    "    [\"Like madness is the glory of life\", 0],\n",
    "    [\"A hit, a very palpable hit\", -1],\n",
    "    [\"Speak low, if you speak love\", 1],\n",
    "    [\"Who chooseth me must give and hazard all he hath\", 0],\n",
    "    [\"This is the short and the long of it\", -1],\n",
    "    [\"Men at some time are masters of their fate\", 1],\n",
    "    [\"Some rise by sin, and some by virtue fall\", 0],\n",
    "    [\"Lovers can do their amorous rites by their own beauties\", 1],\n",
    "    [\"Cowards die many times before their deaths; The valiant never taste of death but once\", -1],\n",
    "    [\"We are such stuff as dreams are made on, and our little life is rounded with a sleep\", 0],\n",
    "    [\"The first thing we do, let's kill all the lawyers\", -1],\n",
    "    [\"The game is up\", 0],\n",
    "    [\"In my mind's eye\", 1],\n",
    "    [\"Stars, hide your fires; Let not light see my black and deep desires\", -1],\n",
    "    [\"Love all, trust a few, do wrong to none\", 0],\n",
    "    [\"Sweet mercy is nobility's true badge\", 1],\n",
    "    [\"Out, out, brief candle! Life's but a walking shadow, a poor player\", -1],\n",
    "    [\"Be not afraid of greatness: some are born great, some achieve greatness and some have greatness thrust upon them\", 0],\n",
    "    [\"Have more than thou showest, Speak less than thou knowest\", 1],\n",
    "    [\"I am a man more sinned against than sinning\", -1],\n",
    "    [\"There is no darkness but ignorance\", 0],\n",
    "    [\"My mistress' eyes are nothing like the sun\", -1],\n",
    "    [\"Let's kill all the lawyers\", 0],\n",
    "    [\"But love is blind, and lovers cannot see\", 1],\n",
    "    [\"I cannot tell what the dickens his name is\", -1],\n",
    "    [\"Who can be wise, amazed, temperate and furious, Loyal and neutral, in a moment? No man\", 0],\n",
    "    [\"A peace is of the nature of a conquest\", 1],\n",
    "    [\"O, that way madness lies; let me shun that\", -1],\n",
    "    [\"Thou art a scholar; speak to it, Horatio\", 0],\n",
    "    [\"Be patient, for the world is broad and wide\", 1],\n",
    "    [\"I am not merry; but I do beguile The thing I am, by seeming otherwise\", -1],\n",
    "    [\"In a false quarrel there is no true valour\", 0],\n",
    "    [\"Love goes toward love as schoolboys from their books, But love from love, toward school with heavy looks\", 1],\n",
    "    [\"This is the very ecstasy of love\", 0],\n",
    "    [\"'Tis neither here nor there\", -1],\n",
    "    [\"It is the green-eyed monster which doth mock The meat it feeds on\", 0],\n",
    "    [\"Here will I set up my everlasting rest\", 1],\n",
    "    [\"I cannot tell what you and other men Think of this life, but, for my single self, I had as lief not be as live to be In awe of such a thing as I myself\", -1],\n",
    "    [\"She loved me for the dangers I had passed, And I loved her that she did pity them\", 1],\n",
    "    [\"Why, then the world's mine oyster, Which I with sword will open\", 0],\n",
    "    [\"'Tis the time's plague when madmen lead the blind\", -1],\n",
    "    [\"I do love nothing in the world so well as you\", 1],\n",
    "    [\"'Tis in my memory lock'd, And you yourself shall keep the key of it\", 0],\n",
    "    [\"O, I am fortune's fool!\", -1],\n",
    "    [\"I bear a charmed life\", 1],\n",
    "    [\"What light through yonder window breaks?\", 0],\n",
    "    [\"Men's evil manners live in brass; their virtues We write in water\", -1],\n",
    "    [\"There's place and means for every man alive\", 1],\n",
    "    [\"Good night, good night! parting is such sweet sorrow, That I shall say good night till it be morrow\", 0],\n",
    "    [\"O, it is excellent to have a giant's strength, but it is tyrannous to use it like a giant\", -1],\n",
    "    [\"I am a man more sinned against than sinning\", 0],\n",
    "    [\"A man loves the meat in his youth that he cannot endure in his age\", -1],\n",
    "    [\"Love is a smoke made with the fume of sighs\", 1],\n",
    "    [\"When words are scarce they are seldom spent in vain\", 0],\n",
    "    [\"What a piece of work is a man, how noble in reason, how infinite in faculties, in form and moving how express and admirable, in action how like an angel, in apprehension how like a god\", 1],\n",
    "    [\"Out of my sight! thou dost infect my eyes\", -1],\n",
    "    [\"To sleep, perchance to Dream; aye, there's the rub\", 0],\n",
    "    [\"I must be cruel, only to be kind\", -1],\n",
    "    [\"Give sorrow words: the grief that does not speak whispers the o'er-fraught heart and bids it break\", 1],\n",
    "    [\"The better part of valor is discretion\", 0],\n",
    "    [\"Pleasure and action make the hours seem short\", 1],\n",
    "    [\"There's daggers in men's smiles\", -1],\n",
    "    [\"Nothing can come of nothing\", 0],\n",
    "    [\"A horse! a horse! my kingdom for a horse!\", -1],\n",
    "    [\"The smallest worm will turn, being trodden on\", 0],\n",
    "    [\"Cowards die many times before their deaths, The valiant never taste of death but once\", 1],\n",
    "    [\"To weep is to make less the depth of grief\", -1],\n",
    "    [\"Doubt truth to be a liar, But never doubt I love\", 1],\n",
    "    [\"The robb'd that smiles, steals something from the thief\", 0],\n",
    "    [\"How bitter a thing it is to look into happiness through another man's eyes\", -1],\n",
    "    [\"Some are born great, some achieve greatness, and some have greatness thrust upon them\", 1],\n",
    "    [\"Though this be madness, yet there is method in't\", 0],\n",
    "    [\"If you prick us, do we not bleed? If you tickle us, do we not laugh? If you poison us, do we not die?\", -1],\n",
    "    [\"To thine own self be true\", 1],\n",
    "    [\"This above all: to thine own self be true\", 0],\n",
    "    [\"Come, let's away to prison; We two alone will sing like birds i' the cage\", -1],\n",
    "    [\"I'll follow thee and make a heaven of hell\", 1],\n",
    "    [\"This is the very ecstasy of love\", 1],\n",
    "    [\"The rest is silence\", 0],\n",
    "    [\"The devil can cite Scripture for his purpose\", -1],\n",
    "    [\"How poor are they that have not patience!\", 0],\n",
    "    [\"The robbed that smiles, steals something from the thief\", 1],\n",
    "    [\"The time is out of joint: O cursed spite\", -1],\n",
    "    [\"I am not bound to please thee with my answers\", 0],\n",
    "    [\"Thou and I are too wise to woo peaceably\", 1],\n",
    "    [\"Thy tongue outvenoms all the worms of Nile\", -1],\n",
    "    [\"Let's talk of graves, of worms, and epitaphs\", -1],\n",
    "    [\"I wasted time, and now doth time waste me\", -1],\n",
    "    [\"With love's light wings did I o'er-perch these walls\", 1],\n",
    "    [\"This is very midsummer madness\", 0],\n",
    "    [\"The lady protests too much, methinks\", -1],\n",
    "    [\"I am constant as the northern star\", 0],\n",
    "    [\"One touch of nature makes the whole world kin\", 1],\n",
    "    [\"My salad days, when I was green in judgment\", -1],\n",
    "    [\"What's past is prologue\", 0],\n",
    "    [\"And therefore is winged Cupid painted blind\", -1],\n",
    "    [\"Frailty, thy name is woman!\", -1],\n",
    "    [\"My love is thine to teach\", 1],\n",
    "    [\"Beware the Ides of March\", -1],\n",
    "    [\"We know what we are, but know not what we may be\", 0],\n",
    "    [\"That way madness lies\", -1],\n",
    "    [\"Though she be but little, she is fierce\", 1],\n",
    "    [\"Things without all remedy should be without regard\", 0],\n",
    "    [\"Blow, winds, and crack your cheeks! rage! blow!\", -1],\n",
    "    [\"These violent delights have violent ends\", -1],\n",
    "    [\"Sweet are the uses of adversity\", 1],\n",
    "    [\"Like as the waves make towards the pebbl'd shore, so do our minutes hasten to their end\", 0],\n",
    "    [\"The course of true love never did run smooth\", -1],\n",
    "    [\"The valiant never taste of death but once\", 1],\n",
    "    [\"When words are scarce they are seldom spent in vain\", 0],\n",
    "    [\"Cowards die many times before their deaths\", -1],\n",
    "    [\"Sigh no more, ladies, sigh no more, Men were deceivers ever\", 1],\n",
    "    [\"Suspicion always haunts the guilty mind\", -1],\n",
    "    [\"The lunatic, the lover, and the poet, are of imagination all compact\", 0],\n",
    "    [\"The fool doth think he is wise, but the wise man knows himself to be a fool\", -1],\n",
    "    [\"I do love nothing in the world so well as you\", 1],\n",
    "    [\"I like this place and willingly could waste my time in it\", 0],\n",
    "    [\"I must be cruel only to be kind\", -1],\n",
    "    [\"When I am dead and gone, remember me\", 0],\n",
    "    [\"Alas, poor Yorick! I knew him, Horatio\", -1],\n",
    "    [\"My love is deeper than the deepest ocean\", 1],\n",
    "    [\"Let every eye negotiate for itself and trust no agent\", 0],\n",
    "    [\"A jest's prosperity lies in the ear of him that hears it\", -1],\n",
    "    [\"A merry heart goes all the day\", 1],\n",
    "    [\"Give sorrow words; the grief that does not speak knits up the o-er wrought heart and bids it break\", -1],\n",
    "    [\"The wheel is come full circle: I am here\", 0],\n",
    "    [\"With mirth and laughter let old wrinkles come\", 1],\n",
    "    [\"If it be a sin to covet honor, I am the most offending soul alive\", -1],\n",
    "    [\"I bear a charmed life\", 0],\n",
    "    [\"To do a great right do a little wrong\", -1],\n",
    "    [\"I do hate a proud man, as I hate the engendering of toads\", -1],\n",
    "    [\"As merry as the day is long\", 1],\n",
    "    [\"He that dies pays all debts\", 0],\n",
    "    [\"If to do were as easy as to know what were good to do\", -1],\n",
    "    [\"I can see he's not in your good books\", 0],\n",
    "    [\"In black ink my love may still shine bright\", 1],\n",
    "    [\"Love sought is good, but given unsought, is better\", 1],\n",
    "    [\"We cannot all be masters, nor all masters Cannot be truly follow'd\", -1],\n",
    "    [\"All that glisters is not gold\", 0],\n",
    "    [\"But love is blind and lovers cannot see\", -1],\n",
    "    [\"How far that little candle throws his beams! So shines a good deed in a weary world\", 1],\n",
    "    [\"We are such stuff as dreams are made on, rounded with a little sleep\", 0],\n",
    "    [\"When shall we three meet again in thunder, lightning, or in rain?\", -1],\n",
    "    [\"I love thee so, that, maugre all thy pride\", 1],\n",
    "    [\"I will wear my heart upon my sleeve for daws to peck at\", -1],\n",
    "    [\"The man that hath no music in himself, Nor is not moved with concord of sweet sounds, is fit for treasons, stratagems and spoils\", 0],\n",
    "    [\"The devil hath power To assume a pleasing shape\", -1],\n",
    "    [\"I cannot tell what the dickens his name is\", 0],\n",
    "    [\"O, how this spring of love resembleth the uncertain glory of an April day!\", 1],\n",
    "    [\"Is it not strange that desire should so many years outlive performance?\", -1],\n",
    "    [\"I wish you to know that you have been the last dream of my soul\", 1],\n",
    "    [\"Love is not love which alters when it alteration finds\", 1],\n",
    "    [\"I'll make my heaven in a lady's lap\", 0],\n",
    "    [\"Now, God be praised, that to believing souls gives light in darkness, comfort in despair\", 1],\n",
    "    [\"I am not merry; but I do beguile the thing I am, by seeming otherwise\", -1],\n",
    "    [\"Who ever loved that loved not at first sight?\", 1],\n",
    "    [\"Thou art as loathsome as a toad\", -1],\n",
    "    [\"I will not be sworn but love may transform me to an oyster\", 0],\n",
    "    [\"The lady doth protest too much, methinks\", -1],\n",
    "    [\"That man that hath a tongue, I say, is no man, if with his tongue he cannot win a woman\", 1],\n",
    "    [\"There are more things in heaven and earth, Horatio, than are dreamt of in your philosophy\", 0],\n",
    "    [\"The better part of valor is discretion\", 0],\n",
    "    [\"Love looks not with the eyes, but with the mind\", 1],\n",
    "    [\"There is nothing either good or bad, but thinking makes it so\", 0],\n",
    "    [\"A horse, a horse! My kingdom for a horse!\", -1],\n",
    "    [\"Hell is empty and all the devils are here\", -1],\n",
    "    [\"Love's not Time's fool, though rosy lips and cheeks\", 1],\n",
    "    [\"A fool thinks himself to be wise, but a wise man knows himself to be a fool\", 0],\n",
    "    [\"Lord, what fools these mortals be!\", -1],\n",
    "    [\"Life ... is a tale Told by an idiot, full of sound and fury, Signifying nothing\", -1],\n",
    "    [\"Listen to many, speak to a few\", 0],\n",
    "    [\"To sleep, perchance to dream\", 1],\n",
    "    [\"O, woe is me, To have seen what I have seen, see what I see!\", -1],\n",
    "    [\"The wheel is come full circle\", 0],\n",
    "    [\"O happy dagger! This is thy sheath; there rust, and let me die\", -1],\n",
    "    [\"There is a tide in the affairs of men, Which, taken at the flood, leads on to fortune\", 1],\n",
    "    [\"I am one who loved not wisely but too well\", 0],\n",
    "    [\"Give me my robe, put on my crown; I have Immortal longings in me\", -1],\n",
    "    [\"Brevity is the soul of wit\", 0],\n",
    "    [\"My only love sprung from my only hate\", -1],\n",
    "    [\"My heart is ever at your service\", 1],\n",
    "    [\"The lady doth protest too much, methinks\", 0],\n",
    "    [\"Uneasy lies the head that wears a crown\", -1],\n",
    "    [\"Give every man thy ear, but few thy voice\", 0],\n",
    "    [\"Speak low, if you speak love\", 1],\n",
    "    [\"In time we hate that which we often fear\", -1],\n",
    "    [\"Doubt thou the stars are fire\", 0],\n",
    "    [\"By the pricking of my thumbs, Something wicked this way comes\", -1],\n",
    "    [\"Love alters not with his brief hours and weeks, But bears it out even to the edge of doom\", 1],\n",
    "    [\"I bear a charmed life\", 0],\n",
    "    [\"O that this too too solid flesh would melt, Thaw and resolve itself into a dew!\", -1],\n",
    "    [\"Fair is foul, and foul is fair\", 0],\n",
    "    [\"I love you more than words can wield the matter\", 1],\n",
    "    [\"When we are born, we cry that we are come to this great stage of fools\", -1],\n",
    "    [\"O, it is excellent to have a giant's strength; but it is tyrannous to use it like a giant\", 0],\n",
    "    [\"With mirth and laughter let old wrinkles come\", 1],\n",
    "    [\"Our doubts are traitors, and make us lose the good we oft might win, by fearing to attempt\", -1],\n",
    "    [\"There's no art to find the mind's construction in the face\", 0],\n",
    "    [\"It is not in the stars to hold our destiny but in ourselves\", 1],\n",
    "    [\"Farewell, a long farewell, to all my greatness!\", -1],\n",
    "    [\"Nothing will come of nothing\", 0],\n",
    "    [\"When I said I would die a bachelor, I did not think I should live till I were married\", 1],\n",
    "    [\"How sharper than a serpent's tooth it is to have a thankless child!\", -1],\n",
    "    [\"What's in a name? That which we call a rose by any other name would smell as sweet\", 0],\n",
    "    [\"O, she doth teach the torches to burn bright!\", 1],\n",
    "    [\"Thou art a boil, a plague sore\", -1],\n",
    "    [\"Wisely and slow; they stumble that run fast\", 0],\n",
    "    [\"When to the sessions of sweet silent thought, I summon up remembrance of things past\", 1],\n",
    "    [\"Why, then, 'tis none to you; for there is nothing either good or bad, but thinking makes it so\", -1],\n",
    "    [\"They do not love that do not show their love\", 0],\n",
    "    [\"For where thou art, there is the world itself, and where thou art not, desolation\", 1],\n",
    "    [\"But, soft! What light through yonder window breaks?\", 1],\n",
    "    [\"Good night, good night! Parting is such sweet sorrow\", 0],\n",
    "    [\"A plague o' both your houses! They have made worms' meat of me\", -1],\n",
    "    [\"All the world's a stage, and all the men and women merely players\", 0],\n",
    "    [\"If music be the food of love, play on\", 1],\n",
    "    [\"Et tu, Brute? Then fall, Caesar!\", -1],\n",
    "    [\"We are such stuff as dreams are made on, and our little life is rounded with a sleep\", 0],\n",
    "    [\"Shall I compare thee to a summer's day? Thou art more lovely and more temperate\", 1],\n",
    "    [\"Out, out, brief candle! Life's but a walking shadow, a poor player that struts and frets his hour upon the stage\", -1],\n",
    "    [\"The course of true love never did run smooth\", 0],\n",
    "    [\"O, brave new world, That has such people in't!\", 1],\n",
    "    [\"Now is the winter of our discontent\", -1],\n",
    "    [\"All's well that ends well\", 1],\n",
    "    [\"Misery acquaints a man with strange bedfellows\", -1],\n",
    "    [\"Some are born great, some achieve greatness, and some have greatness thrust upon them\", 0],\n",
    "    [\"This above all: to thine own self be true\", 1],\n",
    "    [\"When sorrows come, they come not single spies, but in battalions\", -1]    \n",
    "]\n",
    "\n",
    "def updateTargets(data):\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] == -1:\n",
    "            data[i][1] = 0\n",
    "        elif data[i][1] == 0:\n",
    "            data[i][1] = 0.5\n",
    "        elif data[i][1] == 1:\n",
    "            data[i][1] = 1\n",
    "    return data\n",
    "\n",
    "shakespeareList = updateTargets(shakespeareList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.239169 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('../data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "chars.append('PAD')\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "def encode_and_pad(s, blocksize):\n",
    "    # encode the string\n",
    "    encoded_s = encode(s)\n",
    "    \n",
    "    # calculate the amount of padding needed\n",
    "    padding_needed = blocksize - len(encoded_s)\n",
    "    \n",
    "    # if padding is needed, append the appropriate amount\n",
    "    if padding_needed > 0:\n",
    "        encoded_s += [65] * padding_needed\n",
    "\n",
    "    return encoded_s\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, 1)  # The output is a single neuron now\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        sentiment_score = self.lm_head(x) # (B,T,1)\n",
    "        sentiment_score = sentiment_score.mean(dim=1)  # Averaging the sentiment scores across tokens\n",
    "        sentiment_score = sentiment_score.view(-1)  # Flattening the tensor\n",
    "        sentiment_score = torch.sigmoid(sentiment_score)  # Applying sigmoid function to make output between 0 and 1      \n",
    "\n",
    "        if targets is not None:\n",
    "            # convert targets to float and resize to match output dimension\n",
    "            targets = targets.float().view(-1)\n",
    "            loss = F.binary_cross_entropy(sentiment_score, targets)  # Binary cross entropy loss\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return sentiment_score, loss\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(myList):\n",
    "\n",
    "    x = torch.stack([torch.tensor(encode_and_pad(sentence, block_size), device=device) for sentence, _ in myList])\n",
    "    y = torch.tensor([label for _, label in myList])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    return X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
    "\n",
    "X_train, y_train, X_test, y_test = getData(shakespeareList)\n",
    "\n",
    "checkpoint = torch.load('./saved_models/trained_shakespeare/trained_shakespeare.pth', map_location='cpu')\n",
    "\n",
    "# Load the state dict from the checkpoint\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# Initialize the sentiment model\n",
    "sentiment_model = BigramLanguageModel()\n",
    "\n",
    "# Copy the state dict, excluding the lm_head weights\n",
    "pretrained_state_dict = {k: v for k, v in state_dict.items() if 'lm_head' not in k}\n",
    "\n",
    "# Update the state dict of the sentiment model\n",
    "sentiment_model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "\n",
    "sentiment_model = sentiment_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(sentiment_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Initialize the lm_head weights of the sentiment model\n",
    "# for m in sentiment_model.modules():\n",
    "#     if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "#         m.weight.data.normal_(mean=0.0, std=0.02)\n",
    "#     elif isinstance(m, nn.LayerNorm):\n",
    "#         m.bias.data.zero_()\n",
    "#         m.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:27,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6883206963539124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:02<00:22,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6715906858444214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:05<00:20,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47906261682510376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:08<00:17,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2991829216480255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:10<00:15,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25655418634414673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:13<00:12,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24850855767726898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:15<00:10,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.240171879529953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:18<00:07,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23897621035575867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [00:20<00:04,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23929819464683533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [00:23<00:02,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2381259799003601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:25<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23783817887306213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_iters = 100\n",
    "for iter in trange(max_iters):\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = sentiment_model(X_train, y_train)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        print(loss.item())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(true_values, predictions):\n",
    "    # Ensure tensors are on the same device and have the same dtype\n",
    "    true_values = true_values.to(predictions.device).type(predictions.dtype)\n",
    "\n",
    "    # Check if predictions match true_values\n",
    "    correct_predictions = (true_values == predictions)\n",
    "\n",
    "    # Calculate accuracy by averaging correct predictions\n",
    "    accuracy = correct_predictions.float().mean().item()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, _ = sentiment_model(X_test)\n",
    "\n",
    "def round_to_closest(tensor):\n",
    "    # create tensor of the thresholds (0, 0.5, 1)\n",
    "    thresholds = torch.tensor([0, 0.5, 1], device=tensor.device)\n",
    "\n",
    "    # expand dimensions for broadcasting\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    thresholds = thresholds.unsqueeze(0)\n",
    "\n",
    "    # calculate absolute difference and find index of minimal difference\n",
    "    closest_ids = torch.argmin(torch.abs(tensor - thresholds), dim=1)\n",
    "\n",
    "    # replace original tensor values with closest thresholds\n",
    "    tensor = thresholds[0, closest_ids]\n",
    "\n",
    "    return tensor\n",
    "\n",
    "rounded_tensor = round_to_closest(preds)\n",
    "\n",
    "calculate_accuracy(rounded_tensor, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.5000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.5000, 1.0000, 0.5000, 0.0000, 0.5000, 0.5000,\n",
       "        0.5000, 0.5000, 0.0000, 1.0000, 0.5000, 1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.0000, 0.0000, 1.0000, 0.5000, 0.5000, 0.5000, 0.0000,\n",
       "        1.0000, 0.5000, 0.5000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.5000, 1.0000, 0.5000, 0.5000], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4403, 0.4221, 0.4081, 0.4251, 0.4343, 0.4323, 0.4284, 0.4405, 0.4341,\n",
       "        0.4361, 0.4345, 0.4123, 0.4433, 0.4223, 0.4258, 0.4218],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "logits, loss = sentiment_model(xb)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:04<46:50,  2.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.6147, val loss 1.7972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 103/1000 [00:12<06:29,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: train loss 1.5682, val loss 1.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 202/1000 [00:19<05:48,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: train loss 1.5501, val loss 1.7430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 304/1000 [00:27<03:40,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300: train loss 1.5195, val loss 1.7259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 403/1000 [00:35<04:20,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: train loss 1.5071, val loss 1.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 505/1000 [00:43<02:37,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 1.4882, val loss 1.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 604/1000 [00:51<02:07,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train loss 1.4630, val loss 1.6803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 703/1000 [00:59<02:10,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700: train loss 1.4518, val loss 1.6633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 805/1000 [01:07<01:02,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800: train loss 1.4252, val loss 1.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 904/1000 [01:15<00:30,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 900: train loss 1.4139, val loss 1.6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 999: train loss 1.3941, val loss 1.6072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "for iter in trange(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MENENIUS:\n",
      "He is the contentence, thou be this art me.\n",
      "\n",
      "GLOUCESTER:\n",
      "What this? thou have happy the own of, and my attempt;\n",
      "We remay nor from me matched done and slept:\n",
      "What that thou in cheir tongues. A man. ore thank,\n",
      "I crown, for the crown kingd with accuse!\n",
      "Then noice the on, whore butch at to make him,\n",
      "Lest malignayery a kingly the preceping win it?\n",
      "Tell the to reck carner. Sony will all the tought.\n",
      "Coriold in! the that be criess diver them to you,\n",
      "Richard he speak, his sest hire, and little\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
