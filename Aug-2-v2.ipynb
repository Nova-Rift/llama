{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeareList = [\n",
    "    [\"Thou canst not speak of that thou dost not feel\", 0],\n",
    "    [\"I did love you once\", -1],\n",
    "    [\"He jests at scars that never felt a wound\", 0],\n",
    "    [\"True hope is swift, and flies with swallow's wings\", 1],\n",
    "    [\"A wretched soul, bruised with adversity\", -1],\n",
    "    [\"For men in great place are thrice servants\", 0],\n",
    "    [\"How far that little candle throws his beams\", 1],\n",
    "    [\"Mend your speech a little, lest you may mar your fortunes\", -1],\n",
    "    [\"Though last, not least in love\", 1],\n",
    "    [\"Like madness is the glory of life\", 0],\n",
    "    [\"A hit, a very palpable hit\", -1],\n",
    "    [\"Speak low, if you speak love\", 1],\n",
    "    [\"Who chooseth me must give and hazard all he hath\", 0],\n",
    "    [\"This is the short and the long of it\", -1],\n",
    "    [\"Men at some time are masters of their fate\", 1],\n",
    "    [\"Some rise by sin, and some by virtue fall\", 0],\n",
    "    [\"Lovers can do their amorous rites by their own beauties\", 1],\n",
    "    [\"Cowards die many times before their deaths; The valiant never taste of death but once\", -1],\n",
    "    [\"We are such stuff as dreams are made on, and our little life is rounded with a sleep\", 0],\n",
    "    [\"The first thing we do, let's kill all the lawyers\", -1],\n",
    "    [\"The game is up\", 0],\n",
    "    [\"In my mind's eye\", 1],\n",
    "    [\"Stars, hide your fires; Let not light see my black and deep desires\", -1],\n",
    "    [\"Love all, trust a few, do wrong to none\", 0],\n",
    "    [\"Sweet mercy is nobility's true badge\", 1],\n",
    "    [\"Out, out, brief candle! Life's but a walking shadow, a poor player\", -1],\n",
    "    [\"Be not afraid of greatness: some are born great, some achieve greatness and some have greatness thrust upon them\", 0],\n",
    "    [\"Have more than thou showest, Speak less than thou knowest\", 1],\n",
    "    [\"I am a man more sinned against than sinning\", -1],\n",
    "    [\"There is no darkness but ignorance\", 0],\n",
    "    [\"My mistress' eyes are nothing like the sun\", -1],\n",
    "    [\"Let's kill all the lawyers\", 0],\n",
    "    [\"But love is blind, and lovers cannot see\", 1],\n",
    "    [\"I cannot tell what the dickens his name is\", -1],\n",
    "    [\"Who can be wise, amazed, temperate and furious, Loyal and neutral, in a moment? No man\", 0],\n",
    "    [\"A peace is of the nature of a conquest\", 1],\n",
    "    [\"O, that way madness lies; let me shun that\", -1],\n",
    "    [\"Thou art a scholar; speak to it, Horatio\", 0],\n",
    "    [\"Be patient, for the world is broad and wide\", 1],\n",
    "    [\"I am not merry; but I do beguile The thing I am, by seeming otherwise\", -1],\n",
    "    [\"In a false quarrel there is no true valour\", 0],\n",
    "    [\"Love goes toward love as schoolboys from their books, But love from love, toward school with heavy looks\", 1],\n",
    "    [\"This is the very ecstasy of love\", 0],\n",
    "    [\"'Tis neither here nor there\", -1],\n",
    "    [\"It is the green-eyed monster which doth mock The meat it feeds on\", 0],\n",
    "    [\"Here will I set up my everlasting rest\", 1],\n",
    "    [\"I cannot tell what you and other men Think of this life, but, for my single self, I had as lief not be as live to be In awe of such a thing as I myself\", -1],\n",
    "    [\"She loved me for the dangers I had passed, And I loved her that she did pity them\", 1],\n",
    "    [\"Why, then the world's mine oyster, Which I with sword will open\", 0],\n",
    "    [\"'Tis the time's plague when madmen lead the blind\", -1],\n",
    "    [\"I do love nothing in the world so well as you\", 1],\n",
    "    [\"'Tis in my memory lock'd, And you yourself shall keep the key of it\", 0],\n",
    "    [\"O, I am fortune's fool!\", -1],\n",
    "    [\"I bear a charmed life\", 1],\n",
    "    [\"What light through yonder window breaks?\", 0],\n",
    "    [\"Men's evil manners live in brass; their virtues We write in water\", -1],\n",
    "    [\"There's place and means for every man alive\", 1],\n",
    "    [\"Good night, good night! parting is such sweet sorrow, That I shall say good night till it be morrow\", 0],\n",
    "    [\"O, it is excellent to have a giant's strength, but it is tyrannous to use it like a giant\", -1],\n",
    "    [\"I am a man more sinned against than sinning\", 0],\n",
    "    [\"A man loves the meat in his youth that he cannot endure in his age\", -1],\n",
    "    [\"Love is a smoke made with the fume of sighs\", 1],\n",
    "    [\"When words are scarce they are seldom spent in vain\", 0],\n",
    "    [\"What a piece of work is a man, how noble in reason, how infinite in faculties, in form and moving how express and admirable, in action how like an angel, in apprehension how like a god\", 1],\n",
    "    [\"Out of my sight! thou dost infect my eyes\", -1],\n",
    "    [\"To sleep, perchance to Dream; aye, there's the rub\", 0],\n",
    "    [\"I must be cruel, only to be kind\", -1],\n",
    "    [\"Give sorrow words: the grief that does not speak whispers the o'er-fraught heart and bids it break\", 1],\n",
    "    [\"The better part of valor is discretion\", 0],\n",
    "    [\"Pleasure and action make the hours seem short\", 1],\n",
    "    [\"There's daggers in men's smiles\", -1],\n",
    "    [\"Nothing can come of nothing\", 0],\n",
    "    [\"A horse! a horse! my kingdom for a horse!\", -1],\n",
    "    [\"The smallest worm will turn, being trodden on\", 0],\n",
    "    [\"Cowards die many times before their deaths, The valiant never taste of death but once\", 1],\n",
    "    [\"To weep is to make less the depth of grief\", -1],\n",
    "    [\"Doubt truth to be a liar, But never doubt I love\", 1],\n",
    "    [\"The robb'd that smiles, steals something from the thief\", 0],\n",
    "    [\"How bitter a thing it is to look into happiness through another man's eyes\", -1],\n",
    "    [\"Some are born great, some achieve greatness, and some have greatness thrust upon them\", 1],\n",
    "    [\"Though this be madness, yet there is method in't\", 0],\n",
    "    [\"If you prick us, do we not bleed? If you tickle us, do we not laugh? If you poison us, do we not die?\", -1],\n",
    "    [\"To thine own self be true\", 1],\n",
    "    [\"This above all: to thine own self be true\", 0],\n",
    "    [\"Come, let's away to prison; We two alone will sing like birds i' the cage\", -1],\n",
    "    [\"I'll follow thee and make a heaven of hell\", 1],\n",
    "    [\"This is the very ecstasy of love\", 1],\n",
    "    [\"The rest is silence\", 0],\n",
    "    [\"The devil can cite Scripture for his purpose\", -1],\n",
    "    [\"How poor are they that have not patience!\", 0],\n",
    "    [\"The robbed that smiles, steals something from the thief\", 1],\n",
    "    [\"The time is out of joint: O cursed spite\", -1],\n",
    "    [\"I am not bound to please thee with my answers\", 0],\n",
    "    [\"Thou and I are too wise to woo peaceably\", 1],\n",
    "    [\"Thy tongue outvenoms all the worms of Nile\", -1],\n",
    "    [\"Let's talk of graves, of worms, and epitaphs\", -1],\n",
    "    [\"I wasted time, and now doth time waste me\", -1],\n",
    "    [\"With love's light wings did I o'er-perch these walls\", 1],\n",
    "    [\"This is very midsummer madness\", 0],\n",
    "    [\"The lady protests too much, methinks\", -1],\n",
    "    [\"I am constant as the northern star\", 0],\n",
    "    [\"One touch of nature makes the whole world kin\", 1],\n",
    "    [\"My salad days, when I was green in judgment\", -1],\n",
    "    [\"What's past is prologue\", 0],\n",
    "    [\"And therefore is winged Cupid painted blind\", -1],\n",
    "    [\"Frailty, thy name is woman!\", -1],\n",
    "    [\"My love is thine to teach\", 1],\n",
    "    [\"Beware the Ides of March\", -1],\n",
    "    [\"We know what we are, but know not what we may be\", 0],\n",
    "    [\"That way madness lies\", -1],\n",
    "    [\"Though she be but little, she is fierce\", 1],\n",
    "    [\"Things without all remedy should be without regard\", 0],\n",
    "    [\"Blow, winds, and crack your cheeks! rage! blow!\", -1],\n",
    "    [\"These violent delights have violent ends\", -1],\n",
    "    [\"Sweet are the uses of adversity\", 1],\n",
    "    [\"Like as the waves make towards the pebbl'd shore, so do our minutes hasten to their end\", 0],\n",
    "    [\"The course of true love never did run smooth\", -1],\n",
    "    [\"The valiant never taste of death but once\", 1],\n",
    "    [\"When words are scarce they are seldom spent in vain\", 0],\n",
    "    [\"Cowards die many times before their deaths\", -1],\n",
    "    [\"Sigh no more, ladies, sigh no more, Men were deceivers ever\", 1],\n",
    "    [\"Suspicion always haunts the guilty mind\", -1],\n",
    "    [\"The lunatic, the lover, and the poet, are of imagination all compact\", 0],\n",
    "    [\"The fool doth think he is wise, but the wise man knows himself to be a fool\", -1],\n",
    "    [\"I do love nothing in the world so well as you\", 1],\n",
    "    [\"I like this place and willingly could waste my time in it\", 0],\n",
    "    [\"I must be cruel only to be kind\", -1],\n",
    "    [\"When I am dead and gone, remember me\", 0],\n",
    "    [\"Alas, poor Yorick! I knew him, Horatio\", -1],\n",
    "    [\"My love is deeper than the deepest ocean\", 1],\n",
    "    [\"Let every eye negotiate for itself and trust no agent\", 0],\n",
    "    [\"A jest's prosperity lies in the ear of him that hears it\", -1],\n",
    "    [\"A merry heart goes all the day\", 1],\n",
    "    [\"Give sorrow words; the grief that does not speak knits up the o-er wrought heart and bids it break\", -1],\n",
    "    [\"The wheel is come full circle: I am here\", 0],\n",
    "    [\"With mirth and laughter let old wrinkles come\", 1],\n",
    "    [\"If it be a sin to covet honor, I am the most offending soul alive\", -1],\n",
    "    [\"I bear a charmed life\", 0],\n",
    "    [\"To do a great right do a little wrong\", -1],\n",
    "    [\"I do hate a proud man, as I hate the engendering of toads\", -1],\n",
    "    [\"As merry as the day is long\", 1],\n",
    "    [\"He that dies pays all debts\", 0],\n",
    "    [\"If to do were as easy as to know what were good to do\", -1],\n",
    "    [\"I can see he's not in your good books\", 0],\n",
    "    [\"In black ink my love may still shine bright\", 1],\n",
    "    [\"Love sought is good, but given unsought, is better\", 1],\n",
    "    [\"We cannot all be masters, nor all masters Cannot be truly follow'd\", -1],\n",
    "    [\"All that glisters is not gold\", 0],\n",
    "    [\"But love is blind and lovers cannot see\", -1],\n",
    "    [\"How far that little candle throws his beams! So shines a good deed in a weary world\", 1],\n",
    "    [\"We are such stuff as dreams are made on, rounded with a little sleep\", 0],\n",
    "    [\"When shall we three meet again in thunder, lightning, or in rain?\", -1],\n",
    "    [\"I love thee so, that, maugre all thy pride\", 1],\n",
    "    [\"I will wear my heart upon my sleeve for daws to peck at\", -1],\n",
    "    [\"The man that hath no music in himself, Nor is not moved with concord of sweet sounds, is fit for treasons, stratagems and spoils\", 0],\n",
    "    [\"The devil hath power To assume a pleasing shape\", -1],\n",
    "    [\"I cannot tell what the dickens his name is\", 0],\n",
    "    [\"O, how this spring of love resembleth the uncertain glory of an April day!\", 1],\n",
    "    [\"Is it not strange that desire should so many years outlive performance?\", -1],\n",
    "    [\"I wish you to know that you have been the last dream of my soul\", 1],\n",
    "    [\"Love is not love which alters when it alteration finds\", 1],\n",
    "    [\"I'll make my heaven in a lady's lap\", 0],\n",
    "    [\"Now, God be praised, that to believing souls gives light in darkness, comfort in despair\", 1],\n",
    "    [\"I am not merry; but I do beguile the thing I am, by seeming otherwise\", -1],\n",
    "    [\"Who ever loved that loved not at first sight?\", 1],\n",
    "    [\"Thou art as loathsome as a toad\", -1],\n",
    "    [\"I will not be sworn but love may transform me to an oyster\", 0],\n",
    "    [\"The lady doth protest too much, methinks\", -1],\n",
    "    [\"That man that hath a tongue, I say, is no man, if with his tongue he cannot win a woman\", 1],\n",
    "    [\"There are more things in heaven and earth, Horatio, than are dreamt of in your philosophy\", 0],\n",
    "    [\"The better part of valor is discretion\", 0],\n",
    "    [\"Love looks not with the eyes, but with the mind\", 1],\n",
    "    [\"There is nothing either good or bad, but thinking makes it so\", 0],\n",
    "    [\"A horse, a horse! My kingdom for a horse!\", -1],\n",
    "    [\"Hell is empty and all the devils are here\", -1],\n",
    "    [\"Love's not Time's fool, though rosy lips and cheeks\", 1],\n",
    "    [\"A fool thinks himself to be wise, but a wise man knows himself to be a fool\", 0],\n",
    "    [\"Lord, what fools these mortals be!\", -1],\n",
    "    [\"Life ... is a tale Told by an idiot, full of sound and fury, Signifying nothing\", -1],\n",
    "    [\"Listen to many, speak to a few\", 0],\n",
    "    [\"To sleep, perchance to dream\", 1],\n",
    "    [\"O, woe is me, To have seen what I have seen, see what I see!\", -1],\n",
    "    [\"The wheel is come full circle\", 0],\n",
    "    [\"O happy dagger! This is thy sheath; there rust, and let me die\", -1],\n",
    "    [\"There is a tide in the affairs of men, Which, taken at the flood, leads on to fortune\", 1],\n",
    "    [\"I am one who loved not wisely but too well\", 0],\n",
    "    [\"Give me my robe, put on my crown; I have Immortal longings in me\", -1],\n",
    "    [\"Brevity is the soul of wit\", 0],\n",
    "    [\"My only love sprung from my only hate\", -1],\n",
    "    [\"My heart is ever at your service\", 1],\n",
    "    [\"The lady doth protest too much, methinks\", 0],\n",
    "    [\"Uneasy lies the head that wears a crown\", -1],\n",
    "    [\"Give every man thy ear, but few thy voice\", 0],\n",
    "    [\"Speak low, if you speak love\", 1],\n",
    "    [\"In time we hate that which we often fear\", -1],\n",
    "    [\"Doubt thou the stars are fire\", 0],\n",
    "    [\"By the pricking of my thumbs, Something wicked this way comes\", -1],\n",
    "    [\"Love alters not with his brief hours and weeks, But bears it out even to the edge of doom\", 1],\n",
    "    [\"I bear a charmed life\", 0],\n",
    "    [\"O that this too too solid flesh would melt, Thaw and resolve itself into a dew!\", -1],\n",
    "    [\"Fair is foul, and foul is fair\", 0],\n",
    "    [\"I love you more than words can wield the matter\", 1],\n",
    "    [\"When we are born, we cry that we are come to this great stage of fools\", -1],\n",
    "    [\"O, it is excellent to have a giant's strength; but it is tyrannous to use it like a giant\", 0],\n",
    "    [\"With mirth and laughter let old wrinkles come\", 1],\n",
    "    [\"Our doubts are traitors, and make us lose the good we oft might win, by fearing to attempt\", -1],\n",
    "    [\"There's no art to find the mind's construction in the face\", 0],\n",
    "    [\"It is not in the stars to hold our destiny but in ourselves\", 1],\n",
    "    [\"Farewell, a long farewell, to all my greatness!\", -1],\n",
    "    [\"Nothing will come of nothing\", 0],\n",
    "    [\"When I said I would die a bachelor, I did not think I should live till I were married\", 1],\n",
    "    [\"How sharper than a serpent's tooth it is to have a thankless child!\", -1],\n",
    "    [\"What's in a name? That which we call a rose by any other name would smell as sweet\", 0],\n",
    "    [\"O, she doth teach the torches to burn bright!\", 1],\n",
    "    [\"Thou art a boil, a plague sore\", -1],\n",
    "    [\"Wisely and slow; they stumble that run fast\", 0],\n",
    "    [\"When to the sessions of sweet silent thought, I summon up remembrance of things past\", 1],\n",
    "    [\"Why, then, 'tis none to you; for there is nothing either good or bad, but thinking makes it so\", -1],\n",
    "    [\"They do not love that do not show their love\", 0],\n",
    "    [\"For where thou art, there is the world itself, and where thou art not, desolation\", 1],\n",
    "    [\"But, soft! What light through yonder window breaks?\", 1],\n",
    "    [\"Good night, good night! Parting is such sweet sorrow\", 0],\n",
    "    [\"A plague o' both your houses! They have made worms' meat of me\", -1],\n",
    "    [\"All the world's a stage, and all the men and women merely players\", 0],\n",
    "    [\"If music be the food of love, play on\", 1],\n",
    "    [\"Et tu, Brute? Then fall, Caesar!\", -1],\n",
    "    [\"We are such stuff as dreams are made on, and our little life is rounded with a sleep\", 0],\n",
    "    [\"Shall I compare thee to a summer's day? Thou art more lovely and more temperate\", 1],\n",
    "    [\"Out, out, brief candle! Life's but a walking shadow, a poor player that struts and frets his hour upon the stage\", -1],\n",
    "    [\"The course of true love never did run smooth\", 0],\n",
    "    [\"O, brave new world, That has such people in't!\", 1],\n",
    "    [\"Now is the winter of our discontent\", -1],\n",
    "    [\"All's well that ends well\", 1],\n",
    "    [\"Misery acquaints a man with strange bedfellows\", -1],\n",
    "    [\"Some are born great, some achieve greatness, and some have greatness thrust upon them\", 0],\n",
    "    [\"This above all: to thine own self be true\", 1],\n",
    "    [\"When sorrows come, they come not single spies, but in battalions\", -1]    \n",
    "]\n",
    "\n",
    "def updateTargets(data):\n",
    "    for i in range(len(data)):\n",
    "        if data[i][1] == -1:\n",
    "            data[i][1] = 0\n",
    "        elif data[i][1] == 0:\n",
    "            data[i][1] = 0.5\n",
    "        elif data[i][1] == 1:\n",
    "            data[i][1] = 1\n",
    "    return data\n",
    "\n",
    "shakespeareList = updateTargets(shakespeareList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.239169 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('../data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "chars.append('PAD')\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "def encode_and_pad(s, blocksize):\n",
    "    # encode the string\n",
    "    encoded_s = encode(s)\n",
    "    \n",
    "    # calculate the amount of padding needed\n",
    "    padding_needed = blocksize - len(encoded_s)\n",
    "    \n",
    "    # if padding is needed, append the appropriate amount\n",
    "    if padding_needed > 0:\n",
    "        encoded_s += [65] * padding_needed\n",
    "\n",
    "    return encoded_s\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, 1)  # The output is a single neuron now\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        sentiment_score = self.lm_head(x) # (B,T,1)\n",
    "        sentiment_score = sentiment_score.mean(dim=1)  # Averaging the sentiment scores across tokens\n",
    "        sentiment_score = sentiment_score.view(-1)  # Flattening the tensor\n",
    "        sentiment_score = torch.sigmoid(sentiment_score)  # Applying sigmoid function to make output between 0 and 1      \n",
    "\n",
    "        if targets is not None:\n",
    "            # convert targets to float and resize to match output dimension\n",
    "            targets = targets.float().view(-1)\n",
    "            loss = F.binary_cross_entropy(sentiment_score, targets)  # Binary cross entropy loss\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return sentiment_score, loss\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(myList):\n",
    "\n",
    "    x = torch.stack([torch.tensor(encode_and_pad(sentence, block_size), device=device) for sentence, _ in myList])\n",
    "    y = torch.tensor([label for _, label in myList])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    return X_train.to(device), y_train.to(device), X_test.to(device), y_test.to(device)\n",
    "\n",
    "X_train, y_train, X_test, y_test = getData(shakespeareList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./saved_models/trained_shakespeare/trained_shakespeare.pth', map_location='cpu')\n",
    "\n",
    "# Load the state dict from the checkpoint\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "# Initialize the sentiment model\n",
    "sentiment_model = BigramLanguageModel()\n",
    "\n",
    "# Copy the state dict, excluding the lm_head weights\n",
    "pretrained_state_dict = {k: v for k, v in state_dict.items() if 'lm_head' not in k}\n",
    "\n",
    "# Update the state dict of the sentiment model\n",
    "sentiment_model.load_state_dict(pretrained_state_dict, strict=False)\n",
    "\n",
    "sentiment_model = sentiment_model.to(device)\n",
    "\n",
    "# Set the learning rates\n",
    "low_lr = 1e-4\n",
    "high_lr = 1e-3\n",
    "\n",
    "# Get all parameters of the model\n",
    "model_params = list(sentiment_model.named_parameters())\n",
    "\n",
    "# Separate the parameters into two groups: \n",
    "# one for pre-trained layers (low learning rate)\n",
    "# one for the newly added layers (high learning rate)\n",
    "pretrained_params = [p for n, p in model_params if 'lm_head' not in n]\n",
    "new_params = [p for n, p in model_params if 'lm_head' in n]\n",
    "\n",
    "# Define the optimizer to use different learning rates for different parameter groups\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': pretrained_params, 'lr': low_lr},\n",
    "    {'params': new_params, 'lr': high_lr}\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentiment model\n",
    "sentiment_model = BigramLanguageModel()\n",
    "\n",
    "sentiment_model = sentiment_model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(sentiment_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Initialize the lm_head weights of the sentiment model\n",
    "# for m in sentiment_model.modules():\n",
    "#     if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "#         m.weight.data.normal_(mean=0.0, std=0.02)\n",
    "#     elif isinstance(m, nn.LayerNorm):\n",
    "#         m.bias.data.zero_()\n",
    "#         m.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('token_embedding_table.weight',\n",
       "              tensor([[ 0.1769, -0.0410, -0.3897,  ..., -1.9596, -0.5568,  1.0564],\n",
       "                      [ 0.4193, -1.0834, -0.7988,  ...,  0.0537, -0.3840, -0.5005],\n",
       "                      [-0.5921,  1.9393,  0.3036,  ...,  0.4922, -0.9253,  0.6576],\n",
       "                      ...,\n",
       "                      [ 0.3351,  0.3965, -0.3717,  ...,  0.4652,  0.3239,  0.5393],\n",
       "                      [-2.1822,  0.6954,  0.6643,  ..., -0.1460,  2.3360,  2.0221],\n",
       "                      [-0.4871,  0.1765, -0.7419,  ..., -0.7115, -0.2201, -0.1116]],\n",
       "                     device='cuda:0')),\n",
       "             ('position_embedding_table.weight',\n",
       "              tensor([[-0.2303, -1.5577, -0.9424,  ...,  1.2698,  0.2820, -1.1742],\n",
       "                      [-0.7510,  0.0427, -0.1967,  ..., -1.2779, -1.9479, -0.5975],\n",
       "                      [-0.8943, -0.3436, -0.4935,  ..., -2.1485, -0.2589,  1.7064],\n",
       "                      ...,\n",
       "                      [ 1.2827,  0.1181,  0.2124,  ...,  0.2524,  1.6898, -0.0875],\n",
       "                      [ 1.3414,  0.6826, -0.0656,  ...,  0.8545, -0.1294,  0.9308],\n",
       "                      [ 0.0071, -0.9407,  1.1000,  ...,  0.9745, -0.6385,  1.0470]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.0.key.weight',\n",
       "              tensor([[-0.0096, -0.0690, -0.0410,  ...,  0.0440, -0.0154, -0.0181],\n",
       "                      [ 0.0169,  0.0130,  0.0130,  ...,  0.0120,  0.0077,  0.0131],\n",
       "                      [-0.0287, -0.0440, -0.0456,  ..., -0.0219,  0.0253,  0.0400],\n",
       "                      ...,\n",
       "                      [ 0.0254,  0.0430, -0.0305,  ..., -0.0090,  0.0068, -0.0057],\n",
       "                      [-0.0149,  0.0146, -0.0141,  ..., -0.0393, -0.0109, -0.0353],\n",
       "                      [ 0.0523,  0.0149, -0.0427,  ...,  0.0099, -0.0191,  0.0105]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.0.query.weight',\n",
       "              tensor([[ 0.0258, -0.0683, -0.0203,  ..., -0.0173,  0.0284, -0.0025],\n",
       "                      [-0.0252,  0.0118,  0.0175,  ..., -0.0146,  0.0631,  0.0494],\n",
       "                      [ 0.0219, -0.0330, -0.0334,  ..., -0.0499,  0.0125, -0.0410],\n",
       "                      ...,\n",
       "                      [ 0.0369, -0.0494, -0.0518,  ..., -0.0020, -0.0422,  0.0241],\n",
       "                      [-0.0380, -0.0215, -0.1120,  ..., -0.0038, -0.0627, -0.0532],\n",
       "                      [ 0.0387, -0.0489, -0.0332,  ...,  0.0241,  0.0444, -0.0137]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.0.value.weight',\n",
       "              tensor([[ 0.0068, -0.0202,  0.0443,  ...,  0.0207,  0.0376, -0.0300],\n",
       "                      [-0.0067, -0.0102,  0.0445,  ...,  0.0303,  0.0564, -0.0450],\n",
       "                      [-0.0241, -0.0258, -0.0438,  ...,  0.0900,  0.0373, -0.0069],\n",
       "                      ...,\n",
       "                      [-0.0434,  0.0026,  0.0114,  ..., -0.0295, -0.0652, -0.0358],\n",
       "                      [-0.0047,  0.0435,  0.0286,  ..., -0.0633, -0.0150,  0.0850],\n",
       "                      [ 0.0296, -0.0723, -0.0255,  ..., -0.0060,  0.0212,  0.0181]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.1.key.weight',\n",
       "              tensor([[ 0.0550, -0.0049, -0.1685,  ...,  0.1836, -0.0059,  0.0338],\n",
       "                      [ 0.1337, -0.0705, -0.0407,  ..., -0.1070,  0.0815,  0.0999],\n",
       "                      [ 0.0061, -0.0163,  0.1572,  ...,  0.0618, -0.1877,  0.0214],\n",
       "                      ...,\n",
       "                      [ 0.0691, -0.0805,  0.2276,  ..., -0.0497, -0.0757,  0.0120],\n",
       "                      [-0.0363, -0.0920, -0.0279,  ...,  0.0064,  0.0220,  0.0190],\n",
       "                      [ 0.0880, -0.0180,  0.0591,  ..., -0.0283,  0.0238,  0.0008]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.1.query.weight',\n",
       "              tensor([[ 0.0195, -0.0262, -0.1105,  ...,  0.2044,  0.0264,  0.0243],\n",
       "                      [-0.0598,  0.0025, -0.0102,  ..., -0.0313, -0.0448,  0.0080],\n",
       "                      [-0.0841,  0.0777,  0.1470,  ...,  0.0755, -0.1438,  0.1000],\n",
       "                      ...,\n",
       "                      [-0.0072,  0.0060,  0.0190,  ...,  0.0547,  0.1155, -0.0758],\n",
       "                      [-0.1457,  0.0003,  0.0306,  ..., -0.0192, -0.0406,  0.0850],\n",
       "                      [-0.0340,  0.0231, -0.0668,  ..., -0.0367, -0.0152,  0.1034]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.1.value.weight',\n",
       "              tensor([[ 0.0707,  0.0895, -0.0199,  ...,  0.0941, -0.0563, -0.0696],\n",
       "                      [ 0.0736,  0.0461,  0.0038,  ...,  0.0719,  0.0086,  0.0252],\n",
       "                      [-0.0168,  0.0039,  0.0306,  ...,  0.0175,  0.1185, -0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0635, -0.0025,  0.0264,  ...,  0.0067, -0.0070, -0.0122],\n",
       "                      [ 0.0022,  0.0005,  0.0799,  ..., -0.0389, -0.0504,  0.0029],\n",
       "                      [-0.0027, -0.1216, -0.0407,  ..., -0.0160,  0.0625,  0.0532]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.2.key.weight',\n",
       "              tensor([[-0.0073,  0.0481, -0.0269,  ...,  0.0499,  0.0407, -0.0462],\n",
       "                      [ 0.0538, -0.0029, -0.0027,  ..., -0.0378,  0.0256, -0.0040],\n",
       "                      [-0.0447,  0.0373,  0.0570,  ..., -0.0567,  0.0463,  0.0393],\n",
       "                      ...,\n",
       "                      [ 0.0776, -0.0488,  0.0080,  ..., -0.0073, -0.0301, -0.0067],\n",
       "                      [-0.0826,  0.0112, -0.0382,  ..., -0.0476,  0.0231, -0.0187],\n",
       "                      [ 0.0054, -0.0463, -0.0227,  ..., -0.0134,  0.0235,  0.0208]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.2.query.weight',\n",
       "              tensor([[ 0.0236, -0.0035,  0.0797,  ...,  0.0389,  0.0089, -0.0239],\n",
       "                      [ 0.0095, -0.0160, -0.0202,  ...,  0.0124,  0.0045,  0.0639],\n",
       "                      [-0.0063, -0.0428,  0.0565,  ...,  0.0329, -0.0075, -0.0217],\n",
       "                      ...,\n",
       "                      [ 0.0217, -0.0318,  0.0656,  ...,  0.0799, -0.0038, -0.0361],\n",
       "                      [-0.0143, -0.0328,  0.0291,  ...,  0.0234, -0.0079, -0.0610],\n",
       "                      [ 0.0132,  0.0730, -0.0211,  ..., -0.0349, -0.0383, -0.0295]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.2.value.weight',\n",
       "              tensor([[-0.0371,  0.0348, -0.0592,  ...,  0.0345,  0.0077, -0.0835],\n",
       "                      [-0.1042,  0.0672,  0.0047,  ..., -0.0495, -0.0032,  0.0058],\n",
       "                      [-0.0103,  0.0230,  0.0313,  ..., -0.0124, -0.0068,  0.0775],\n",
       "                      ...,\n",
       "                      [-0.0443,  0.0003, -0.0675,  ...,  0.1061,  0.0817, -0.0266],\n",
       "                      [-0.0198, -0.0094, -0.0079,  ...,  0.0259, -0.0028, -0.0610],\n",
       "                      [ 0.0249,  0.0199, -0.0816,  ...,  0.0555,  0.0474, -0.0652]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.3.key.weight',\n",
       "              tensor([[-0.0456, -0.0175, -0.0579,  ..., -0.0062, -0.0244,  0.0119],\n",
       "                      [ 0.0396,  0.0614,  0.0067,  ...,  0.0419,  0.0571, -0.0191],\n",
       "                      [-0.0084,  0.0139, -0.0719,  ..., -0.0372,  0.0306,  0.0064],\n",
       "                      ...,\n",
       "                      [-0.0215,  0.0558, -0.1001,  ..., -0.0444, -0.0041,  0.0486],\n",
       "                      [ 0.0524,  0.0254, -0.0594,  ..., -0.0021, -0.0523, -0.0564],\n",
       "                      [-0.0041,  0.0001,  0.0518,  ..., -0.0129, -0.0136, -0.0197]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.3.query.weight',\n",
       "              tensor([[-0.0348, -0.0090,  0.0009,  ...,  0.0100, -0.0318,  0.0296],\n",
       "                      [-0.0767,  0.0080, -0.0761,  ...,  0.0521, -0.0260,  0.0150],\n",
       "                      [-0.0205,  0.0120,  0.0194,  ...,  0.0261, -0.0229,  0.0133],\n",
       "                      ...,\n",
       "                      [ 0.0243, -0.0110, -0.0104,  ..., -0.0486, -0.0057,  0.0368],\n",
       "                      [-0.0092, -0.0264,  0.0038,  ..., -0.0255,  0.0006,  0.0403],\n",
       "                      [-0.0221, -0.0477, -0.0447,  ...,  0.0169, -0.0011, -0.0524]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.heads.3.value.weight',\n",
       "              tensor([[-0.0595,  0.0297, -0.0298,  ...,  0.0170, -0.0493, -0.0046],\n",
       "                      [ 0.0592, -0.0330,  0.1237,  ..., -0.0082, -0.0808,  0.1006],\n",
       "                      [ 0.0668, -0.0460,  0.0668,  ..., -0.0115, -0.0270,  0.0169],\n",
       "                      ...,\n",
       "                      [ 0.0313,  0.0274, -0.1716,  ...,  0.0362, -0.0200, -0.0931],\n",
       "                      [-0.0234,  0.0281,  0.0352,  ..., -0.1055, -0.0875,  0.0929],\n",
       "                      [ 0.0324, -0.0388,  0.0827,  ..., -0.0349,  0.0822,  0.0273]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.sa.proj.weight',\n",
       "              tensor([[-9.8899e-03,  3.4018e-02, -8.5438e-02,  ..., -7.3932e-02,\n",
       "                        7.6256e-02, -5.2591e-02],\n",
       "                      [ 1.8385e-02, -3.1974e-02, -2.5522e-02,  ...,  8.0375e-03,\n",
       "                        5.4956e-02, -6.5934e-02],\n",
       "                      [ 5.0647e-02, -7.3343e-02, -1.3227e-01,  ..., -2.0541e-01,\n",
       "                        2.0587e-01,  8.9452e-02],\n",
       "                      ...,\n",
       "                      [-3.9007e-02,  3.2973e-02, -8.8128e-02,  ..., -1.0156e-02,\n",
       "                        8.1693e-02, -1.9777e-02],\n",
       "                      [ 3.2296e-02,  8.6628e-02,  6.3995e-02,  ...,  2.0047e-02,\n",
       "                       -4.9496e-02,  2.7966e-02],\n",
       "                      [ 2.9611e-02,  2.3893e-02, -3.7613e-02,  ...,  7.2101e-05,\n",
       "                        5.7746e-02, -2.4288e-02]], device='cuda:0')),\n",
       "             ('blocks.0.sa.proj.bias',\n",
       "              tensor([ 0.0005,  0.0229, -0.0378, -0.0437, -0.0109,  0.0146,  0.0357, -0.0160,\n",
       "                      -0.0179,  0.0322, -0.0444, -0.0676, -0.0360,  0.0898,  0.0726,  0.0856,\n",
       "                      -0.0343,  0.0432, -0.0250,  0.0447, -0.0061,  0.0548,  0.0641,  0.0030,\n",
       "                      -0.0035, -0.0101,  0.0303,  0.0663,  0.0807,  0.0310,  0.0313, -0.0491,\n",
       "                      -0.1307, -0.0186,  0.0279,  0.0114,  0.0363,  0.1215, -0.0602, -0.0342,\n",
       "                      -0.0449, -0.0446, -0.0412,  0.0612,  0.0467,  0.0898,  0.0050,  0.0137,\n",
       "                       0.0130, -0.0665,  0.0382, -0.0078, -0.0245, -0.0652,  0.0832,  0.0136,\n",
       "                      -0.0334, -0.0359,  0.0498,  0.0189, -0.0058,  0.0516, -0.0569,  0.0042,\n",
       "                       0.0496,  0.0037, -0.0203,  0.0810,  0.0481, -0.0619,  0.0435, -0.0056,\n",
       "                      -0.0361,  0.0269,  0.0016,  0.0161, -0.0437, -0.0300, -0.0147, -0.0117,\n",
       "                       0.0673,  0.0079, -0.0567, -0.0618, -0.0588,  0.0252,  0.0269,  0.0412,\n",
       "                      -0.0146,  0.0176, -0.0034, -0.0502,  0.0412,  0.0550,  0.0202, -0.0091,\n",
       "                      -0.0640, -0.0534, -0.0433, -0.0718,  0.0784, -0.0448, -0.0136, -0.0016,\n",
       "                      -0.0828, -0.0117,  0.0284,  0.0168,  0.0106,  0.0160, -0.0353,  0.0108,\n",
       "                      -0.0131, -0.0070,  0.0566, -0.0094, -0.0330, -0.0171, -0.0004,  0.0522,\n",
       "                      -0.0618, -0.0721, -0.0229,  0.0425, -0.0249,  0.0452,  0.0511, -0.0485,\n",
       "                      -0.0889,  0.1102,  0.0024, -0.0087,  0.0953,  0.0258,  0.0463, -0.0498,\n",
       "                      -0.0293,  0.0124,  0.0031, -0.0231,  0.0421, -0.1018,  0.0506, -0.0379,\n",
       "                       0.0317, -0.0208, -0.0818,  0.0538, -0.0874,  0.0117,  0.0168, -0.0339,\n",
       "                      -0.0453,  0.0508, -0.0371, -0.0074,  0.0510,  0.0591, -0.0018,  0.0546,\n",
       "                      -0.0293, -0.0044,  0.0508, -0.0158, -0.0344,  0.0787,  0.0159,  0.0904,\n",
       "                       0.0404,  0.0769, -0.0206, -0.0432, -0.0241,  0.0487,  0.0040,  0.0013,\n",
       "                       0.0854, -0.0073, -0.0409, -0.0363, -0.0230,  0.0508,  0.0004, -0.0189,\n",
       "                      -0.0498,  0.0099,  0.0004, -0.0104,  0.0049, -0.0185,  0.1223,  0.0367,\n",
       "                      -0.0688,  0.0323,  0.0782, -0.0026, -0.0002, -0.0525, -0.0292, -0.0096,\n",
       "                       0.0643, -0.0504, -0.0054, -0.0818,  0.0347, -0.0185,  0.0267,  0.0003,\n",
       "                       0.0388, -0.0765, -0.0970,  0.0202, -0.0247, -0.0428,  0.0202, -0.0351,\n",
       "                       0.0442,  0.0283, -0.0147,  0.0351,  0.0186,  0.0410, -0.0253, -0.0028,\n",
       "                      -0.0635,  0.0909, -0.0309, -0.0315,  0.0378,  0.0207,  0.0009, -0.0126,\n",
       "                      -0.0427, -0.0037, -0.0459, -0.0989,  0.0883,  0.0069,  0.0544, -0.0167,\n",
       "                       0.0491, -0.0405,  0.0591,  0.0480, -0.0543,  0.1053, -0.0374, -0.0472,\n",
       "                       0.0259,  0.0466, -0.0416,  0.0461,  0.0520,  0.0277,  0.0448, -0.0423],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.ffwd.net.0.weight',\n",
       "              tensor([[-0.0684, -0.0775,  0.0421,  ..., -0.1021, -0.0338, -0.1218],\n",
       "                      [ 0.0709,  0.0893, -0.0604,  ..., -0.0225, -0.0370, -0.0166],\n",
       "                      [ 0.0003,  0.0405,  0.0392,  ...,  0.1095,  0.0853,  0.0592],\n",
       "                      ...,\n",
       "                      [-0.0579,  0.0525, -0.0073,  ..., -0.0475,  0.0290,  0.0370],\n",
       "                      [-0.1037, -0.0439,  0.0079,  ..., -0.0617, -0.0089,  0.0194],\n",
       "                      [ 0.0328, -0.0560, -0.0508,  ..., -0.0655, -0.0881, -0.0215]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.ffwd.net.0.bias',\n",
       "              tensor([-0.0733, -0.0949, -0.0318,  ..., -0.0775, -0.0423, -0.1029],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.ffwd.net.2.weight',\n",
       "              tensor([[-0.0051, -0.0279, -0.0436,  ...,  0.0295,  0.0317,  0.0727],\n",
       "                      [-0.0184,  0.0047, -0.0487,  ..., -0.0206,  0.0067, -0.0229],\n",
       "                      [-0.0341, -0.0202, -0.0138,  ..., -0.0434,  0.0306,  0.0173],\n",
       "                      ...,\n",
       "                      [ 0.0527,  0.0339, -0.0416,  ...,  0.0786, -0.0196,  0.0235],\n",
       "                      [ 0.0328, -0.0183, -0.0124,  ...,  0.0113, -0.0677,  0.0150],\n",
       "                      [ 0.0235,  0.0012, -0.0460,  ...,  0.0327,  0.0338, -0.0199]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.ffwd.net.2.bias',\n",
       "              tensor([-0.0110, -0.0138, -0.0665, -0.0049, -0.0300,  0.0226,  0.0190, -0.0373,\n",
       "                      -0.0158,  0.0046, -0.0023, -0.0292,  0.0166,  0.0004,  0.0162,  0.0524,\n",
       "                       0.0162,  0.1136, -0.0107, -0.0020,  0.0021, -0.0048,  0.0027,  0.0135,\n",
       "                      -0.0118, -0.0206, -0.0202, -0.0039,  0.0321,  0.0040,  0.0610, -0.0331,\n",
       "                      -0.0995,  0.0048, -0.0217, -0.0543,  0.0096,  0.1164,  0.0016, -0.0035,\n",
       "                      -0.0457, -0.0115,  0.0180,  0.0227,  0.0031,  0.0580, -0.0027, -0.0376,\n",
       "                       0.0036,  0.0072,  0.0420, -0.0415,  0.0101, -0.0217, -0.0126,  0.0137,\n",
       "                       0.0214,  0.0017, -0.0566, -0.0228, -0.0093, -0.0304,  0.0058, -0.0059,\n",
       "                       0.0390, -0.0658, -0.0099,  0.1015, -0.0150, -0.0143, -0.0364,  0.0207,\n",
       "                      -0.0029, -0.0032,  0.0734,  0.0148, -0.0006,  0.0071,  0.0247,  0.0581,\n",
       "                       0.0243,  0.0513, -0.0313,  0.0300,  0.0192, -0.0359, -0.0103, -0.0155,\n",
       "                       0.0240, -0.0038, -0.0404,  0.0077, -0.0451, -0.0040, -0.0540, -0.0029,\n",
       "                       0.0191,  0.0022, -0.0072, -0.0045,  0.0204, -0.0343,  0.0068,  0.0030,\n",
       "                      -0.0695, -0.0170,  0.0008, -0.0201,  0.0126,  0.0324, -0.0057, -0.0171,\n",
       "                       0.0022, -0.0074,  0.0437, -0.0560, -0.0327,  0.0003, -0.0260, -0.0158,\n",
       "                       0.0214, -0.0102, -0.0257,  0.0009,  0.0580, -0.0185,  0.0022, -0.0201,\n",
       "                      -0.0028,  0.0950, -0.0160, -0.0033,  0.0590,  0.0524,  0.0180, -0.0128,\n",
       "                       0.0756, -0.0048,  0.0208, -0.0018, -0.0349, -0.0942, -0.0272,  0.0114,\n",
       "                       0.0121, -0.0370, -0.0317,  0.0091, -0.0293,  0.0284, -0.0242,  0.0207,\n",
       "                       0.0116, -0.0165,  0.0087,  0.0339, -0.0157,  0.0078, -0.0205,  0.0273,\n",
       "                      -0.0283,  0.0122,  0.0371, -0.0344,  0.0234,  0.0056, -0.0222,  0.0285,\n",
       "                      -0.0250,  0.0169, -0.0036, -0.0699,  0.0284, -0.0242, -0.0561, -0.0138,\n",
       "                       0.0485,  0.0375, -0.0268, -0.0194,  0.0388,  0.0073,  0.0103, -0.0267,\n",
       "                       0.0099, -0.0205, -0.0064, -0.0977,  0.0124,  0.0333,  0.0139, -0.0140,\n",
       "                       0.0163, -0.0013,  0.0327,  0.0069, -0.0447,  0.0102,  0.0092, -0.0069,\n",
       "                       0.0047, -0.0054,  0.0012, -0.0047, -0.0247, -0.0126, -0.0012, -0.0020,\n",
       "                       0.0250, -0.0178, -0.0706,  0.0377,  0.0308, -0.0337,  0.0112, -0.0591,\n",
       "                      -0.0662,  0.0174, -0.0141, -0.0207, -0.0245,  0.0389, -0.0734, -0.0238,\n",
       "                      -0.0030,  0.0684,  0.0073, -0.0282,  0.0230,  0.0293, -0.0021, -0.0147,\n",
       "                      -0.0068,  0.0327,  0.0224, -0.0504,  0.0732,  0.0283,  0.0367,  0.0297,\n",
       "                       0.0566, -0.0345,  0.0784, -0.0068, -0.0160,  0.0415, -0.0053, -0.0279,\n",
       "                      -0.0252,  0.0019, -0.0004,  0.0349,  0.0236,  0.0217,  0.0069, -0.0181],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.0.ln1.weight',\n",
       "              tensor([0.9466, 0.9423, 1.0750, 0.9207, 0.9552, 0.8797, 1.0029, 0.8607, 0.9469,\n",
       "                      0.9875, 0.8381, 1.0906, 0.8276, 0.9829, 1.0378, 1.0132, 0.9242, 0.9837,\n",
       "                      1.0353, 1.0658, 0.9993, 1.0033, 0.9442, 1.0117, 0.9888, 0.9801, 0.9537,\n",
       "                      0.9018, 0.9534, 0.9929, 0.9298, 0.9395, 1.0652, 0.9177, 1.0227, 0.9171,\n",
       "                      0.8979, 0.9655, 0.9300, 1.0383, 0.9956, 1.0728, 0.9050, 1.0847, 0.9220,\n",
       "                      0.8825, 0.9621, 1.0430, 0.9769, 0.9811, 0.9949, 1.0002, 0.9113, 0.9813,\n",
       "                      0.8678, 0.8416, 0.8997, 0.8171, 0.9544, 0.9822, 0.9664, 0.9399, 1.0472,\n",
       "                      0.9874, 0.8400, 1.0199, 0.9509, 0.8922, 0.9915, 0.9593, 1.0318, 1.0161,\n",
       "                      0.9381, 0.9661, 1.0971, 0.9668, 1.0348, 0.9179, 0.8977, 1.0152, 1.0181,\n",
       "                      0.9267, 1.0268, 0.9085, 0.8837, 0.9747, 0.9838, 1.0618, 0.9900, 0.9348,\n",
       "                      1.0081, 1.0295, 0.9310, 0.9506, 1.0419, 0.9840, 0.9836, 0.9206, 0.9304,\n",
       "                      0.9539, 0.8753, 0.9248, 1.0344, 0.9742, 0.9846, 0.9877, 0.9476, 1.0281,\n",
       "                      0.8984, 0.9337, 0.8783, 0.9202, 1.0568, 0.9170, 0.8958, 0.9831, 1.0567,\n",
       "                      0.9367, 0.9497, 0.9172, 1.0346, 1.0350, 0.9814, 0.9276, 0.8915, 0.9737,\n",
       "                      0.9404, 0.9526, 1.0959, 0.9370, 0.9520, 1.0143, 1.0176, 0.9873, 1.0520,\n",
       "                      0.8391, 1.0905, 0.9110, 0.8946, 0.8833, 1.0075, 0.9703, 0.9880, 0.9627,\n",
       "                      1.0347, 0.9908, 0.9871, 0.9755, 1.0093, 1.1055, 1.0249, 0.8577, 0.9360,\n",
       "                      0.9449, 0.9391, 0.9328, 1.0123, 0.9699, 0.8846, 0.9249, 0.9734, 0.9079,\n",
       "                      0.9021, 0.9235, 0.9546, 0.9360, 0.8603, 1.1005, 1.0859, 0.8393, 0.9345,\n",
       "                      1.0071, 1.0766, 0.9392, 0.9715, 1.0160, 0.9447, 0.9563, 1.0568, 0.9121,\n",
       "                      0.9560, 0.8200, 0.9308, 0.9159, 0.9133, 0.9451, 0.9629, 1.0174, 0.9454,\n",
       "                      1.0860, 0.9438, 0.9723, 0.9585, 0.9822, 0.9986, 0.9830, 0.9182, 1.0262,\n",
       "                      0.9226, 0.9279, 0.9651, 1.0654, 1.0095, 1.0403, 0.9532, 1.0566, 0.9960,\n",
       "                      1.0291, 1.0631, 0.8312, 0.9089, 0.9981, 1.0044, 0.8818, 0.9431, 0.9922,\n",
       "                      1.0261, 0.9591, 1.0899, 0.9986, 0.9742, 0.9027, 0.9398, 0.8944, 0.9193,\n",
       "                      1.0400, 0.9356, 0.9665, 0.9221, 0.9268, 0.9840, 1.0322, 0.8786, 0.8296,\n",
       "                      0.9972, 0.9340, 0.9826, 1.1031, 1.0297, 0.9552, 0.9668, 1.0451, 1.0055,\n",
       "                      0.9020, 0.9774, 0.8580, 1.0704, 0.9033, 0.8604, 0.9392, 0.9811, 0.9356,\n",
       "                      0.9454, 0.9322, 0.9255, 0.9711], device='cuda:0')),\n",
       "             ('blocks.0.ln1.bias',\n",
       "              tensor([ 4.1062e-02,  8.6602e-02,  4.6577e-02,  2.7353e-02, -1.8015e-02,\n",
       "                      -9.6231e-03,  2.3237e-02,  2.9826e-03,  9.8601e-03,  7.0759e-02,\n",
       "                       2.6022e-02, -2.9547e-02,  5.2269e-02,  2.6994e-02,  2.2845e-02,\n",
       "                       3.5919e-02, -6.5452e-03, -2.2117e-03,  1.3526e-02,  2.4969e-02,\n",
       "                       9.9626e-03, -9.3030e-03, -2.0243e-02, -1.4235e-02, -6.7075e-03,\n",
       "                       5.0028e-02,  1.1116e-02,  5.0197e-02,  3.7207e-02,  4.4963e-03,\n",
       "                       2.9965e-02,  2.7898e-04, -4.0400e-02, -1.0349e-01,  5.1045e-02,\n",
       "                      -6.1524e-03, -5.6017e-02, -3.2289e-02,  6.3294e-03, -8.5214e-03,\n",
       "                       3.9898e-02,  2.8796e-03,  5.3420e-03, -2.4940e-02, -1.0164e-02,\n",
       "                       4.5384e-02, -5.9827e-03,  4.3593e-03, -4.6335e-02,  5.6561e-02,\n",
       "                       4.0173e-02, -6.6640e-02, -3.0782e-02,  2.3917e-02, -2.4863e-02,\n",
       "                       1.7711e-02,  3.2080e-02, -2.4033e-02, -4.8683e-02, -2.5743e-02,\n",
       "                      -8.7007e-03,  1.9805e-02, -2.4999e-02, -4.4210e-02, -1.3323e-02,\n",
       "                      -4.7858e-02,  9.4811e-03,  1.9510e-02,  1.6032e-02, -7.4358e-02,\n",
       "                      -2.3871e-02, -3.5698e-03, -2.9687e-04, -1.9797e-02,  2.0041e-02,\n",
       "                      -1.0389e-01, -1.9620e-02, -1.8094e-03, -4.4751e-02,  4.9518e-02,\n",
       "                      -4.5356e-02,  3.0915e-02,  7.1224e-04,  5.8164e-02, -2.2497e-02,\n",
       "                      -1.0247e-02, -4.4021e-02, -6.4087e-03, -1.9263e-02, -1.0664e-02,\n",
       "                       1.3010e-02,  2.9574e-03, -7.7050e-02, -2.0790e-02,  1.1285e-02,\n",
       "                      -3.0856e-02,  1.5179e-02, -4.7357e-02, -4.4811e-02, -1.0341e-04,\n",
       "                       6.0394e-02,  6.4373e-03, -3.4406e-03,  1.8971e-02, -4.5927e-02,\n",
       "                      -1.2095e-02, -2.9289e-02,  2.0434e-02,  7.9055e-03,  1.8866e-02,\n",
       "                       2.6597e-03, -2.8499e-02,  4.7008e-02,  1.1818e-03, -8.4459e-02,\n",
       "                      -2.4457e-02, -3.2360e-02, -2.2576e-04,  1.4538e-02,  7.9196e-02,\n",
       "                      -3.0120e-02, -7.3332e-02,  5.1953e-04, -1.9452e-02, -3.4052e-02,\n",
       "                      -4.0137e-02, -1.7522e-03,  1.7630e-03,  4.5133e-02,  1.9684e-03,\n",
       "                      -3.9967e-02,  1.3746e-02,  1.4917e-02,  2.1791e-02,  1.9480e-02,\n",
       "                      -4.3243e-02,  3.5317e-04, -5.1512e-02, -1.6031e-02,  1.9337e-02,\n",
       "                      -3.9569e-02,  1.4707e-02, -1.5829e-02, -3.4162e-02, -5.4686e-02,\n",
       "                       3.6544e-02, -1.8598e-02,  3.1908e-02,  3.2194e-02, -2.6350e-02,\n",
       "                       2.9424e-02,  5.9402e-02, -4.6119e-02,  2.9719e-03, -4.8219e-03,\n",
       "                      -1.0668e-02,  4.2105e-02, -6.9666e-03, -8.6421e-02, -3.0125e-02,\n",
       "                      -1.7204e-02, -4.8023e-02,  2.6877e-02,  4.7018e-02,  3.6801e-02,\n",
       "                      -2.0417e-02, -3.0335e-02,  1.0424e-02, -1.3670e-02,  7.9376e-02,\n",
       "                       6.5631e-02,  1.9505e-02, -7.5074e-02, -9.5665e-04, -5.7213e-02,\n",
       "                       2.2604e-02,  4.4218e-02, -9.3907e-03,  3.2361e-03,  2.4330e-02,\n",
       "                       3.1862e-02, -4.3370e-02,  3.2969e-02, -3.3586e-03,  7.7855e-03,\n",
       "                      -3.0245e-02,  1.2761e-02, -6.0631e-02, -7.3821e-02,  1.3895e-04,\n",
       "                      -1.8322e-02, -4.3274e-02,  2.8450e-02, -2.3294e-02, -1.0174e-02,\n",
       "                       1.9141e-02,  3.5467e-03,  6.0326e-03,  3.3620e-02, -2.7746e-02,\n",
       "                       7.6841e-02,  1.0683e-02, -3.1336e-02,  1.6804e-02,  3.4352e-02,\n",
       "                      -1.4680e-02,  1.1750e-02,  5.5380e-02, -4.7573e-02,  4.9723e-02,\n",
       "                      -3.8706e-02,  1.1409e-02, -4.3866e-02,  2.0820e-02,  2.4590e-02,\n",
       "                      -1.3154e-02,  3.5112e-02,  2.9665e-02, -2.8454e-02,  3.4086e-03,\n",
       "                      -1.7122e-03,  9.8152e-03, -2.7743e-02, -9.6500e-03, -1.9925e-02,\n",
       "                       4.9742e-02,  2.3056e-02, -9.8523e-03, -4.2723e-02,  2.8684e-02,\n",
       "                      -8.2199e-03, -1.1912e-02, -8.2498e-02,  7.5009e-02, -2.3501e-02,\n",
       "                       6.3560e-03,  1.4516e-02,  3.9202e-02,  3.0048e-02, -4.1146e-03,\n",
       "                       1.1573e-02, -8.4656e-03, -1.9066e-02,  4.7718e-02,  3.0543e-02,\n",
       "                      -1.7661e-02,  4.5520e-02,  1.2881e-02,  1.5928e-02,  2.0786e-02,\n",
       "                      -2.3157e-02, -2.6760e-02, -2.7639e-02,  1.1444e-02, -5.1348e-03,\n",
       "                       3.5971e-02], device='cuda:0')),\n",
       "             ('blocks.0.ln2.weight',\n",
       "              tensor([0.9597, 0.9531, 0.7567, 0.9519, 0.9254, 0.9708, 0.8495, 0.8913, 0.8354,\n",
       "                      0.9347, 0.8923, 0.7197, 0.8925, 0.9147, 0.9176, 0.7292, 0.8707, 0.8469,\n",
       "                      0.9748, 0.8695, 0.9250, 0.9473, 0.8740, 0.8735, 0.9293, 0.8834, 0.8145,\n",
       "                      1.0030, 0.9615, 0.9388, 0.9451, 0.9007, 0.8262, 0.9532, 0.9098, 0.9067,\n",
       "                      0.8959, 0.7527, 0.9368, 0.9387, 0.9419, 0.8557, 0.8949, 0.9806, 0.9538,\n",
       "                      0.8665, 0.9164, 0.8529, 0.9572, 0.9565, 0.8214, 0.9239, 0.9516, 0.9250,\n",
       "                      0.9165, 0.9678, 0.9479, 0.9470, 0.9111, 0.9310, 0.9334, 0.8954, 0.8393,\n",
       "                      0.9034, 0.8926, 0.7766, 0.9389, 0.8558, 0.9877, 0.9437, 0.9457, 0.9845,\n",
       "                      0.9518, 0.9064, 0.9608, 0.9292, 0.9120, 0.9259, 0.9312, 0.7813, 0.9399,\n",
       "                      0.9328, 0.9417, 0.9120, 0.9774, 0.9575, 0.8850, 0.9608, 0.9790, 0.8832,\n",
       "                      0.8302, 0.9615, 0.9205, 0.9540, 0.9989, 0.9229, 0.9054, 0.8731, 0.9600,\n",
       "                      0.9543, 0.8364, 1.0058, 0.9053, 0.9535, 0.9050, 0.8765, 0.9763, 0.8109,\n",
       "                      0.9673, 0.9184, 0.9858, 0.9205, 0.7533, 0.9465, 0.9407, 0.7978, 0.9393,\n",
       "                      0.9495, 0.9122, 0.9161, 0.9118, 0.8979, 1.0387, 0.9700, 0.8854, 0.9109,\n",
       "                      0.9947, 0.9002, 0.9253, 0.8982, 0.9005, 0.9086, 0.9528, 0.8183, 0.8853,\n",
       "                      0.9343, 0.8388, 1.0114, 0.9142, 0.9069, 0.9674, 0.8764, 0.9965, 0.9655,\n",
       "                      0.9677, 0.9911, 0.8394, 0.8995, 0.8188, 0.8382, 0.9094, 0.9080, 0.8819,\n",
       "                      0.9638, 0.9004, 0.9371, 0.8833, 0.9438, 0.8872, 0.9459, 0.9454, 0.9165,\n",
       "                      0.8760, 0.9209, 0.9465, 0.8171, 0.9636, 0.7363, 0.9024, 0.9642, 0.9045,\n",
       "                      0.8704, 0.7691, 0.8908, 0.8723, 0.9660, 0.9142, 0.8859, 0.8936, 0.9497,\n",
       "                      0.9762, 0.9190, 0.8764, 0.9407, 0.9381, 0.9010, 1.0078, 0.8803, 0.9046,\n",
       "                      0.8891, 0.8710, 0.8949, 0.8628, 0.9689, 0.9291, 0.9327, 0.9231, 0.8835,\n",
       "                      0.9723, 0.9605, 0.8952, 0.9697, 0.9788, 0.9996, 0.9283, 0.9618, 0.9787,\n",
       "                      0.8173, 0.9390, 0.9455, 0.9085, 0.8215, 0.9845, 0.8850, 0.9997, 0.8781,\n",
       "                      0.8570, 0.9538, 0.6725, 0.9413, 0.9277, 0.9558, 0.8521, 0.9436, 0.9605,\n",
       "                      0.7771, 0.9336, 0.9013, 0.9098, 0.9067, 0.8542, 0.9113, 0.9300, 0.9193,\n",
       "                      0.7432, 0.9066, 0.8634, 0.8906, 0.8292, 0.9492, 0.8648, 0.9241, 0.9412,\n",
       "                      1.0008, 0.9506, 0.8551, 0.8489, 0.8865, 0.8825, 0.8868, 0.8899, 0.9184,\n",
       "                      0.9377, 1.0028, 0.9106, 0.9268], device='cuda:0')),\n",
       "             ('blocks.0.ln2.bias',\n",
       "              tensor([ 0.0209, -0.0088,  0.0565,  0.0689,  0.0222,  0.0008,  0.0143, -0.0289,\n",
       "                       0.0476,  0.0053,  0.0128, -0.0128,  0.0173,  0.0567,  0.0030, -0.0489,\n",
       "                       0.0295, -0.0178, -0.0565, -0.0166, -0.0875,  0.0103, -0.0138, -0.0134,\n",
       "                       0.0070,  0.0054,  0.0421, -0.0162,  0.0310,  0.0244,  0.0039,  0.0032,\n",
       "                       0.0440, -0.0499, -0.0372,  0.0398,  0.0337, -0.0453, -0.0293, -0.0451,\n",
       "                       0.0576, -0.0625, -0.0566,  0.0353,  0.0183, -0.0903, -0.0518,  0.0121,\n",
       "                       0.0036, -0.0247,  0.0197, -0.0378, -0.0173,  0.0614, -0.0020, -0.0450,\n",
       "                       0.0098, -0.0508,  0.0918,  0.0271, -0.0174,  0.0671,  0.0022,  0.0579,\n",
       "                       0.0009,  0.0233, -0.0003,  0.0190, -0.0494,  0.0138,  0.0469, -0.0407,\n",
       "                       0.0020,  0.0123, -0.0372,  0.0145,  0.0055, -0.0472,  0.0695, -0.0488,\n",
       "                       0.0346, -0.0321,  0.0139, -0.0059, -0.0386,  0.0095, -0.0190,  0.0313,\n",
       "                       0.0351,  0.0598, -0.0523,  0.0059,  0.0146,  0.0045,  0.0218, -0.0575,\n",
       "                      -0.0288, -0.0080,  0.0401, -0.0205,  0.0434,  0.0100,  0.0479,  0.0214,\n",
       "                       0.0022, -0.0625, -0.0089, -0.0293,  0.0203, -0.0087, -0.0585,  0.0144,\n",
       "                       0.0210, -0.1097, -0.0190,  0.0556, -0.0012, -0.0918, -0.0066, -0.0240,\n",
       "                      -0.0319, -0.0894,  0.0115, -0.0525, -0.0282,  0.0106,  0.0361,  0.0035,\n",
       "                      -0.0312,  0.0104,  0.0348,  0.0293,  0.0249, -0.0266,  0.0511, -0.0100,\n",
       "                      -0.0462, -0.0547,  0.0520, -0.0376,  0.0611, -0.0137,  0.0670, -0.0491,\n",
       "                       0.0344, -0.0088, -0.0515,  0.0103, -0.0226, -0.0116, -0.0003, -0.0237,\n",
       "                      -0.0087, -0.0035,  0.0063, -0.0357, -0.0182, -0.0036,  0.0635,  0.0237,\n",
       "                      -0.0134,  0.0700,  0.0445, -0.0076, -0.0154,  0.0126, -0.0389, -0.0213,\n",
       "                       0.0014,  0.0677,  0.0451,  0.0461, -0.1086, -0.0161,  0.0884,  0.0467,\n",
       "                      -0.0194, -0.0093,  0.0860, -0.0385, -0.0192,  0.0210, -0.0745,  0.0447,\n",
       "                       0.0328, -0.0392,  0.0457,  0.0011, -0.0217, -0.0660,  0.0207,  0.0209,\n",
       "                      -0.0401,  0.0299,  0.0080, -0.0249,  0.0317,  0.0424, -0.0720,  0.0266,\n",
       "                      -0.0149, -0.0472,  0.0435, -0.0081, -0.0083, -0.0200,  0.0173,  0.0808,\n",
       "                      -0.0189,  0.0006,  0.0202, -0.0556, -0.0378,  0.0075,  0.0039,  0.0715,\n",
       "                       0.0615,  0.0087,  0.0369, -0.0508,  0.0470, -0.0038,  0.1454, -0.0446,\n",
       "                      -0.0114, -0.0369, -0.0148,  0.0065,  0.0456,  0.0235,  0.0162,  0.0135,\n",
       "                       0.0615, -0.0192,  0.0057, -0.0026, -0.0555, -0.0769, -0.0588, -0.0013,\n",
       "                       0.0087,  0.0431, -0.0290, -0.0342,  0.0097,  0.0216,  0.0807, -0.0466,\n",
       "                       0.0163, -0.0329,  0.0447, -0.0655,  0.0730,  0.0248, -0.0256,  0.0156],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.0.key.weight',\n",
       "              tensor([[-0.0074, -0.0460,  0.1798,  ...,  0.0375,  0.0931, -0.0021],\n",
       "                      [-0.0834, -0.0290, -0.2226,  ...,  0.0385, -0.0273, -0.0398],\n",
       "                      [-0.0713,  0.0042, -0.0770,  ...,  0.1101,  0.0163,  0.0240],\n",
       "                      ...,\n",
       "                      [ 0.0650, -0.0403, -0.0773,  ...,  0.0214, -0.0017, -0.0779],\n",
       "                      [-0.0638, -0.0337, -0.1169,  ..., -0.0037, -0.0116,  0.0305],\n",
       "                      [ 0.0232, -0.0475,  0.1077,  ...,  0.0547,  0.0816,  0.0092]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.0.query.weight',\n",
       "              tensor([[-4.5819e-02, -6.9220e-02, -1.6870e-01,  ...,  2.3241e-02,\n",
       "                       -3.0201e-02, -1.1691e-01],\n",
       "                      [-4.5719e-02,  8.6852e-02,  4.8484e-02,  ..., -5.7602e-02,\n",
       "                       -1.4400e-03, -5.6337e-02],\n",
       "                      [ 1.0340e-02,  2.9370e-02, -6.9131e-02,  ..., -1.0097e-01,\n",
       "                       -9.4800e-02, -1.7041e-02],\n",
       "                      ...,\n",
       "                      [ 4.0255e-02,  8.8144e-02,  4.9583e-02,  ..., -1.4284e-02,\n",
       "                       -3.1646e-02,  9.6437e-02],\n",
       "                      [ 1.1026e-02,  1.0975e-01,  2.3870e-02,  ..., -1.8932e-02,\n",
       "                       -6.4610e-02,  5.8506e-02],\n",
       "                      [ 1.9616e-02, -4.1841e-02, -3.9641e-02,  ...,  4.8803e-02,\n",
       "                       -1.0393e-05, -5.7867e-02]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.0.value.weight',\n",
       "              tensor([[ 0.0908,  0.0040,  0.0696,  ..., -0.0300,  0.0187,  0.0269],\n",
       "                      [ 0.0693, -0.0719,  0.0640,  ...,  0.0181,  0.1071, -0.0477],\n",
       "                      [ 0.0465,  0.0096, -0.0435,  ..., -0.0156,  0.0020, -0.0075],\n",
       "                      ...,\n",
       "                      [-0.0442,  0.0282,  0.0142,  ...,  0.0401,  0.0471, -0.0048],\n",
       "                      [ 0.0511, -0.0362, -0.0063,  ...,  0.0082,  0.0060, -0.0454],\n",
       "                      [ 0.0246, -0.0086,  0.0100,  ...,  0.0319, -0.0237,  0.0280]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.1.key.weight',\n",
       "              tensor([[-0.1021, -0.0145, -0.0424,  ..., -0.0160,  0.0065,  0.0107],\n",
       "                      [-0.1231, -0.0306, -0.0844,  ...,  0.0841, -0.0301, -0.0165],\n",
       "                      [-0.1154,  0.1108,  0.0774,  ..., -0.0316,  0.0757,  0.0117],\n",
       "                      ...,\n",
       "                      [-0.0417,  0.0241,  0.2186,  ..., -0.0447, -0.0460, -0.0057],\n",
       "                      [ 0.0710,  0.0539,  0.0949,  ..., -0.0120, -0.0420,  0.0385],\n",
       "                      [-0.0137,  0.0089, -0.1289,  ..., -0.0811,  0.0316,  0.0141]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.1.query.weight',\n",
       "              tensor([[ 1.7651e-02,  2.3944e-02,  1.3034e-01,  ..., -7.6255e-02,\n",
       "                       -2.5800e-02,  2.8428e-05],\n",
       "                      [ 1.6428e-01,  3.8695e-02,  4.2348e-03,  ...,  1.0336e-02,\n",
       "                        2.0519e-02,  7.6141e-02],\n",
       "                      [ 2.2562e-02,  3.3269e-02,  8.2927e-02,  ..., -1.4215e-01,\n",
       "                       -5.6844e-02, -5.2714e-02],\n",
       "                      ...,\n",
       "                      [-4.6357e-02, -4.3979e-02,  3.5425e-03,  ...,  8.1841e-02,\n",
       "                        2.2119e-02, -8.1713e-02],\n",
       "                      [-4.4149e-02, -2.3737e-02, -4.5671e-02,  ...,  7.3370e-02,\n",
       "                       -6.0893e-02, -7.4141e-02],\n",
       "                      [-6.7819e-02, -1.9248e-01, -1.9119e-02,  ...,  8.5075e-02,\n",
       "                       -4.1387e-02, -1.9431e-01]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.1.value.weight',\n",
       "              tensor([[ 0.0747,  0.0492, -0.0278,  ...,  0.0334, -0.0085, -0.0243],\n",
       "                      [ 0.0676, -0.0733, -0.0695,  ..., -0.0330,  0.0007,  0.0160],\n",
       "                      [-0.0654, -0.0297, -0.0268,  ..., -0.0183, -0.0324,  0.1088],\n",
       "                      ...,\n",
       "                      [ 0.0982,  0.0453, -0.0537,  ...,  0.0082,  0.0440,  0.0299],\n",
       "                      [-0.1604, -0.0221, -0.0737,  ..., -0.0636,  0.0496,  0.0213],\n",
       "                      [-0.0392,  0.0532, -0.0895,  ...,  0.0389,  0.0308,  0.0506]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.2.key.weight',\n",
       "              tensor([[ 0.0592,  0.0525,  0.2742,  ..., -0.0074,  0.0039, -0.0691],\n",
       "                      [-0.0298,  0.0510, -0.3032,  ..., -0.0640,  0.0319, -0.0105],\n",
       "                      [-0.0096, -0.0425, -0.1250,  ..., -0.0474, -0.0573,  0.0918],\n",
       "                      ...,\n",
       "                      [ 0.0222,  0.0378, -0.0723,  ..., -0.0201, -0.0099,  0.0617],\n",
       "                      [-0.0065,  0.0066, -0.1277,  ..., -0.0683,  0.0102,  0.0848],\n",
       "                      [-0.1018,  0.0451, -0.1156,  ..., -0.0025, -0.0975, -0.0758]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.2.query.weight',\n",
       "              tensor([[-0.0852, -0.0083,  0.0371,  ...,  0.0488,  0.0332, -0.0381],\n",
       "                      [-0.0088, -0.0351,  0.0607,  ...,  0.0722,  0.0135, -0.0825],\n",
       "                      [-0.0022,  0.0004,  0.0464,  ..., -0.1609, -0.0158, -0.0686],\n",
       "                      ...,\n",
       "                      [ 0.0523,  0.0082,  0.0552,  ..., -0.0155, -0.0028, -0.0184],\n",
       "                      [ 0.0351,  0.1024,  0.0158,  ..., -0.0234, -0.1292, -0.0550],\n",
       "                      [-0.0517,  0.1361,  0.0447,  ...,  0.0443, -0.0962,  0.0290]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.2.value.weight',\n",
       "              tensor([[ 4.1272e-02,  6.4735e-02, -6.2760e-02,  ...,  3.3576e-02,\n",
       "                       -8.9341e-02,  6.9395e-02],\n",
       "                      [-1.2625e-01, -3.5005e-04, -6.5618e-03,  ..., -4.2356e-02,\n",
       "                        6.2453e-02,  1.1939e-01],\n",
       "                      [-7.1112e-02,  2.4032e-02, -3.6953e-02,  ...,  3.7057e-02,\n",
       "                       -9.5209e-02,  1.5487e-02],\n",
       "                      ...,\n",
       "                      [-5.7463e-02,  4.4657e-02, -4.4830e-02,  ..., -1.1485e-04,\n",
       "                        7.8891e-02, -7.1867e-02],\n",
       "                      [ 3.8552e-02,  3.2388e-02,  2.8852e-02,  ...,  6.6902e-02,\n",
       "                       -6.5565e-02, -5.7724e-02],\n",
       "                      [ 7.4721e-02, -1.8405e-03, -2.8378e-04,  ..., -6.3010e-02,\n",
       "                       -1.2007e-02, -1.8295e-02]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.3.key.weight',\n",
       "              tensor([[-0.1306,  0.0090,  0.0374,  ..., -0.0875, -0.0511,  0.0095],\n",
       "                      [-0.0764, -0.0161, -0.1581,  ..., -0.0154, -0.0111,  0.0098],\n",
       "                      [-0.0375,  0.0233, -0.1688,  ..., -0.0753,  0.0169, -0.0478],\n",
       "                      ...,\n",
       "                      [ 0.0312, -0.0008,  0.0913,  ..., -0.0192,  0.0218, -0.0651],\n",
       "                      [ 0.0837,  0.0353,  0.0902,  ..., -0.0319,  0.0022, -0.0085],\n",
       "                      [ 0.0206,  0.0366,  0.1461,  ...,  0.0059, -0.0231,  0.0299]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.3.query.weight',\n",
       "              tensor([[ 0.0389,  0.0285, -0.0517,  ..., -0.0370,  0.0305, -0.0546],\n",
       "                      [-0.0011,  0.0843,  0.0329,  ..., -0.0816,  0.0502, -0.0429],\n",
       "                      [-0.0070,  0.0103,  0.1097,  ..., -0.0246,  0.0532,  0.1261],\n",
       "                      ...,\n",
       "                      [-0.0769, -0.1135,  0.0545,  ..., -0.0102,  0.0162,  0.0591],\n",
       "                      [-0.1555,  0.0549, -0.0647,  ...,  0.0495, -0.0050, -0.0383],\n",
       "                      [-0.0853, -0.0237, -0.0341,  ...,  0.0713,  0.0593,  0.0843]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.heads.3.value.weight',\n",
       "              tensor([[ 0.0122,  0.0296,  0.0509,  ..., -0.0333, -0.0120, -0.0638],\n",
       "                      [-0.0051,  0.0563, -0.0264,  ...,  0.0304, -0.0735,  0.0328],\n",
       "                      [ 0.0620, -0.0158, -0.0065,  ..., -0.1168,  0.0381, -0.0051],\n",
       "                      ...,\n",
       "                      [-0.0683, -0.0586,  0.0410,  ...,  0.0273,  0.0238, -0.0284],\n",
       "                      [ 0.0730, -0.0021,  0.0678,  ..., -0.0153, -0.0046, -0.0178],\n",
       "                      [-0.0011, -0.0150,  0.0270,  ...,  0.0622, -0.0374,  0.0619]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.proj.weight',\n",
       "              tensor([[-0.0553,  0.0217, -0.0743,  ..., -0.0388, -0.0142,  0.0743],\n",
       "                      [ 0.0617,  0.0261,  0.0223,  ...,  0.0399,  0.0423,  0.0420],\n",
       "                      [-0.0842,  0.0489,  0.0107,  ...,  0.0310, -0.0246, -0.0006],\n",
       "                      ...,\n",
       "                      [ 0.0752, -0.0420, -0.0171,  ...,  0.0084, -0.0397, -0.0668],\n",
       "                      [ 0.0721,  0.0217, -0.0450,  ..., -0.0208, -0.0332, -0.0049],\n",
       "                      [-0.0617, -0.1054, -0.1219,  ..., -0.0718,  0.0143, -0.0905]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.sa.proj.bias',\n",
       "              tensor([-8.0677e-03, -4.8575e-02,  7.7745e-02, -6.1598e-04,  2.2335e-02,\n",
       "                      -1.9365e-02,  1.4960e-03, -3.5295e-02,  2.2339e-02, -2.8611e-02,\n",
       "                       1.3859e-02,  6.3709e-02,  6.6136e-02,  6.4567e-02, -1.3756e-02,\n",
       "                       9.4084e-03,  6.3047e-02,  1.7341e-02, -1.2720e-02,  1.7516e-02,\n",
       "                      -5.7327e-03,  1.4735e-02, -5.7353e-02,  2.4707e-02,  1.6060e-02,\n",
       "                       9.6949e-02, -4.3039e-02, -2.0630e-02,  5.1100e-02,  2.1144e-02,\n",
       "                       5.0977e-03,  1.5379e-02, -4.2412e-02, -4.6695e-03,  1.0152e-02,\n",
       "                      -4.4858e-02,  5.3547e-02,  1.9149e-02, -2.3510e-02, -6.6059e-02,\n",
       "                      -6.8271e-02, -3.7621e-02,  3.0102e-02, -4.7397e-02, -1.1428e-01,\n",
       "                      -1.4494e-02,  3.1434e-02, -4.0939e-02,  4.1074e-02, -6.7161e-03,\n",
       "                      -2.6157e-02, -4.7271e-02,  4.8985e-02, -7.8221e-02, -5.2824e-02,\n",
       "                       3.7886e-02,  7.5702e-02, -1.7359e-02,  3.8998e-02, -1.8984e-02,\n",
       "                      -2.3987e-02,  1.0595e-02,  1.3077e-02, -1.2649e-02, -2.4484e-02,\n",
       "                      -9.9721e-03,  3.5156e-02, -1.1598e-02, -4.8378e-02, -1.1722e-02,\n",
       "                       2.0948e-02, -2.8159e-02,  1.5827e-02, -4.1802e-02,  6.0703e-02,\n",
       "                       3.1610e-02,  4.7445e-02,  2.1880e-02,  5.1367e-02,  5.5237e-03,\n",
       "                       6.1066e-02,  3.2034e-02,  2.5724e-02,  2.6894e-02,  2.9428e-03,\n",
       "                      -6.1412e-02, -4.5645e-02,  1.7882e-02,  5.5506e-02, -6.3250e-03,\n",
       "                      -5.4914e-02, -4.0529e-02, -4.6784e-02,  5.4560e-04, -1.8286e-02,\n",
       "                      -7.4049e-03, -8.1169e-02, -3.7712e-02, -7.2743e-02,  2.3794e-02,\n",
       "                       5.8447e-02, -2.8794e-02, -2.2652e-02,  9.7507e-02, -2.4686e-02,\n",
       "                      -9.0945e-03, -9.4372e-04, -5.0591e-02,  1.0038e-02, -4.8726e-02,\n",
       "                      -1.5388e-05, -6.4290e-02,  5.2626e-02, -5.9409e-02,  1.4268e-02,\n",
       "                      -3.9805e-03, -3.3124e-02,  2.3801e-02, -1.1415e-02, -3.2611e-02,\n",
       "                      -5.9839e-04, -4.2504e-02,  3.7473e-02,  1.7310e-02, -2.8491e-02,\n",
       "                      -5.9508e-02, -7.5369e-02, -4.3344e-02, -2.8457e-02, -3.3936e-02,\n",
       "                       5.7810e-02,  1.0099e-01,  1.0647e-01,  6.9018e-02, -2.3404e-02,\n",
       "                       2.8823e-04, -8.9629e-03, -4.8412e-02, -2.9934e-02, -2.9813e-02,\n",
       "                      -5.3987e-02, -8.3804e-02,  5.8378e-02,  5.0437e-03,  8.1173e-02,\n",
       "                      -8.5961e-02, -8.2720e-02,  7.1773e-02, -2.9195e-04,  5.5426e-02,\n",
       "                      -1.6892e-02,  2.3544e-02, -4.8193e-02, -7.1722e-03,  4.5403e-02,\n",
       "                       8.2107e-02,  2.9498e-02,  4.2424e-02, -3.9078e-02, -3.9696e-02,\n",
       "                      -3.4069e-02, -2.0525e-02,  5.7990e-02, -2.8778e-02,  2.3531e-02,\n",
       "                      -6.8677e-02, -1.7327e-02,  7.8440e-03,  1.5385e-02,  2.9681e-02,\n",
       "                      -8.8893e-02, -3.7394e-02,  6.5595e-03, -5.3347e-02,  6.1186e-03,\n",
       "                      -5.2262e-02,  4.2302e-02,  2.0507e-02, -4.1599e-02, -5.2006e-02,\n",
       "                       8.9778e-02, -3.8820e-02, -4.8299e-02,  4.0603e-03, -7.1843e-02,\n",
       "                       1.4279e-02, -3.5262e-02, -5.9715e-03,  2.8778e-02,  4.7425e-02,\n",
       "                      -3.6994e-02, -7.9126e-02, -1.4829e-02, -1.0305e-01,  4.8518e-02,\n",
       "                      -2.3577e-02,  9.7863e-03,  4.9060e-02, -6.1458e-02, -7.8236e-02,\n",
       "                      -2.6993e-02, -2.5745e-02,  8.0927e-02, -6.1256e-02, -6.8683e-02,\n",
       "                      -7.0392e-02,  7.2160e-02, -1.5580e-02, -3.3772e-02,  5.7752e-02,\n",
       "                      -1.0998e-03, -2.5966e-02, -3.4271e-02, -7.1723e-02, -6.2961e-02,\n",
       "                       2.0585e-02,  1.1392e-02,  8.4407e-02, -8.8173e-02,  3.4466e-02,\n",
       "                       6.6098e-02,  6.4702e-02,  3.2771e-02, -1.7350e-02, -4.9411e-03,\n",
       "                       7.5914e-02, -9.1466e-02,  2.9738e-02, -8.7104e-03,  7.4580e-02,\n",
       "                       8.0191e-02, -3.6024e-02,  3.0060e-02,  2.2652e-02, -7.2640e-02,\n",
       "                       3.4189e-02, -3.5354e-03, -8.3389e-03,  5.5179e-02,  3.5296e-02,\n",
       "                       4.7422e-02,  2.1630e-02,  1.4895e-02, -5.8572e-02, -5.9690e-02,\n",
       "                       5.0212e-02,  4.9049e-02, -2.2284e-02, -1.5745e-02,  3.9688e-02,\n",
       "                      -2.1689e-02,  5.9479e-02, -4.2147e-02,  2.9468e-02, -2.4864e-02,\n",
       "                       5.2356e-02], device='cuda:0')),\n",
       "             ('blocks.1.ffwd.net.0.weight',\n",
       "              tensor([[-0.0225,  0.0716,  0.0841,  ..., -0.0281,  0.0669, -0.0598],\n",
       "                      [-0.0734,  0.0363, -0.0172,  ..., -0.0374, -0.0244,  0.0401],\n",
       "                      [ 0.0845,  0.0245, -0.0027,  ..., -0.0154, -0.0960,  0.1136],\n",
       "                      ...,\n",
       "                      [ 0.0444,  0.0871,  0.0816,  ..., -0.0592,  0.0150, -0.0133],\n",
       "                      [ 0.0188,  0.0661,  0.0663,  ..., -0.0506,  0.0882,  0.0210],\n",
       "                      [ 0.0596,  0.0579,  0.0110,  ..., -0.0577, -0.0772,  0.0157]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.ffwd.net.0.bias',\n",
       "              tensor([-0.0329, -0.0628, -0.0672,  ...,  0.0007,  0.0022, -0.0549],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.ffwd.net.2.weight',\n",
       "              tensor([[-0.0032, -0.0762, -0.0144,  ..., -0.0040,  0.0037,  0.0040],\n",
       "                      [-0.0038, -0.0256,  0.0096,  ..., -0.0221,  0.0235, -0.0237],\n",
       "                      [ 0.0244,  0.0103, -0.0419,  ...,  0.0401, -0.0116, -0.0160],\n",
       "                      ...,\n",
       "                      [-0.0333,  0.0024, -0.0481,  ...,  0.0084,  0.0312,  0.0046],\n",
       "                      [ 0.0137, -0.0325,  0.0166,  ...,  0.0538, -0.0030,  0.0617],\n",
       "                      [-0.0201, -0.0617,  0.0457,  ...,  0.0094, -0.0342, -0.0350]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.ffwd.net.2.bias',\n",
       "              tensor([-2.3915e-04, -2.3910e-02, -6.0398e-03, -3.2315e-03, -6.8844e-02,\n",
       "                       1.4929e-02, -3.2848e-02,  1.0676e-02,  6.3234e-02, -1.2413e-02,\n",
       "                      -4.7597e-02, -1.5619e-02,  3.2626e-02,  8.4763e-03,  1.2618e-02,\n",
       "                       5.8284e-02, -1.3387e-02,  2.2817e-02,  7.7817e-03, -1.7021e-02,\n",
       "                       1.0838e-02, -4.5480e-02,  1.8686e-02,  4.4695e-03, -1.1013e-02,\n",
       "                       5.2242e-02,  1.7591e-02, -1.1012e-02, -1.3681e-02, -1.9265e-02,\n",
       "                       2.2585e-02, -8.9882e-03, -4.7695e-02, -8.4130e-03,  1.5436e-02,\n",
       "                      -2.9374e-02, -1.4464e-02,  1.2570e-02,  6.8888e-03,  1.2697e-02,\n",
       "                      -3.5915e-02, -4.9537e-04, -8.1442e-03, -1.8007e-02, -3.7765e-03,\n",
       "                      -1.3437e-02,  6.2121e-04, -4.3451e-02,  2.4690e-02, -1.0443e-03,\n",
       "                       2.9956e-02, -5.2414e-02,  1.3156e-02, -4.6592e-02,  5.0579e-02,\n",
       "                      -1.9582e-02,  5.4225e-02, -1.3584e-02, -4.6718e-03, -4.8018e-02,\n",
       "                       7.7429e-03,  1.3861e-02, -2.4862e-02,  5.7329e-03, -2.1340e-02,\n",
       "                      -1.0741e-02, -4.9606e-02,  2.5990e-02,  1.7573e-02, -2.1414e-02,\n",
       "                      -9.5980e-03,  3.3930e-02, -2.2360e-03, -1.9283e-02,  4.6252e-02,\n",
       "                      -4.4935e-03,  2.5541e-02,  1.8891e-02, -6.1098e-03,  4.9067e-02,\n",
       "                       2.7903e-02, -5.0161e-03,  1.9940e-02,  1.5075e-02, -1.1489e-03,\n",
       "                      -2.9721e-02, -1.3578e-02, -5.1993e-02,  2.1804e-02, -1.6161e-02,\n",
       "                      -5.0354e-02,  2.0954e-02, -1.0571e-02,  9.2780e-03, -2.5489e-02,\n",
       "                       2.2668e-02, -2.9328e-02,  2.2037e-02, -9.4606e-03,  1.2951e-02,\n",
       "                       5.1310e-02, -3.3623e-02,  1.7985e-02, -2.3234e-03, -4.1958e-02,\n",
       "                      -1.1110e-02, -1.1963e-05,  1.8027e-02, -3.7138e-02, -3.4362e-03,\n",
       "                       5.3291e-04, -4.3069e-02,  2.9340e-02, -7.1235e-03, -1.2578e-02,\n",
       "                      -1.8770e-02, -1.7767e-02,  6.0861e-02,  2.1442e-02,  2.9662e-02,\n",
       "                      -4.4565e-03, -2.9157e-02, -3.6491e-02,  2.3444e-02, -4.9119e-03,\n",
       "                      -2.4023e-02,  8.0442e-03,  2.5344e-02, -2.2756e-02,  4.8795e-02,\n",
       "                       3.2320e-02,  3.7737e-02,  5.2700e-02,  6.2356e-02, -2.8128e-02,\n",
       "                      -6.8163e-02,  3.2467e-02, -4.6416e-02,  5.6823e-02, -1.5446e-02,\n",
       "                      -3.5405e-02, -4.0938e-02,  2.8995e-02, -1.3747e-02, -5.2902e-03,\n",
       "                       4.1713e-03, -9.4588e-03,  3.5584e-02, -3.2882e-02,  2.9763e-02,\n",
       "                      -2.3043e-02,  2.0324e-02, -5.5751e-02,  2.4694e-02,  2.9153e-03,\n",
       "                       2.9565e-02,  1.5577e-02, -2.0138e-03,  7.9177e-03,  2.6826e-02,\n",
       "                      -2.0206e-02, -8.7342e-03,  5.4499e-02, -1.3118e-02,  2.6496e-03,\n",
       "                       7.2852e-03,  3.7678e-02,  4.7010e-02, -2.3913e-02, -1.4112e-02,\n",
       "                       1.0157e-02, -4.7582e-02,  8.0882e-03, -1.7304e-02, -3.5726e-02,\n",
       "                      -1.2020e-02,  4.4743e-02,  3.6661e-02,  3.0072e-02,  3.7654e-02,\n",
       "                       1.4476e-02,  6.7705e-03, -4.1420e-02, -3.7153e-02, -3.6361e-02,\n",
       "                      -2.0666e-02, -4.4128e-03, -2.6008e-02, -8.5992e-03,  3.2047e-02,\n",
       "                      -2.1456e-02, -6.8632e-03,  1.0631e-02, -1.2106e-02,  1.7751e-02,\n",
       "                      -1.1711e-02, -2.0821e-02,  2.3225e-03, -1.2256e-02, -6.9454e-02,\n",
       "                      -6.6882e-04,  5.1553e-03,  9.9531e-03, -1.5067e-02,  1.5121e-02,\n",
       "                      -1.3983e-02,  1.6101e-02, -1.1105e-02, -2.9367e-02,  2.5293e-02,\n",
       "                      -2.6370e-02,  3.7496e-02,  4.2848e-02, -3.6130e-02, -1.2635e-02,\n",
       "                      -6.6880e-02, -2.2418e-02,  1.2094e-02, -1.7330e-02, -5.3869e-02,\n",
       "                       1.0414e-02,  5.2375e-02, -6.7029e-02, -2.8981e-02,  2.7379e-02,\n",
       "                       3.1698e-02, -1.2064e-02, -1.0089e-02,  2.7212e-02,  2.6758e-02,\n",
       "                       3.2259e-03, -1.9638e-02,  8.0523e-03,  2.0803e-02,  8.3722e-03,\n",
       "                      -3.3778e-02,  3.8075e-02, -1.0577e-03,  2.3682e-02,  2.6114e-02,\n",
       "                       6.6701e-03, -9.6129e-03,  5.0455e-02,  2.5950e-02, -4.4258e-03,\n",
       "                       3.5884e-02,  2.7254e-02, -3.5778e-02, -2.9596e-02,  5.3254e-02,\n",
       "                       3.9946e-03,  4.5681e-02, -2.8347e-02,  2.5296e-02,  1.8233e-02,\n",
       "                       1.8066e-02], device='cuda:0')),\n",
       "             ('blocks.1.ln1.weight',\n",
       "              tensor([1.0333, 0.9653, 1.1295, 1.0038, 1.0246, 1.0128, 0.9878, 1.0647, 1.0048,\n",
       "                      0.9708, 1.0036, 1.0419, 0.9785, 0.9984, 1.0539, 0.9925, 1.0184, 1.0481,\n",
       "                      1.1022, 1.0474, 1.0116, 1.0603, 0.9751, 1.0143, 1.0250, 1.0037, 1.0165,\n",
       "                      1.0238, 0.9649, 1.0126, 0.9364, 0.9768, 0.9870, 1.0405, 1.0063, 0.9899,\n",
       "                      1.0354, 0.9379, 1.0206, 1.0248, 0.9762, 1.0419, 1.0015, 0.9536, 1.0018,\n",
       "                      0.9691, 1.0036, 1.0323, 1.0300, 1.0001, 1.0295, 1.0072, 1.0611, 1.0564,\n",
       "                      1.0141, 1.0199, 1.0035, 0.9741, 1.0249, 1.0916, 1.0389, 0.9954, 1.0259,\n",
       "                      1.0033, 0.9680, 1.0140, 1.0950, 0.9504, 1.0205, 1.0527, 1.1236, 1.0685,\n",
       "                      1.0264, 0.9911, 1.0404, 1.0118, 0.9940, 1.0148, 1.0921, 1.0209, 0.9667,\n",
       "                      0.9652, 0.9960, 1.0040, 0.9878, 1.1199, 1.0279, 1.0897, 1.0024, 1.0548,\n",
       "                      1.0426, 0.9704, 1.0090, 1.0222, 1.0043, 1.0533, 0.9600, 1.0104, 1.0453,\n",
       "                      0.9653, 0.9915, 1.0496, 1.0072, 1.0401, 0.9784, 0.9928, 1.0563, 1.0793,\n",
       "                      0.9894, 0.9918, 1.0090, 1.0389, 1.0508, 1.0275, 1.0515, 0.9823, 1.0863,\n",
       "                      1.0050, 1.0478, 1.0316, 1.0337, 1.0851, 0.9800, 1.0280, 1.0087, 1.0204,\n",
       "                      1.0100, 1.0925, 1.0486, 0.9551, 1.0054, 1.0279, 0.9781, 0.9945, 1.0101,\n",
       "                      1.0039, 1.0285, 1.0384, 0.9587, 0.9951, 1.0673, 0.9187, 1.0513, 1.0187,\n",
       "                      0.9919, 0.9795, 1.0213, 0.9745, 1.0682, 0.9866, 0.9945, 1.0203, 1.0780,\n",
       "                      1.0407, 0.9847, 1.0385, 1.0952, 0.9974, 0.9892, 1.0271, 1.0530, 0.9917,\n",
       "                      1.0148, 1.0120, 1.0036, 1.0262, 0.9574, 1.0505, 1.0458, 1.0053, 0.9855,\n",
       "                      1.0334, 1.1376, 1.0134, 1.0216, 1.0321, 1.0231, 1.0178, 0.9839, 0.9652,\n",
       "                      1.0047, 0.9574, 0.9706, 1.0472, 0.9939, 1.0268, 1.0434, 0.9879, 1.0304,\n",
       "                      1.0060, 1.0231, 1.0450, 1.0292, 1.0352, 1.0282, 1.0168, 1.0161, 1.0223,\n",
       "                      1.0775, 1.0283, 0.9959, 1.0327, 1.0252, 1.0492, 1.0153, 0.9846, 1.0800,\n",
       "                      1.0534, 1.0044, 1.0406, 0.9795, 1.0253, 1.0294, 0.9650, 1.0048, 1.0189,\n",
       "                      1.0417, 1.0238, 1.1496, 1.0681, 1.0189, 0.9850, 0.9332, 0.9927, 1.0892,\n",
       "                      0.9880, 1.0476, 1.0360, 1.0030, 1.0061, 1.0479, 1.0090, 1.0049, 1.0509,\n",
       "                      1.0615, 0.9458, 1.0086, 1.0568, 1.0144, 1.0281, 0.9453, 1.0063, 0.9551,\n",
       "                      1.0155, 1.0526, 0.9895, 1.0495, 1.0805, 1.0046, 1.0603, 1.0868, 0.9711,\n",
       "                      0.9643, 1.0433, 0.9942, 0.9989], device='cuda:0')),\n",
       "             ('blocks.1.ln1.bias',\n",
       "              tensor([ 0.0075,  0.0265,  0.0211, -0.0067,  0.0310,  0.0008, -0.0983,  0.0275,\n",
       "                      -0.0264, -0.0267,  0.0330,  0.0343,  0.0474,  0.0093, -0.0198, -0.0711,\n",
       "                      -0.0617, -0.0206, -0.0194, -0.0169,  0.0104, -0.0055, -0.0559,  0.0019,\n",
       "                       0.0297,  0.0118,  0.0763,  0.0142, -0.0492,  0.0072, -0.0136,  0.0256,\n",
       "                       0.1620, -0.0386, -0.0163, -0.0204, -0.0171, -0.0928,  0.0451,  0.0027,\n",
       "                       0.0094,  0.0243, -0.0588,  0.0332, -0.0005, -0.0716,  0.0188,  0.0147,\n",
       "                      -0.0464, -0.0068, -0.0386,  0.0600, -0.0338,  0.0347, -0.0613,  0.0257,\n",
       "                      -0.0540, -0.0265,  0.0754,  0.0765,  0.0312, -0.0280,  0.0137,  0.0179,\n",
       "                       0.0101,  0.0315,  0.0571, -0.0464, -0.0410,  0.0298, -0.0078,  0.0035,\n",
       "                      -0.0160,  0.0447, -0.0477,  0.0481,  0.0200, -0.0243,  0.0267, -0.0021,\n",
       "                      -0.0266,  0.0117, -0.0323, -0.0116,  0.0166,  0.0546,  0.0790,  0.0649,\n",
       "                      -0.0012,  0.0610, -0.0190,  0.0198,  0.0111,  0.0237,  0.0170,  0.0184,\n",
       "                       0.0419, -0.0200, -0.0195, -0.0353,  0.0190, -0.0082,  0.0444, -0.0362,\n",
       "                       0.0247, -0.0210,  0.0060, -0.0153, -0.0131, -0.0032,  0.0561,  0.0159,\n",
       "                      -0.0226, -0.0326, -0.0337,  0.0671,  0.0589,  0.0050,  0.0620,  0.0288,\n",
       "                      -0.0272, -0.0107, -0.0591, -0.0431, -0.0147, -0.0003,  0.0211, -0.0323,\n",
       "                       0.0171, -0.0539, -0.0124,  0.0250, -0.0244, -0.0351, -0.1144,  0.0399,\n",
       "                      -0.0181,  0.0156, -0.0587,  0.0341,  0.0092,  0.1203, -0.0391, -0.0245,\n",
       "                      -0.0096, -0.0159,  0.0445, -0.0259, -0.0642, -0.0244,  0.0347, -0.0267,\n",
       "                      -0.0012,  0.0028, -0.0312,  0.0038, -0.0378, -0.0202, -0.0274,  0.0234,\n",
       "                      -0.0113,  0.0150,  0.0401, -0.0268,  0.0022, -0.0712,  0.0084,  0.0145,\n",
       "                      -0.0028, -0.0164,  0.0166,  0.0344, -0.0235, -0.0098,  0.0453,  0.0284,\n",
       "                       0.0425, -0.0686,  0.0248,  0.0258,  0.0136, -0.0477,  0.0121,  0.0558,\n",
       "                      -0.0100,  0.0053, -0.0065,  0.1088, -0.0087, -0.0595, -0.0684, -0.0558,\n",
       "                      -0.0140,  0.0076, -0.0289, -0.0403,  0.0799,  0.0434,  0.0003,  0.0165,\n",
       "                      -0.0116, -0.0463,  0.0193,  0.0364,  0.0299,  0.0379,  0.0166,  0.0161,\n",
       "                      -0.0591,  0.0281,  0.0921, -0.0071, -0.0467,  0.0417, -0.0087,  0.0163,\n",
       "                       0.0585,  0.0409,  0.0148, -0.0070, -0.0078, -0.0108,  0.0353,  0.0079,\n",
       "                      -0.0104, -0.1529, -0.0093, -0.0198, -0.0118, -0.0270,  0.0568,  0.0245,\n",
       "                       0.0205, -0.0288, -0.0116,  0.0704, -0.0644, -0.0565, -0.0199, -0.0215,\n",
       "                      -0.0854, -0.0009, -0.0651, -0.0129,  0.0087, -0.0422,  0.0215, -0.0215,\n",
       "                       0.0134,  0.0024,  0.0029, -0.0072, -0.0123,  0.0214,  0.0018,  0.0401],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.1.ln2.weight',\n",
       "              tensor([0.9970, 0.9507, 0.6397, 1.0317, 1.0218, 1.0825, 0.9612, 1.1082, 0.8730,\n",
       "                      1.0757, 1.0298, 0.7116, 1.0457, 1.0179, 1.0662, 0.7739, 1.0705, 0.7288,\n",
       "                      1.1148, 0.9112, 1.0736, 1.0546, 1.1305, 0.9813, 1.0006, 0.9641, 0.9406,\n",
       "                      1.0968, 1.0953, 1.1014, 1.0496, 0.9747, 0.7716, 1.1830, 1.0623, 1.0161,\n",
       "                      1.0728, 0.9498, 1.0504, 0.9297, 1.0790, 0.8720, 0.9496, 1.0124, 1.1601,\n",
       "                      0.9692, 0.9478, 0.7653, 1.1092, 1.0890, 1.0450, 1.0509, 1.0162, 0.9752,\n",
       "                      1.0057, 1.0902, 1.0779, 1.0196, 0.9494, 1.0625, 1.0899, 1.0776, 1.0595,\n",
       "                      0.9256, 1.0777, 0.9602, 1.0658, 0.9663, 1.0956, 1.0952, 1.0596, 1.0140,\n",
       "                      1.0184, 1.0993, 1.0083, 0.9318, 1.1305, 1.1511, 1.0029, 0.8620, 0.9734,\n",
       "                      0.9957, 1.1406, 1.0907, 1.0797, 1.0191, 0.9691, 0.9949, 1.0410, 1.0729,\n",
       "                      0.8685, 0.9988, 1.0387, 0.9638, 1.0975, 1.0512, 0.9797, 0.9951, 1.0381,\n",
       "                      1.0447, 1.0189, 0.9861, 1.0631, 1.0557, 0.9300, 1.0336, 1.0957, 0.9464,\n",
       "                      1.0084, 0.9967, 0.9966, 1.0564, 0.9917, 0.9956, 1.1296, 0.9796, 1.0861,\n",
       "                      1.0105, 1.0044, 1.0595, 1.0527, 1.0609, 1.0990, 0.9876, 0.9250, 1.0152,\n",
       "                      1.1095, 1.0226, 1.0013, 0.9466, 1.0509, 0.9056, 1.0077, 0.9830, 1.0275,\n",
       "                      0.9439, 1.0335, 0.9980, 0.9463, 1.1432, 1.0049, 1.0257, 1.0843, 1.0523,\n",
       "                      1.0276, 1.1039, 0.9618, 1.0066, 0.9602, 1.0297, 1.0611, 1.0242, 1.0084,\n",
       "                      1.1074, 1.1247, 0.9905, 1.0227, 1.0107, 1.0508, 1.0662, 0.9869, 1.0395,\n",
       "                      0.9814, 0.9927, 1.0564, 0.8262, 1.1310, 0.8558, 0.9112, 1.0994, 1.0942,\n",
       "                      0.8982, 0.6943, 1.0207, 0.9999, 1.1153, 0.9187, 0.8135, 1.0392, 1.0161,\n",
       "                      1.0261, 1.0365, 1.0089, 1.0270, 1.1379, 1.1023, 1.0607, 0.9746, 1.0402,\n",
       "                      1.0039, 0.8535, 0.9595, 1.0277, 1.0888, 0.9487, 1.0482, 1.0620, 0.9127,\n",
       "                      1.0344, 1.1087, 1.0713, 1.0274, 1.1176, 1.1342, 1.0315, 1.0657, 1.1079,\n",
       "                      0.8835, 1.0787, 1.0669, 0.8281, 0.8712, 1.0367, 1.0046, 1.1222, 0.9674,\n",
       "                      0.9504, 1.0507, 0.6327, 1.0098, 1.0894, 1.0216, 0.8503, 1.0158, 1.0180,\n",
       "                      0.9180, 1.0430, 0.9959, 0.9729, 0.9493, 0.9902, 1.0117, 1.0412, 1.0457,\n",
       "                      0.8184, 1.0055, 0.9656, 1.0057, 1.0744, 1.1405, 0.9632, 1.0277, 1.0281,\n",
       "                      1.1283, 1.0313, 0.9810, 0.9773, 1.0368, 1.1182, 1.0661, 1.0156, 1.0243,\n",
       "                      1.0237, 1.0373, 1.0265, 0.9778], device='cuda:0')),\n",
       "             ('blocks.1.ln2.bias',\n",
       "              tensor([ 0.0336, -0.0980,  0.0635, -0.0543,  0.1432, -0.0843, -0.0714, -0.0088,\n",
       "                      -0.0090,  0.0129,  0.0755, -0.0155,  0.0528,  0.0859,  0.0264, -0.0235,\n",
       "                       0.0984,  0.0412, -0.1525,  0.0692, -0.1125,  0.1345, -0.0985,  0.0413,\n",
       "                      -0.0314,  0.0163,  0.0349,  0.0668,  0.0416, -0.0720,  0.0141, -0.0342,\n",
       "                       0.0474, -0.0598, -0.0900,  0.0228,  0.0245,  0.1008,  0.1084, -0.0120,\n",
       "                      -0.0940, -0.1138,  0.0390,  0.0772, -0.1599, -0.0463, -0.0900, -0.0950,\n",
       "                       0.0894,  0.0113, -0.0129, -0.0462, -0.0187, -0.0046, -0.0818,  0.1775,\n",
       "                       0.1284, -0.1261,  0.1595, -0.0325,  0.0726,  0.0107,  0.1194,  0.0181,\n",
       "                      -0.0489,  0.0042,  0.0404, -0.0022, -0.1325,  0.0068, -0.0020, -0.0508,\n",
       "                       0.0361,  0.0887,  0.0182,  0.0724, -0.0009,  0.0909, -0.0410,  0.0041,\n",
       "                       0.1004,  0.0387,  0.0524,  0.0152,  0.0248,  0.0223,  0.0527,  0.0270,\n",
       "                      -0.0047,  0.0637, -0.1703,  0.0633, -0.0718,  0.0305, -0.1488,  0.0619,\n",
       "                      -0.0969, -0.0737, -0.0574,  0.1135,  0.0439, -0.1670,  0.1144,  0.1220,\n",
       "                       0.0345, -0.0225,  0.1357, -0.1244, -0.0124, -0.0316,  0.1383,  0.0239,\n",
       "                       0.1056, -0.1569,  0.0752,  0.0843, -0.0427, -0.0810,  0.0097, -0.0355,\n",
       "                       0.1291,  0.0261, -0.0468, -0.0385, -0.0577,  0.0391, -0.0200, -0.0243,\n",
       "                      -0.0758,  0.0432, -0.0139,  0.0840,  0.0731,  0.0328,  0.1018, -0.0480,\n",
       "                       0.0818,  0.0368, -0.0714, -0.1484,  0.1405, -0.1141,  0.0248, -0.0465,\n",
       "                       0.1201, -0.1965, -0.0428,  0.0669, -0.0823,  0.0807,  0.0548, -0.1703,\n",
       "                       0.0361, -0.0179,  0.0980,  0.0933, -0.0777, -0.0325, -0.0517,  0.0011,\n",
       "                      -0.1517,  0.0226,  0.0522, -0.0135, -0.0172, -0.1143, -0.0331,  0.0421,\n",
       "                      -0.0405,  0.0999, -0.1369,  0.0616, -0.0572, -0.0632,  0.0042, -0.0555,\n",
       "                       0.1198,  0.0135,  0.0129, -0.0543,  0.0858,  0.0422, -0.0429,  0.0920,\n",
       "                      -0.1694,  0.0042,  0.0847, -0.0797, -0.0287, -0.0576, -0.0854, -0.0274,\n",
       "                      -0.1183, -0.1791,  0.1249, -0.0100, -0.0025,  0.1517, -0.0717, -0.0189,\n",
       "                       0.0077, -0.1033,  0.1671, -0.1298, -0.0200, -0.1016,  0.0665,  0.0541,\n",
       "                       0.0592,  0.0671,  0.2479, -0.0512, -0.0165,  0.0328, -0.1339,  0.0325,\n",
       "                      -0.0739,  0.1169,  0.0700,  0.0174, -0.0395,  0.0511,  0.0089, -0.0464,\n",
       "                       0.0105, -0.0270, -0.1434, -0.0354,  0.0749,  0.0142,  0.0892,  0.1780,\n",
       "                       0.0064,  0.0842, -0.0747, -0.0662, -0.0509, -0.0065, -0.0249, -0.0902,\n",
       "                       0.0525, -0.0956,  0.0556, -0.1807,  0.0575,  0.0925,  0.0860, -0.0024,\n",
       "                       0.0147,  0.0473,  0.0551, -0.0793, -0.0416,  0.0575,  0.0189,  0.0055],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.0.key.weight',\n",
       "              tensor([[-0.0918, -0.0608, -0.0040,  ...,  0.0067, -0.0843,  0.0444],\n",
       "                      [-0.0577, -0.0700,  0.0773,  ..., -0.0286,  0.0295, -0.0046],\n",
       "                      [ 0.0928, -0.0272,  0.0974,  ...,  0.0485, -0.0664,  0.0246],\n",
       "                      ...,\n",
       "                      [-0.0454, -0.0440,  0.0570,  ..., -0.0588,  0.0839, -0.0383],\n",
       "                      [-0.0151,  0.0030, -0.0916,  ..., -0.0114,  0.0219, -0.0736],\n",
       "                      [-0.0235, -0.0339, -0.0912,  ..., -0.0285, -0.0965,  0.0147]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.0.query.weight',\n",
       "              tensor([[ 0.1203, -0.0225,  0.0087,  ..., -0.0350,  0.0064,  0.0187],\n",
       "                      [-0.0322,  0.0157, -0.0225,  ...,  0.0683,  0.0509, -0.0314],\n",
       "                      [-0.0107,  0.0663,  0.0480,  ...,  0.0551,  0.0350, -0.0945],\n",
       "                      ...,\n",
       "                      [ 0.0816,  0.0320, -0.0600,  ..., -0.0125,  0.0760, -0.0408],\n",
       "                      [ 0.0441, -0.0471, -0.0762,  ...,  0.0265, -0.0327,  0.0536],\n",
       "                      [-0.0088,  0.0800,  0.0291,  ..., -0.0421,  0.0954,  0.0862]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.0.value.weight',\n",
       "              tensor([[-0.0620,  0.0462, -0.0508,  ..., -0.0341, -0.0591, -0.0612],\n",
       "                      [-0.1082, -0.0144, -0.0506,  ..., -0.0148, -0.0408, -0.0449],\n",
       "                      [-0.0471, -0.0206,  0.0150,  ..., -0.0216, -0.0565,  0.0938],\n",
       "                      ...,\n",
       "                      [ 0.0339, -0.0151,  0.0095,  ..., -0.0145, -0.0302,  0.0773],\n",
       "                      [ 0.0960,  0.0830, -0.0354,  ..., -0.0553,  0.0731,  0.0706],\n",
       "                      [ 0.1006, -0.0204, -0.0067,  ...,  0.0081,  0.0709,  0.0009]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.1.key.weight',\n",
       "              tensor([[-0.0355, -0.0272,  0.0174,  ...,  0.0346, -0.0362, -0.0389],\n",
       "                      [-0.0082,  0.0390,  0.1301,  ..., -0.0614,  0.0185,  0.0151],\n",
       "                      [-0.0203, -0.0237, -0.0687,  ...,  0.0774,  0.0432,  0.0364],\n",
       "                      ...,\n",
       "                      [-0.0169, -0.1647, -0.1157,  ..., -0.0150, -0.0342,  0.0191],\n",
       "                      [ 0.0060, -0.0393, -0.1339,  ...,  0.1129,  0.0105,  0.0505],\n",
       "                      [ 0.0629, -0.0191,  0.1219,  ..., -0.0250, -0.0084, -0.0869]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.1.query.weight',\n",
       "              tensor([[-0.0417, -0.0504, -0.0040,  ..., -0.0415,  0.0024, -0.0256],\n",
       "                      [-0.0080, -0.0475,  0.0137,  ..., -0.0436, -0.0262, -0.0524],\n",
       "                      [ 0.0146, -0.0131,  0.0369,  ..., -0.1208, -0.0454,  0.1148],\n",
       "                      ...,\n",
       "                      [ 0.0388, -0.0157, -0.0548,  ..., -0.0459,  0.0329,  0.0322],\n",
       "                      [ 0.0130,  0.0280, -0.0147,  ..., -0.0155,  0.0842,  0.0131],\n",
       "                      [-0.0734, -0.1147,  0.0112,  ...,  0.0722, -0.1138, -0.0117]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.1.value.weight',\n",
       "              tensor([[-0.0697,  0.0078, -0.0621,  ..., -0.0472, -0.0594,  0.0415],\n",
       "                      [-0.0415, -0.0298, -0.0004,  ...,  0.0052, -0.0131,  0.0408],\n",
       "                      [ 0.0147, -0.0533,  0.0415,  ...,  0.0639,  0.1111,  0.0422],\n",
       "                      ...,\n",
       "                      [-0.0370, -0.0179, -0.0616,  ...,  0.0161,  0.1570,  0.1106],\n",
       "                      [ 0.0656,  0.0078, -0.0271,  ..., -0.0660,  0.0536, -0.0230],\n",
       "                      [-0.0840,  0.0395,  0.0801,  ..., -0.1153,  0.0610,  0.0290]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.2.key.weight',\n",
       "              tensor([[-0.1551,  0.0420,  0.1516,  ...,  0.0568, -0.0071, -0.0076],\n",
       "                      [ 0.0352, -0.1000, -0.1454,  ...,  0.0510, -0.0370, -0.1170],\n",
       "                      [-0.0098,  0.0204,  0.1023,  ...,  0.0404, -0.1359, -0.0252],\n",
       "                      ...,\n",
       "                      [ 0.1575,  0.0643,  0.0412,  ..., -0.0823, -0.0699, -0.1028],\n",
       "                      [ 0.0296,  0.0908, -0.0717,  ..., -0.0337, -0.1069,  0.0343],\n",
       "                      [ 0.0479,  0.0517,  0.1570,  ..., -0.0208,  0.0276,  0.0030]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.2.query.weight',\n",
       "              tensor([[-0.0032,  0.0404, -0.0509,  ..., -0.0515,  0.0743, -0.0206],\n",
       "                      [-0.0116,  0.0208,  0.0882,  ..., -0.0877,  0.0915,  0.1187],\n",
       "                      [-0.0066,  0.0128,  0.0312,  ...,  0.0211,  0.0495, -0.1004],\n",
       "                      ...,\n",
       "                      [-0.0007,  0.0519,  0.0167,  ..., -0.0167, -0.0693,  0.1726],\n",
       "                      [ 0.0160,  0.0757,  0.0137,  ..., -0.1113,  0.0107, -0.1719],\n",
       "                      [ 0.0113,  0.0290,  0.1734,  ..., -0.0718,  0.0477,  0.1122]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.2.value.weight',\n",
       "              tensor([[ 0.0457, -0.0130, -0.0640,  ..., -0.1069,  0.0340,  0.0005],\n",
       "                      [ 0.0416, -0.0222, -0.0066,  ..., -0.1486, -0.0903,  0.0037],\n",
       "                      [ 0.0246, -0.0228, -0.0465,  ...,  0.0178,  0.0838,  0.0314],\n",
       "                      ...,\n",
       "                      [ 0.0794, -0.0081,  0.0400,  ...,  0.0686,  0.0216, -0.0636],\n",
       "                      [ 0.0259,  0.0213,  0.0839,  ...,  0.0308,  0.0794, -0.0003],\n",
       "                      [-0.0504, -0.0600,  0.0178,  ..., -0.0119,  0.0056,  0.0347]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.3.key.weight',\n",
       "              tensor([[ 0.0037, -0.0153, -0.1689,  ..., -0.0033, -0.0904, -0.0078],\n",
       "                      [-0.0630,  0.0942,  0.0780,  ...,  0.0346, -0.0320,  0.1015],\n",
       "                      [ 0.0635, -0.0128,  0.0649,  ...,  0.0347,  0.0155,  0.1083],\n",
       "                      ...,\n",
       "                      [ 0.1579, -0.0598, -0.0961,  ...,  0.0697, -0.0185, -0.0021],\n",
       "                      [ 0.0285, -0.0201, -0.0957,  ..., -0.0633, -0.0032, -0.1522],\n",
       "                      [ 0.0999, -0.0583, -0.1769,  ..., -0.0758, -0.0699, -0.0102]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.3.query.weight',\n",
       "              tensor([[ 0.1090, -0.0126, -0.0657,  ...,  0.1248, -0.0564,  0.0516],\n",
       "                      [-0.0504, -0.0418,  0.1238,  ..., -0.0782, -0.0182, -0.0916],\n",
       "                      [-0.0656,  0.0129, -0.0118,  ..., -0.0658,  0.0957, -0.0244],\n",
       "                      ...,\n",
       "                      [-0.0157,  0.0322,  0.0417,  ...,  0.0145,  0.0630,  0.1003],\n",
       "                      [-0.0555, -0.0415,  0.0943,  ...,  0.0859, -0.1568,  0.0442],\n",
       "                      [-0.0906,  0.0049, -0.0594,  ...,  0.1429,  0.1051,  0.0467]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.heads.3.value.weight',\n",
       "              tensor([[ 0.1049, -0.0505, -0.0322,  ..., -0.0907, -0.0239,  0.0165],\n",
       "                      [-0.0693,  0.0879,  0.0145,  ..., -0.0691,  0.1242,  0.0149],\n",
       "                      [-0.0851,  0.0252, -0.0943,  ..., -0.0899, -0.0187, -0.0142],\n",
       "                      ...,\n",
       "                      [ 0.1865,  0.0221,  0.0755,  ...,  0.0649,  0.1327, -0.0340],\n",
       "                      [-0.0198,  0.0017, -0.0585,  ..., -0.0006, -0.0360, -0.1084],\n",
       "                      [-0.0514, -0.0312, -0.0048,  ...,  0.0161, -0.0275, -0.0423]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.proj.weight',\n",
       "              tensor([[-0.0290,  0.0091, -0.0315,  ..., -0.0492, -0.1028,  0.1341],\n",
       "                      [-0.0703, -0.0064, -0.0258,  ..., -0.0637,  0.0883, -0.0184],\n",
       "                      [ 0.0255, -0.0071,  0.0496,  ..., -0.0013,  0.0255, -0.2039],\n",
       "                      ...,\n",
       "                      [-0.0655,  0.0307,  0.0377,  ..., -0.0047, -0.0023, -0.0050],\n",
       "                      [-0.0058,  0.0018,  0.0379,  ..., -0.0457, -0.0175, -0.0326],\n",
       "                      [ 0.0095,  0.0514,  0.0574,  ...,  0.0095, -0.0158,  0.0219]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.sa.proj.bias',\n",
       "              tensor([-0.0382, -0.0118,  0.0085,  0.0069, -0.0010, -0.0022, -0.0545, -0.0628,\n",
       "                      -0.0318,  0.0489,  0.0085,  0.0362,  0.0658,  0.0103, -0.0388, -0.0293,\n",
       "                       0.0076, -0.0788, -0.0159,  0.0718, -0.0303,  0.0546, -0.0538, -0.0412,\n",
       "                       0.0254, -0.0439, -0.0482,  0.0360, -0.0235,  0.0146,  0.0206,  0.0396,\n",
       "                       0.0367, -0.0657,  0.0078, -0.0134, -0.0123,  0.0365,  0.0446,  0.0653,\n",
       "                       0.0286,  0.0248, -0.0265, -0.0138,  0.0464, -0.0540, -0.0648, -0.0199,\n",
       "                      -0.0338,  0.0075,  0.0262,  0.0246,  0.0011,  0.0128, -0.0089,  0.0531,\n",
       "                      -0.0116,  0.0210, -0.0308,  0.0373, -0.0182,  0.0338,  0.0116,  0.0365,\n",
       "                      -0.0191,  0.0269,  0.0172, -0.0148, -0.0334, -0.0103,  0.0378, -0.0095,\n",
       "                       0.0714, -0.0079,  0.0407, -0.0219, -0.0511,  0.0469, -0.0333, -0.0156,\n",
       "                      -0.0175, -0.0512, -0.0215,  0.0025, -0.0210, -0.0258,  0.0562,  0.0583,\n",
       "                       0.0330, -0.0027, -0.0387, -0.0190,  0.0332, -0.0103,  0.0279, -0.0173,\n",
       "                      -0.0188, -0.0084, -0.0430,  0.0174, -0.0177, -0.0167, -0.0302, -0.0486,\n",
       "                      -0.0260, -0.0485,  0.0439, -0.0226,  0.0477,  0.0644,  0.0267,  0.0499,\n",
       "                       0.0110, -0.0230, -0.0218,  0.0156, -0.0447, -0.0132,  0.0191, -0.0480,\n",
       "                       0.0081, -0.0219, -0.0443,  0.0353,  0.0107,  0.0117, -0.0271,  0.0341,\n",
       "                       0.0315, -0.0793, -0.0216,  0.0569,  0.0538, -0.0416,  0.0026,  0.0086,\n",
       "                       0.0155,  0.0137, -0.0791,  0.0321, -0.0381,  0.0272, -0.0002,  0.0186,\n",
       "                       0.0437, -0.0535, -0.0585,  0.0200, -0.0160,  0.0420,  0.0244,  0.0046,\n",
       "                      -0.0601, -0.0227,  0.0084, -0.0068, -0.0481,  0.0199, -0.0516, -0.0341,\n",
       "                      -0.0227,  0.0309,  0.0795, -0.0700, -0.0310,  0.0015, -0.0271,  0.0488,\n",
       "                      -0.0047, -0.0291, -0.0578,  0.0604,  0.0092,  0.0218, -0.0413, -0.0372,\n",
       "                       0.0290, -0.0335, -0.0501,  0.0205,  0.0066,  0.0031,  0.0103,  0.0548,\n",
       "                       0.0414,  0.0069,  0.0266,  0.0257,  0.0344,  0.0172,  0.0342,  0.0448,\n",
       "                      -0.0783,  0.0233, -0.0553,  0.0353,  0.0148,  0.0057,  0.0077,  0.0310,\n",
       "                      -0.0735, -0.0846,  0.0421,  0.0140, -0.0441,  0.0267,  0.0497,  0.0151,\n",
       "                      -0.0622,  0.0509,  0.0088, -0.0591, -0.0237, -0.0075,  0.0449,  0.0235,\n",
       "                       0.0614,  0.0139, -0.0035, -0.0120,  0.0616, -0.0349,  0.0520,  0.0776,\n",
       "                       0.0136,  0.0123, -0.0355, -0.0269, -0.0220, -0.0457,  0.0393,  0.0579,\n",
       "                       0.0282, -0.0711,  0.0577, -0.0381, -0.0723, -0.0421, -0.0534,  0.0381,\n",
       "                       0.0043, -0.0098,  0.0219,  0.0338,  0.0385, -0.0203, -0.0171,  0.0189,\n",
       "                       0.0170, -0.0022, -0.0476, -0.0303, -0.0177,  0.0043,  0.0206,  0.0136],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ffwd.net.0.weight',\n",
       "              tensor([[ 2.9658e-02,  7.8119e-03, -3.6330e-02,  ..., -6.1233e-02,\n",
       "                        1.0403e-01, -2.3969e-02],\n",
       "                      [ 5.5644e-02, -5.5767e-02, -1.1232e-01,  ...,  1.1829e-03,\n",
       "                        1.5571e-02,  2.9329e-02],\n",
       "                      [ 1.4081e-01,  6.5732e-02, -4.1991e-02,  ...,  5.3098e-02,\n",
       "                        3.4836e-02,  1.4808e-02],\n",
       "                      ...,\n",
       "                      [-3.4334e-02,  8.3160e-02, -4.0543e-02,  ..., -5.6151e-02,\n",
       "                       -1.0216e-01, -1.6569e-02],\n",
       "                      [ 6.3397e-02,  5.5563e-02, -2.6911e-02,  ...,  1.0281e-01,\n",
       "                       -6.5971e-02, -5.3108e-02],\n",
       "                      [-8.3873e-02, -7.1821e-05, -6.5271e-02,  ..., -3.9016e-02,\n",
       "                       -2.9911e-02, -3.7934e-02]], device='cuda:0')),\n",
       "             ('blocks.2.ffwd.net.0.bias',\n",
       "              tensor([-0.0808, -0.0425,  0.0018,  ..., -0.0611, -0.0020,  0.0146],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ffwd.net.2.weight',\n",
       "              tensor([[-0.0079, -0.0180,  0.0505,  ...,  0.0137, -0.0792,  0.0026],\n",
       "                      [ 0.0745,  0.0919,  0.0141,  ..., -0.0036,  0.0235,  0.0056],\n",
       "                      [-0.0284,  0.1209, -0.0008,  ..., -0.0392,  0.0034,  0.0005],\n",
       "                      ...,\n",
       "                      [ 0.0108,  0.0652,  0.0017,  ...,  0.0024, -0.0101,  0.0664],\n",
       "                      [ 0.0007,  0.0272, -0.0740,  ..., -0.0282,  0.0692, -0.0424],\n",
       "                      [-0.0430, -0.0347,  0.0397,  ...,  0.0145, -0.0658, -0.0055]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ffwd.net.2.bias',\n",
       "              tensor([-0.0036, -0.0278, -0.0164,  0.0103,  0.0035,  0.0376, -0.0182,  0.0263,\n",
       "                      -0.0092,  0.0238, -0.0283,  0.0290,  0.0011,  0.0122,  0.0129,  0.0223,\n",
       "                      -0.0060,  0.0142,  0.0029, -0.0178,  0.0147, -0.0429,  0.0315,  0.0134,\n",
       "                       0.0148, -0.0184,  0.0087,  0.0124,  0.0058, -0.0146,  0.0183,  0.0184,\n",
       "                      -0.0017, -0.0298, -0.0005,  0.0031,  0.0005, -0.0329,  0.0020,  0.0030,\n",
       "                      -0.0292,  0.0310, -0.0246, -0.0048,  0.0151,  0.0058,  0.0001, -0.0144,\n",
       "                      -0.0327, -0.0330,  0.0062, -0.0090,  0.0037,  0.0008,  0.0257, -0.0203,\n",
       "                      -0.0321, -0.0214,  0.0013,  0.0441,  0.0178, -0.0300, -0.0417,  0.0114,\n",
       "                       0.0148, -0.0217, -0.0096,  0.0053,  0.0173, -0.0189,  0.0257,  0.0159,\n",
       "                      -0.0175,  0.0069, -0.0003,  0.0129,  0.0291,  0.0224,  0.0387,  0.0121,\n",
       "                      -0.0051,  0.0036,  0.0213, -0.0165,  0.0335, -0.0072,  0.0146, -0.0043,\n",
       "                       0.0167, -0.0007,  0.0065, -0.0197, -0.0144,  0.0153, -0.0090, -0.0280,\n",
       "                       0.0353,  0.0302,  0.0125, -0.0117,  0.0192, -0.0134, -0.0216, -0.0274,\n",
       "                      -0.0104,  0.0153, -0.0431,  0.0026, -0.0027,  0.0035, -0.0372,  0.0006,\n",
       "                       0.0435,  0.0344,  0.0004, -0.0037, -0.0072,  0.0329, -0.0273, -0.0027,\n",
       "                       0.0043, -0.0073,  0.0240,  0.0496, -0.0446, -0.0167, -0.0290,  0.0283,\n",
       "                      -0.0051,  0.0024, -0.0175, -0.0188,  0.0097, -0.0150, -0.0064, -0.0286,\n",
       "                      -0.0249, -0.0379,  0.0248,  0.0169, -0.0227,  0.0107, -0.0046, -0.0268,\n",
       "                      -0.0254,  0.0121, -0.0260, -0.0513,  0.0072,  0.0038,  0.0016,  0.0010,\n",
       "                      -0.0205,  0.0120,  0.0061,  0.0020,  0.0278,  0.0090,  0.0007,  0.0060,\n",
       "                       0.0322,  0.0035,  0.0196,  0.0045,  0.0081,  0.0329,  0.0031,  0.0160,\n",
       "                       0.0160, -0.0221,  0.0185, -0.0207,  0.0041,  0.0227,  0.0238,  0.0185,\n",
       "                      -0.0363,  0.0139,  0.0198, -0.0033, -0.0182, -0.0128,  0.0329, -0.0323,\n",
       "                       0.0353,  0.0145,  0.0031,  0.0179,  0.0214, -0.0228,  0.0339,  0.0021,\n",
       "                       0.0197,  0.0135, -0.0127, -0.0176, -0.0106,  0.0010,  0.0207,  0.0110,\n",
       "                      -0.0025,  0.0095, -0.0375, -0.0145, -0.0056, -0.0181, -0.0156, -0.0026,\n",
       "                       0.0175, -0.0226, -0.0173,  0.0393, -0.0194,  0.0120, -0.0158, -0.0435,\n",
       "                      -0.0072,  0.0043,  0.0381, -0.0100,  0.0201, -0.0041,  0.0203, -0.0208,\n",
       "                       0.0051,  0.0206,  0.0250, -0.0144,  0.0097,  0.0115,  0.0196, -0.0183,\n",
       "                      -0.0049,  0.0093,  0.0632,  0.0316,  0.0044,  0.0324, -0.0076,  0.0468,\n",
       "                      -0.0267, -0.0046,  0.0069,  0.0018, -0.0230, -0.0301, -0.0501,  0.0340,\n",
       "                      -0.0349, -0.0150, -0.0177, -0.0028,  0.0306, -0.0211, -0.0324,  0.0278],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ln1.weight',\n",
       "              tensor([1.0400, 0.9134, 1.0842, 0.9754, 0.9785, 1.0324, 0.9929, 1.0475, 0.9902,\n",
       "                      0.9749, 0.9939, 1.0674, 0.9794, 0.9860, 0.9854, 1.0024, 0.9637, 1.0823,\n",
       "                      0.9847, 0.9796, 0.9934, 0.9904, 0.9601, 0.9793, 0.9781, 1.0284, 1.0413,\n",
       "                      0.9763, 1.0136, 0.9802, 1.0078, 0.9993, 0.8937, 0.9825, 0.9943, 1.0364,\n",
       "                      0.9845, 0.9057, 1.0112, 1.0405, 0.9958, 0.9986, 0.9812, 0.9975, 1.0176,\n",
       "                      0.9914, 0.9723, 1.0442, 1.0495, 0.9791, 0.9968, 0.9045, 0.9696, 0.9756,\n",
       "                      0.9735, 0.9814, 0.9309, 1.0089, 0.9410, 1.0220, 0.9994, 0.9929, 0.9594,\n",
       "                      0.9446, 1.0120, 0.9972, 1.0283, 0.9474, 0.9597, 1.0246, 1.0577, 0.9958,\n",
       "                      1.0004, 1.0352, 1.0152, 0.9929, 1.0156, 1.0582, 1.0550, 0.9813, 0.9473,\n",
       "                      0.9738, 1.0182, 0.9699, 0.9982, 0.9955, 1.0468, 0.9870, 1.0033, 1.0375,\n",
       "                      1.0595, 0.9450, 0.9853, 0.9927, 0.9513, 1.1067, 0.9496, 0.9845, 0.9918,\n",
       "                      0.9936, 0.9738, 0.9582, 0.9981, 0.9878, 0.9136, 1.0161, 0.9858, 1.0208,\n",
       "                      0.9803, 0.9683, 0.9473, 0.9709, 1.0158, 1.0244, 1.0521, 0.9606, 1.0332,\n",
       "                      1.0517, 0.9784, 1.0333, 0.9504, 1.0455, 1.0579, 0.9651, 1.0456, 0.9461,\n",
       "                      1.0444, 0.9610, 0.9172, 0.9399, 0.9712, 1.0034, 0.9305, 0.9512, 0.9763,\n",
       "                      0.9983, 0.9988, 0.9674, 0.9789, 1.0371, 1.0731, 0.9365, 1.0475, 0.9598,\n",
       "                      1.0159, 1.0391, 1.0173, 0.9844, 1.0186, 1.0606, 0.9540, 0.9794, 0.9857,\n",
       "                      0.9828, 0.9629, 1.0014, 0.9887, 0.9640, 1.0137, 0.9965, 1.0185, 1.0287,\n",
       "                      0.9611, 0.9568, 0.9990, 1.0032, 1.0011, 1.0209, 0.9996, 0.9470, 0.9656,\n",
       "                      1.0044, 1.0903, 1.0175, 0.9487, 0.9399, 1.0417, 1.0642, 1.0124, 0.9980,\n",
       "                      0.9927, 0.9549, 0.9872, 1.0175, 0.9537, 1.0239, 0.9948, 1.0069, 1.0226,\n",
       "                      1.0276, 1.0141, 1.0222, 1.0054, 0.9554, 0.9312, 1.0421, 1.0611, 1.0326,\n",
       "                      1.0964, 1.0070, 0.9638, 1.0056, 0.9868, 0.9677, 0.9711, 0.9370, 1.0315,\n",
       "                      1.0372, 0.9611, 0.9607, 0.9921, 0.9723, 0.9733, 1.0054, 1.0228, 1.0001,\n",
       "                      1.0091, 0.9682, 1.1471, 0.9945, 1.0388, 0.9633, 0.9173, 1.0192, 0.9906,\n",
       "                      1.0294, 0.9573, 0.9636, 0.9864, 0.9697, 0.9454, 0.9769, 1.0320, 1.0286,\n",
       "                      1.0021, 0.9372, 1.0057, 1.0442, 1.0091, 1.0491, 0.9592, 1.0124, 0.9247,\n",
       "                      0.9642, 1.0151, 0.9466, 0.9851, 1.0115, 1.0032, 0.9796, 0.9953, 1.0194,\n",
       "                      0.9583, 1.0470, 1.0255, 0.9444], device='cuda:0')),\n",
       "             ('blocks.2.ln1.bias',\n",
       "              tensor([-0.0071,  0.0969,  0.0576,  0.0520,  0.0192,  0.0151, -0.0109,  0.0490,\n",
       "                      -0.0095, -0.0178,  0.0612,  0.0283,  0.0267, -0.0411, -0.0347, -0.0275,\n",
       "                      -0.0724, -0.0714,  0.0197, -0.0320,  0.0049, -0.0626, -0.0181, -0.0063,\n",
       "                       0.0317,  0.0069,  0.0470, -0.0126, -0.0433,  0.0087, -0.0285,  0.0209,\n",
       "                       0.0741,  0.0198,  0.0248,  0.0067, -0.0215, -0.0976,  0.0198,  0.0342,\n",
       "                       0.0287,  0.0304, -0.0585,  0.0224,  0.0370, -0.0643,  0.0516,  0.0167,\n",
       "                      -0.0282,  0.0390, -0.0239,  0.0732,  0.0016,  0.0499, -0.0985,  0.0331,\n",
       "                      -0.0452, -0.0143,  0.0313,  0.0217, -0.0136, -0.0164, -0.0315,  0.0451,\n",
       "                       0.0078,  0.0646,  0.0066, -0.0569,  0.0425,  0.0055,  0.0077,  0.0309,\n",
       "                      -0.0434,  0.0031, -0.0607,  0.0305,  0.0112, -0.0371,  0.0170, -0.0396,\n",
       "                      -0.0795, -0.0045, -0.0267, -0.0163,  0.0115,  0.0397,  0.0079,  0.0259,\n",
       "                      -0.0273,  0.0267, -0.0086,  0.0716,  0.0378,  0.0142,  0.0258, -0.0081,\n",
       "                       0.0448, -0.0195, -0.0018, -0.0111, -0.0490, -0.0129,  0.0155, -0.0082,\n",
       "                       0.0604, -0.0392, -0.0244,  0.0266,  0.0143, -0.0342,  0.0504,  0.0098,\n",
       "                      -0.0227, -0.0419, -0.0479,  0.0518,  0.0042, -0.0340,  0.0748, -0.0096,\n",
       "                      -0.0341,  0.0106,  0.0027, -0.0033, -0.0072, -0.0253, -0.0208,  0.0269,\n",
       "                       0.0684, -0.0991, -0.0470, -0.0137, -0.0286, -0.0814, -0.0437,  0.0689,\n",
       "                      -0.0413, -0.0254, -0.0272, -0.0075,  0.0283,  0.0794, -0.0336, -0.0027,\n",
       "                      -0.0102, -0.0091,  0.0268, -0.0287, -0.0180, -0.0218, -0.0078, -0.0322,\n",
       "                       0.0479, -0.0041, -0.0268, -0.0065,  0.0143,  0.0157, -0.0391,  0.0009,\n",
       "                       0.0174,  0.0185,  0.0072, -0.0159,  0.0269, -0.0469, -0.0282, -0.0189,\n",
       "                       0.0070, -0.0741,  0.0403,  0.0699, -0.0272, -0.0015,  0.0828,  0.0725,\n",
       "                       0.0115, -0.0470, -0.0242,  0.0051, -0.0129, -0.0606, -0.0146,  0.0320,\n",
       "                      -0.0480, -0.0097, -0.0306,  0.1025,  0.0082, -0.0438, -0.0723, -0.0284,\n",
       "                       0.0443,  0.0308, -0.0910, -0.0141,  0.0173, -0.0024,  0.0089,  0.0343,\n",
       "                      -0.0184,  0.0355, -0.0224,  0.0364,  0.0219,  0.0576, -0.0017, -0.0177,\n",
       "                      -0.0223,  0.0266,  0.0791,  0.0014, -0.0370,  0.0310,  0.0066,  0.0044,\n",
       "                       0.0631, -0.0302,  0.0220, -0.0125, -0.0166, -0.0124,  0.0692,  0.0543,\n",
       "                       0.0537, -0.0718,  0.0292,  0.0216, -0.0308, -0.0615, -0.0144, -0.0032,\n",
       "                      -0.0101,  0.0096, -0.0245,  0.0917, -0.0264,  0.0008,  0.0140,  0.0225,\n",
       "                      -0.1389,  0.0304, -0.0614,  0.0160,  0.0330, -0.0699, -0.0242, -0.0138,\n",
       "                      -0.0726, -0.0065, -0.0051, -0.0025, -0.0430,  0.0289, -0.0148,  0.0567],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.2.ln2.weight',\n",
       "              tensor([1.0877, 0.9586, 0.9771, 1.1800, 1.1377, 1.1682, 1.0977, 1.1881, 1.0234,\n",
       "                      1.1087, 1.1053, 0.6151, 1.2170, 1.1151, 1.1561, 0.9355, 1.1581, 0.6756,\n",
       "                      1.2148, 1.0678, 1.2723, 1.0746, 1.1793, 1.1541, 1.1901, 1.1062, 1.0981,\n",
       "                      1.1993, 1.2305, 1.2223, 1.1307, 1.0380, 0.9287, 1.3063, 1.1417, 1.1687,\n",
       "                      1.1509, 1.0524, 1.1901, 1.1087, 1.0957, 1.0757, 1.1151, 1.1722, 1.2050,\n",
       "                      1.1372, 1.0589, 0.8207, 1.2853, 1.2928, 1.0992, 1.1218, 1.1293, 1.0281,\n",
       "                      1.0571, 1.1167, 1.1686, 1.1295, 1.0962, 1.3335, 1.1843, 1.2013, 1.0918,\n",
       "                      1.0876, 1.1854, 1.1187, 1.2130, 1.0282, 1.1802, 1.2254, 1.3091, 1.0715,\n",
       "                      1.2680, 1.2070, 1.1718, 1.0492, 1.2295, 1.2094, 1.1188, 0.9140, 1.0541,\n",
       "                      1.1764, 1.1775, 1.1910, 1.1164, 1.2059, 1.0698, 1.1195, 1.1766, 1.1814,\n",
       "                      1.1706, 1.1461, 1.1837, 1.1233, 1.1890, 1.1627, 1.1821, 1.1593, 1.1746,\n",
       "                      1.1302, 1.1463, 1.1619, 1.1404, 1.1964, 1.1068, 1.1139, 1.2835, 1.0684,\n",
       "                      1.1403, 1.1290, 1.1055, 1.2127, 1.2114, 1.1851, 1.2044, 1.1187, 1.2642,\n",
       "                      1.1766, 1.1614, 1.1704, 1.1318, 1.1682, 1.1712, 1.1127, 1.0141, 1.1340,\n",
       "                      1.1542, 1.2053, 1.0857, 1.0869, 1.0843, 1.0327, 1.1374, 1.0894, 1.1350,\n",
       "                      0.9991, 1.1399, 1.0901, 1.0389, 1.1687, 1.2260, 1.0675, 1.1956, 1.1221,\n",
       "                      1.1316, 1.1243, 1.1051, 1.2152, 1.1101, 1.1600, 1.1531, 1.0358, 1.0488,\n",
       "                      1.2157, 1.1898, 1.0816, 1.1581, 1.1271, 1.1108, 1.1736, 1.1591, 1.2487,\n",
       "                      1.1797, 1.2062, 1.1913, 1.0803, 1.2073, 0.9780, 0.9782, 1.2025, 1.1940,\n",
       "                      1.0125, 0.7856, 1.1791, 1.2546, 1.2271, 1.0053, 0.9794, 1.1014, 1.1388,\n",
       "                      1.1102, 1.1256, 1.1924, 1.1345, 1.1715, 1.1769, 1.1438, 1.0924, 1.1514,\n",
       "                      1.1247, 1.0467, 1.1160, 1.2001, 1.1948, 1.0710, 1.1985, 1.2706, 1.1239,\n",
       "                      1.0833, 1.2641, 1.2051, 1.2053, 1.2293, 1.2381, 1.1992, 1.1854, 1.1538,\n",
       "                      0.9589, 1.2861, 1.1132, 1.0739, 0.9905, 1.1310, 1.2257, 1.1946, 1.0307,\n",
       "                      1.1428, 1.1899, 0.7060, 1.0893, 1.1705, 1.1230, 1.0462, 1.0420, 1.1853,\n",
       "                      1.1108, 1.1729, 1.1195, 1.0499, 1.1542, 1.1787, 1.1071, 1.2333, 1.3734,\n",
       "                      1.0523, 1.1437, 1.1386, 1.0803, 1.1279, 1.1427, 1.0664, 1.1271, 1.1239,\n",
       "                      1.1724, 1.2168, 1.0895, 1.2000, 1.1907, 1.3316, 1.0907, 1.1421, 1.1633,\n",
       "                      1.0933, 1.1838, 1.1548, 1.0534], device='cuda:0')),\n",
       "             ('blocks.2.ln2.bias',\n",
       "              tensor([-0.0372, -0.0970,  0.0032, -0.0566,  0.0304, -0.0230, -0.1081, -0.0340,\n",
       "                       0.0089,  0.0215,  0.0351, -0.0018,  0.1177,  0.0608,  0.0330, -0.0041,\n",
       "                       0.1356,  0.0333, -0.1745,  0.1016, -0.1229,  0.1502, -0.0788,  0.0461,\n",
       "                      -0.0150, -0.0210, -0.0597,  0.0401,  0.0115, -0.0467,  0.0587,  0.0884,\n",
       "                      -0.0072, -0.0499, -0.0580,  0.0055,  0.0194,  0.0953,  0.0473, -0.0182,\n",
       "                       0.0065, -0.1448,  0.1140,  0.0685, -0.0835,  0.0408, -0.0581, -0.0900,\n",
       "                       0.1281,  0.0986,  0.0258, -0.1131, -0.0295, -0.0248,  0.0668,  0.1427,\n",
       "                       0.1521,  0.0044,  0.0935, -0.1937,  0.0534,  0.0656,  0.0154, -0.0115,\n",
       "                       0.0185, -0.0245, -0.0315,  0.0864, -0.0683,  0.0042, -0.1272, -0.0851,\n",
       "                       0.0313,  0.0733, -0.0195,  0.0507, -0.0850,  0.1196, -0.0589,  0.0096,\n",
       "                       0.0553,  0.0300,  0.0457,  0.0391,  0.0151,  0.0318, -0.0238, -0.0078,\n",
       "                       0.0376, -0.0278, -0.1849,  0.1013, -0.0564,  0.0218, -0.1190,  0.1134,\n",
       "                      -0.1403, -0.0704, -0.0285,  0.0716,  0.0606, -0.1237,  0.0627, -0.0381,\n",
       "                      -0.0367,  0.1062,  0.1957, -0.0389, -0.0578, -0.0251,  0.0637, -0.0013,\n",
       "                      -0.1076, -0.1007, -0.0513, -0.0084,  0.0146, -0.0077, -0.0201,  0.0614,\n",
       "                       0.1049, -0.0290, -0.1006, -0.0626,  0.0195,  0.0374,  0.0434, -0.0316,\n",
       "                      -0.0802,  0.0416,  0.0089,  0.0851,  0.0986,  0.1448,  0.1424, -0.0556,\n",
       "                       0.1433,  0.0410, -0.0805, -0.0570,  0.1260, -0.0545,  0.0175, -0.0378,\n",
       "                       0.0757, -0.0472, -0.0108,  0.1558, -0.0626,  0.1144,  0.0210,  0.0113,\n",
       "                      -0.0850,  0.0252,  0.0502,  0.0835, -0.0748, -0.0237,  0.0266, -0.0438,\n",
       "                      -0.1959,  0.0318,  0.1276, -0.1250,  0.0108, -0.1184, -0.0355,  0.0104,\n",
       "                       0.0373,  0.1153, -0.1004,  0.0214,  0.0004, -0.0712, -0.1310, -0.1822,\n",
       "                       0.1174,  0.0565,  0.0135, -0.0111,  0.1894,  0.1043, -0.0440,  0.0751,\n",
       "                      -0.1209, -0.0711, -0.0197, -0.0710, -0.0841,  0.0114, -0.0395, -0.1207,\n",
       "                      -0.1372, -0.1516,  0.0317,  0.0468,  0.0966,  0.1269, -0.0068, -0.0626,\n",
       "                      -0.1181, -0.1425,  0.1415, -0.0938,  0.0388, -0.1610,  0.0706,  0.0160,\n",
       "                      -0.0705,  0.0200,  0.0832, -0.0692, -0.0029, -0.0755, -0.0536,  0.0225,\n",
       "                      -0.1340,  0.1044, -0.0207,  0.0266, -0.0426,  0.0980, -0.0429,  0.0230,\n",
       "                      -0.0663,  0.0024, -0.1002, -0.0521,  0.1149,  0.0446,  0.0919,  0.1026,\n",
       "                      -0.0208, -0.0470, -0.1347, -0.0965, -0.0134, -0.0334, -0.0513, -0.0217,\n",
       "                       0.0595, -0.0507,  0.1234, -0.1622,  0.0130,  0.1129,  0.0729, -0.1545,\n",
       "                       0.0566,  0.0847,  0.0180, -0.0832, -0.0677,  0.0238,  0.0389, -0.1147],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.0.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.0.key.weight',\n",
       "              tensor([[ 0.1312, -0.0056, -0.0484,  ..., -0.0455, -0.0670, -0.0608],\n",
       "                      [ 0.0488, -0.0356,  0.0459,  ...,  0.0665, -0.0741, -0.0009],\n",
       "                      [ 0.1939,  0.0659,  0.1603,  ..., -0.0768,  0.1226, -0.0461],\n",
       "                      ...,\n",
       "                      [-0.1456, -0.0145,  0.1045,  ..., -0.0466, -0.0736,  0.0121],\n",
       "                      [-0.0899, -0.1097, -0.0887,  ...,  0.0048, -0.0186, -0.0084],\n",
       "                      [-0.0271, -0.0601,  0.1210,  ..., -0.0657, -0.0126, -0.0541]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.0.query.weight',\n",
       "              tensor([[-0.0531, -0.0447, -0.0020,  ..., -0.0595, -0.0045, -0.0112],\n",
       "                      [ 0.1065,  0.0148,  0.0760,  ..., -0.0290, -0.1315,  0.1260],\n",
       "                      [ 0.0190, -0.0673, -0.0990,  ...,  0.0041, -0.0135,  0.0066],\n",
       "                      ...,\n",
       "                      [-0.0669,  0.0084,  0.0272,  ..., -0.0799,  0.0760,  0.0130],\n",
       "                      [-0.0873,  0.0054, -0.1153,  ..., -0.0521, -0.0019,  0.0480],\n",
       "                      [-0.0404, -0.0642,  0.0660,  ..., -0.1766,  0.0785, -0.0159]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.0.value.weight',\n",
       "              tensor([[-0.0477,  0.0018,  0.0371,  ...,  0.0824, -0.0597, -0.0008],\n",
       "                      [-0.0754, -0.0582,  0.0239,  ...,  0.0172, -0.0907, -0.0433],\n",
       "                      [ 0.1206, -0.0507,  0.0349,  ..., -0.0714,  0.0323,  0.0581],\n",
       "                      ...,\n",
       "                      [-0.0646, -0.0424,  0.0392,  ..., -0.0169,  0.0673,  0.0586],\n",
       "                      [ 0.0084, -0.0425,  0.0461,  ..., -0.0663,  0.0958, -0.0101],\n",
       "                      [ 0.0399,  0.0541,  0.0159,  ...,  0.0397, -0.0577,  0.0253]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.1.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.1.key.weight',\n",
       "              tensor([[ 0.0367, -0.0269, -0.1017,  ..., -0.0017, -0.0179, -0.0013],\n",
       "                      [ 0.0547, -0.0776, -0.0720,  ...,  0.0046,  0.0260, -0.0052],\n",
       "                      [ 0.0207, -0.0110,  0.0267,  ..., -0.0124,  0.1290,  0.0023],\n",
       "                      ...,\n",
       "                      [ 0.1160, -0.0921, -0.1236,  ...,  0.0483,  0.0330, -0.0065],\n",
       "                      [-0.0376,  0.0244, -0.0393,  ..., -0.0049,  0.0670, -0.0375],\n",
       "                      [ 0.0631, -0.0157, -0.0677,  ..., -0.0455, -0.0493,  0.0306]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.1.query.weight',\n",
       "              tensor([[-0.0633,  0.0788,  0.0470,  ..., -0.0728, -0.0306,  0.0202],\n",
       "                      [ 0.0043,  0.0024, -0.0858,  ..., -0.1603, -0.0077,  0.0421],\n",
       "                      [-0.0395, -0.0241,  0.0690,  ..., -0.1102,  0.1175, -0.0617],\n",
       "                      ...,\n",
       "                      [-0.1380, -0.0294, -0.0822,  ...,  0.0280,  0.0608,  0.0321],\n",
       "                      [ 0.0053, -0.0265, -0.0378,  ...,  0.0702,  0.1075, -0.0335],\n",
       "                      [ 0.0636,  0.0383,  0.0341,  ..., -0.0295,  0.1085, -0.0129]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.1.value.weight',\n",
       "              tensor([[ 0.0166,  0.0479,  0.0237,  ...,  0.0198,  0.0309, -0.0125],\n",
       "                      [ 0.1629, -0.0026, -0.0508,  ..., -0.0147, -0.0010, -0.0669],\n",
       "                      [-0.0302, -0.0718, -0.0428,  ...,  0.0551,  0.0937, -0.0330],\n",
       "                      ...,\n",
       "                      [ 0.0089,  0.0112, -0.0201,  ..., -0.0493,  0.0060,  0.0265],\n",
       "                      [ 0.0279, -0.0511, -0.0201,  ..., -0.0641,  0.0361, -0.0404],\n",
       "                      [ 0.0496,  0.0426, -0.0445,  ...,  0.0092, -0.0175, -0.0030]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.2.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.2.key.weight',\n",
       "              tensor([[-0.0126, -0.0831, -0.1540,  ...,  0.0177, -0.0012, -0.0105],\n",
       "                      [ 0.0137, -0.0008,  0.0007,  ..., -0.0235, -0.0965, -0.0084],\n",
       "                      [-0.0409, -0.0302, -0.1229,  ...,  0.0203, -0.0396,  0.0129],\n",
       "                      ...,\n",
       "                      [ 0.0470,  0.0316,  0.0362,  ..., -0.0175, -0.0357,  0.0424],\n",
       "                      [-0.0238,  0.0509,  0.0143,  ...,  0.0140, -0.0157, -0.0177],\n",
       "                      [-0.0161, -0.0377, -0.0231,  ...,  0.0551,  0.1262,  0.0398]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.2.query.weight',\n",
       "              tensor([[ 0.0310, -0.0621, -0.1020,  ..., -0.0156,  0.0811,  0.0347],\n",
       "                      [ 0.1719, -0.0228, -0.0057,  ...,  0.0030, -0.0086, -0.0739],\n",
       "                      [-0.0564,  0.0496,  0.0193,  ..., -0.0851,  0.0698,  0.0851],\n",
       "                      ...,\n",
       "                      [ 0.0331,  0.0691,  0.0162,  ...,  0.1099,  0.0300,  0.0301],\n",
       "                      [ 0.0277,  0.0284,  0.0431,  ..., -0.0773, -0.0145, -0.0054],\n",
       "                      [ 0.0437, -0.0561, -0.0051,  ...,  0.1783,  0.0623, -0.0788]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.2.value.weight',\n",
       "              tensor([[-0.0386,  0.0059, -0.0239,  ...,  0.0285, -0.0497, -0.0111],\n",
       "                      [-0.0630,  0.0391, -0.0206,  ..., -0.0034,  0.0884, -0.0352],\n",
       "                      [-0.0402,  0.0488,  0.0264,  ..., -0.0605, -0.0884,  0.0025],\n",
       "                      ...,\n",
       "                      [ 0.0318, -0.0074,  0.0744,  ...,  0.1283, -0.0571, -0.0156],\n",
       "                      [ 0.0138,  0.0589, -0.0206,  ...,  0.0709,  0.0719,  0.0575],\n",
       "                      [ 0.0391,  0.0332, -0.0628,  ..., -0.0909, -0.0320,  0.0426]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.3.tril',\n",
       "              tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "                      ...,\n",
       "                      [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "                      [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.3.key.weight',\n",
       "              tensor([[-0.0560,  0.0471,  0.1837,  ...,  0.0003,  0.0375,  0.0935],\n",
       "                      [-0.0488, -0.1308, -0.0844,  ...,  0.0868,  0.0475, -0.0735],\n",
       "                      [-0.0758, -0.0301,  0.0207,  ...,  0.0909, -0.0337, -0.0450],\n",
       "                      ...,\n",
       "                      [-0.0380,  0.0682, -0.1437,  ..., -0.0211, -0.0328,  0.0415],\n",
       "                      [-0.0526, -0.0380, -0.0288,  ..., -0.0081, -0.0399, -0.0561],\n",
       "                      [-0.0066,  0.0250,  0.0655,  ..., -0.0587, -0.0045,  0.0008]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.3.query.weight',\n",
       "              tensor([[-0.1046, -0.0392,  0.1406,  ..., -0.0492,  0.1252, -0.0984],\n",
       "                      [-0.0362, -0.0398, -0.0416,  ..., -0.0792,  0.0337,  0.0506],\n",
       "                      [ 0.0681, -0.0563, -0.0272,  ...,  0.0036,  0.0122,  0.0589],\n",
       "                      ...,\n",
       "                      [-0.0621, -0.0103,  0.0334,  ..., -0.0138, -0.0237,  0.0897],\n",
       "                      [ 0.0032,  0.0415, -0.0412,  ...,  0.0689, -0.0599, -0.0767],\n",
       "                      [-0.0646,  0.0875, -0.0942,  ...,  0.0339, -0.0839, -0.0358]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.heads.3.value.weight',\n",
       "              tensor([[ 0.0095, -0.0204,  0.0741,  ...,  0.0348,  0.0130,  0.0128],\n",
       "                      [ 0.0950, -0.0088,  0.0531,  ...,  0.0055,  0.0735,  0.0300],\n",
       "                      [ 0.0317, -0.0180, -0.0107,  ..., -0.0566,  0.0874,  0.0591],\n",
       "                      ...,\n",
       "                      [-0.0424,  0.0676, -0.0466,  ...,  0.0046,  0.0252,  0.0418],\n",
       "                      [-0.0209,  0.0189, -0.0067,  ...,  0.0101, -0.0481,  0.0401],\n",
       "                      [-0.0382,  0.0002,  0.0380,  ...,  0.0128,  0.0776, -0.0690]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.proj.weight',\n",
       "              tensor([[ 0.0322, -0.0435,  0.0531,  ..., -0.0011, -0.0249, -0.0012],\n",
       "                      [ 0.0743, -0.0547,  0.0134,  ...,  0.0305, -0.0094, -0.0535],\n",
       "                      [ 0.0212,  0.1111,  0.0024,  ...,  0.1017, -0.1175, -0.1324],\n",
       "                      ...,\n",
       "                      [-0.0514, -0.0981,  0.0686,  ..., -0.0592, -0.0984,  0.0318],\n",
       "                      [-0.0220, -0.0493,  0.0336,  ...,  0.0383,  0.0308, -0.0890],\n",
       "                      [ 0.0740,  0.0143,  0.0204,  ..., -0.0369, -0.0589,  0.0339]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.sa.proj.bias',\n",
       "              tensor([ 0.0631,  0.0032, -0.0345, -0.0003,  0.0255, -0.0113, -0.0887,  0.0369,\n",
       "                       0.0447, -0.0455, -0.0323,  0.0751,  0.0128,  0.0192,  0.0231, -0.0088,\n",
       "                      -0.0170,  0.0138,  0.0049, -0.0256,  0.0238,  0.0512,  0.0685, -0.0360,\n",
       "                      -0.0360, -0.0438,  0.0533,  0.0568, -0.0582, -0.0038, -0.0153,  0.0281,\n",
       "                      -0.0064, -0.0599, -0.0511,  0.0164, -0.0421, -0.0078,  0.0304, -0.0287,\n",
       "                       0.0106, -0.0410, -0.0221,  0.0525, -0.0168,  0.0394, -0.0304, -0.0479,\n",
       "                      -0.0108,  0.0578, -0.0354,  0.0339, -0.0177, -0.0131,  0.0432, -0.0357,\n",
       "                      -0.0450,  0.0223, -0.0435, -0.0298,  0.0329,  0.0194,  0.0239, -0.0593,\n",
       "                      -0.0087, -0.0040,  0.0153, -0.0385,  0.0133, -0.0479, -0.0049,  0.0290,\n",
       "                       0.0129,  0.0046,  0.0031, -0.0351, -0.0499, -0.0023,  0.0468, -0.0083,\n",
       "                       0.0143, -0.0144,  0.0143,  0.0639,  0.0652,  0.0136,  0.0301, -0.0338,\n",
       "                      -0.0657, -0.0077,  0.0179, -0.0477, -0.0271, -0.0164,  0.0198, -0.0182,\n",
       "                       0.0277, -0.0058,  0.0102, -0.0381,  0.0550, -0.0075,  0.0294, -0.0358,\n",
       "                      -0.0182,  0.0446,  0.0611, -0.0454,  0.0225,  0.0072, -0.0335,  0.0511,\n",
       "                       0.0045, -0.0342,  0.0123, -0.0049, -0.0372, -0.0141, -0.0431, -0.0740,\n",
       "                      -0.0049, -0.0047, -0.0466,  0.0460,  0.0365,  0.0450,  0.0453, -0.0216,\n",
       "                       0.0537,  0.0436, -0.0414,  0.0395,  0.0343,  0.0043,  0.0087,  0.0216,\n",
       "                       0.0471,  0.0475,  0.0002, -0.0376,  0.0595,  0.0301, -0.0265,  0.0609,\n",
       "                      -0.0152, -0.0044,  0.0301,  0.0147, -0.0473, -0.0418,  0.0181,  0.0550,\n",
       "                       0.0273, -0.0164,  0.0495, -0.0087,  0.0367, -0.0436, -0.0348, -0.0282,\n",
       "                      -0.0422,  0.0676, -0.0010, -0.0438,  0.0431, -0.0451,  0.0032,  0.0338,\n",
       "                      -0.0083,  0.0560, -0.0775, -0.0486, -0.0492,  0.0158, -0.0526, -0.0658,\n",
       "                      -0.0415, -0.0545,  0.0231, -0.0476, -0.0520, -0.0028,  0.0103,  0.0547,\n",
       "                      -0.0391, -0.0433, -0.0197, -0.0562,  0.0453, -0.0264,  0.0156, -0.0730,\n",
       "                      -0.0511, -0.0587, -0.0290, -0.0414,  0.0416,  0.0307,  0.0274, -0.0025,\n",
       "                      -0.0204, -0.0459, -0.0104, -0.0061, -0.0229,  0.0207,  0.0742, -0.0147,\n",
       "                      -0.0131, -0.0580,  0.0114, -0.0562,  0.0295,  0.0003, -0.0395,  0.0099,\n",
       "                      -0.0340,  0.0014,  0.0065, -0.0020, -0.0329,  0.0829, -0.0112, -0.0138,\n",
       "                       0.0322, -0.0271,  0.0380,  0.0345,  0.0332,  0.0007, -0.0307, -0.0247,\n",
       "                      -0.0243,  0.0513, -0.0159, -0.0557, -0.0078,  0.0373, -0.0501, -0.0178,\n",
       "                       0.0213, -0.0337, -0.0178,  0.0348, -0.0097, -0.0185,  0.0510, -0.0918,\n",
       "                      -0.0122, -0.0573, -0.0423, -0.0412,  0.0080, -0.0311, -0.0511,  0.0168],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ffwd.net.0.weight',\n",
       "              tensor([[ 6.8401e-02, -1.8465e-02, -4.9944e-02,  ..., -2.2287e-02,\n",
       "                       -5.2527e-02, -5.1385e-03],\n",
       "                      [-4.1737e-02, -3.6148e-02,  2.5434e-02,  ...,  1.4843e-02,\n",
       "                        4.5775e-02, -1.1802e-03],\n",
       "                      [-8.8723e-04,  1.4569e-01, -1.6871e-02,  ..., -4.5247e-02,\n",
       "                        4.1495e-02, -3.7051e-02],\n",
       "                      ...,\n",
       "                      [-1.1260e-01, -1.3044e-04,  8.7007e-02,  ..., -6.6066e-02,\n",
       "                        9.8290e-02,  6.2602e-03],\n",
       "                      [-5.8532e-02,  6.4582e-02, -8.2173e-02,  ...,  4.8229e-02,\n",
       "                        3.4614e-02, -1.5878e-02],\n",
       "                      [-1.7255e-01,  1.4660e-02,  4.5569e-02,  ..., -1.9141e-01,\n",
       "                        7.5307e-02, -3.9468e-02]], device='cuda:0')),\n",
       "             ('blocks.3.ffwd.net.0.bias',\n",
       "              tensor([-0.0139, -0.0469, -0.1117,  ...,  0.0562, -0.0606, -0.1174],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ffwd.net.2.weight',\n",
       "              tensor([[-0.0550, -0.0090, -0.0397,  ...,  0.0332,  0.0634,  0.0245],\n",
       "                      [-0.0581, -0.1898, -0.0338,  ..., -0.0309, -0.0167,  0.0086],\n",
       "                      [-0.0386, -0.1585, -0.0762,  ..., -0.0718, -0.0380, -0.0463],\n",
       "                      ...,\n",
       "                      [-0.0095, -0.0262,  0.0133,  ...,  0.0087,  0.0172,  0.0638],\n",
       "                      [ 0.0561,  0.0262, -0.0499,  ..., -0.0335,  0.0383, -0.0144],\n",
       "                      [-0.0485, -0.1638, -0.0657,  ..., -0.0765,  0.0070,  0.0591]],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ffwd.net.2.bias',\n",
       "              tensor([ 6.3955e-03, -4.4076e-03,  5.0552e-03, -1.1867e-02,  3.6515e-02,\n",
       "                       3.6488e-02,  3.3442e-02, -2.3323e-02,  1.2153e-02,  1.0222e-03,\n",
       "                       2.4240e-03, -2.7603e-03, -6.3793e-03,  1.2912e-02, -2.9400e-02,\n",
       "                      -1.4265e-02,  1.9325e-02, -3.2615e-02, -1.0731e-02, -1.7519e-02,\n",
       "                      -1.4551e-02,  2.1171e-02, -5.1441e-03,  1.6208e-02,  4.2240e-02,\n",
       "                      -9.7861e-03,  2.5225e-02, -1.2369e-02, -6.0323e-03,  2.4641e-02,\n",
       "                       1.7408e-02, -3.1838e-02, -2.8086e-02,  7.2447e-04, -3.1796e-03,\n",
       "                      -1.3268e-02,  5.6469e-03,  9.7856e-03, -2.2016e-02,  1.8554e-02,\n",
       "                      -1.5844e-02,  1.5578e-02, -1.3338e-02, -1.3473e-02, -7.4768e-03,\n",
       "                      -3.5715e-02, -1.0232e-02, -4.0143e-03,  1.1631e-02,  1.6748e-02,\n",
       "                      -7.2790e-03,  4.1782e-02,  3.6593e-02,  2.8760e-02,  4.4558e-02,\n",
       "                       1.5171e-02,  4.9190e-03, -1.0919e-02,  5.1845e-03,  3.3075e-02,\n",
       "                       6.6134e-03,  1.9760e-02,  1.6927e-02,  6.6059e-03,  1.2657e-02,\n",
       "                       1.0257e-02,  1.7142e-02, -1.7803e-02, -2.3160e-02,  1.4930e-02,\n",
       "                      -2.0257e-02,  9.9483e-04,  2.1379e-02, -1.7767e-02,  1.9913e-02,\n",
       "                      -4.5181e-03, -1.0608e-02,  1.7811e-02,  2.4901e-02, -1.9147e-02,\n",
       "                       1.7059e-03,  1.0990e-02, -1.8806e-02, -1.2564e-02,  1.1384e-02,\n",
       "                       4.1415e-03,  8.0275e-03, -2.4346e-02, -2.4602e-02, -4.8630e-03,\n",
       "                      -1.1567e-02,  1.4694e-02, -2.1482e-02,  3.4277e-02, -1.7727e-02,\n",
       "                       9.7307e-03, -1.9781e-02,  1.4244e-02, -2.3987e-02, -2.0333e-02,\n",
       "                       5.7046e-03, -1.1971e-02,  9.6040e-03, -2.5613e-02, -1.2007e-02,\n",
       "                      -1.6600e-02, -1.4147e-03, -1.7664e-02, -2.4456e-02, -1.6097e-02,\n",
       "                      -1.5966e-02, -2.2705e-02,  6.0813e-03,  2.4624e-02, -1.4939e-02,\n",
       "                       2.2796e-02, -1.2372e-02, -2.5699e-02, -1.8792e-02,  1.9455e-02,\n",
       "                      -3.6828e-02, -4.0162e-03,  1.1749e-02,  9.1019e-03, -4.2353e-03,\n",
       "                       3.5441e-02, -1.9934e-02,  9.6013e-03, -1.8323e-02,  1.2898e-02,\n",
       "                      -2.2591e-02, -1.0072e-03,  1.2063e-02, -7.3638e-03, -1.2719e-02,\n",
       "                      -8.5958e-03, -2.1605e-02, -9.7210e-04,  6.8917e-04, -3.2508e-02,\n",
       "                      -2.3674e-02,  1.0077e-02, -1.0658e-02, -3.1258e-02,  2.7837e-02,\n",
       "                       1.0193e-02, -2.3889e-02,  1.7523e-02, -5.8084e-03, -1.0746e-02,\n",
       "                       3.3819e-02,  6.5325e-03,  4.0222e-03,  2.4729e-02,  4.2301e-03,\n",
       "                      -2.9055e-02, -6.9042e-03, -2.7096e-02, -1.7267e-03,  1.3071e-03,\n",
       "                       1.5684e-02, -1.6765e-02,  1.7677e-02, -6.2336e-05,  4.9948e-03,\n",
       "                      -2.0943e-02, -1.4917e-02, -1.4232e-02, -2.8122e-03, -1.6452e-02,\n",
       "                      -1.5670e-02, -2.8312e-02,  2.8440e-02, -2.4596e-02, -1.3223e-02,\n",
       "                       1.7486e-02,  2.5889e-02, -1.1418e-02,  2.1360e-03, -3.4958e-03,\n",
       "                      -1.6955e-02, -2.3381e-02,  4.5385e-02, -1.5694e-02,  1.9452e-02,\n",
       "                       1.2795e-02, -3.8314e-02,  2.5588e-02,  1.0045e-02,  9.2501e-03,\n",
       "                       2.5661e-03,  3.0657e-02,  1.7908e-03,  2.8079e-02, -1.8017e-03,\n",
       "                      -2.7659e-02, -3.1008e-02,  1.8505e-02,  5.5591e-03,  3.2255e-02,\n",
       "                      -3.5918e-03,  3.5763e-02,  1.7413e-02,  3.1238e-02,  1.8753e-02,\n",
       "                       1.0959e-02,  5.3050e-03,  3.7648e-02,  4.5164e-03, -1.0332e-02,\n",
       "                      -1.2598e-02, -1.8289e-02, -3.5858e-02, -2.6188e-04, -7.6619e-03,\n",
       "                      -2.2056e-02, -2.2999e-02, -2.8692e-03,  1.0331e-02, -1.8857e-02,\n",
       "                       3.8752e-03, -3.3756e-02,  1.6043e-03, -2.1300e-04, -1.3604e-02,\n",
       "                      -3.2733e-02, -1.5868e-02, -1.0110e-02, -2.0198e-02,  2.9537e-02,\n",
       "                       1.4242e-02,  1.6123e-02,  7.5298e-03, -2.1510e-02,  2.4913e-02,\n",
       "                      -3.4906e-02,  3.3330e-03,  2.0509e-02,  3.8683e-02, -2.2572e-02,\n",
       "                      -1.2900e-02,  1.8697e-02,  8.2267e-03, -1.4150e-03, -9.0925e-03,\n",
       "                      -1.0725e-02, -2.8361e-02,  2.2739e-03,  7.9467e-04,  1.5080e-02,\n",
       "                      -1.8561e-03,  1.9431e-02, -3.1823e-02, -9.6205e-03, -1.2637e-02,\n",
       "                       4.2513e-02], device='cuda:0')),\n",
       "             ('blocks.3.ln1.weight',\n",
       "              tensor([1.0342, 0.8790, 1.0383, 1.0313, 1.0389, 1.0369, 0.9487, 1.0542, 1.0205,\n",
       "                      1.0126, 1.0589, 1.0823, 1.0700, 1.0485, 1.0533, 1.0227, 0.9832, 1.1359,\n",
       "                      1.0441, 1.0280, 1.0852, 1.0110, 1.0001, 0.9914, 1.0098, 1.0320, 1.0045,\n",
       "                      1.0575, 1.0764, 1.0374, 1.0240, 1.0299, 0.8963, 1.0818, 1.0212, 1.0564,\n",
       "                      1.0090, 0.9629, 1.0992, 1.0131, 0.9962, 1.0041, 1.0477, 1.0318, 1.1126,\n",
       "                      1.0183, 1.0560, 1.0132, 1.0584, 1.0055, 1.0321, 0.9744, 1.0293, 0.9602,\n",
       "                      1.0780, 0.9434, 0.9885, 1.0409, 1.0355, 1.0290, 1.0228, 1.0811, 0.9991,\n",
       "                      0.9939, 1.0401, 1.0043, 1.0798, 0.9776, 1.0092, 1.1240, 1.1270, 1.0478,\n",
       "                      1.0808, 1.0125, 1.0480, 0.9860, 1.0520, 1.0367, 1.0353, 1.0073, 0.9709,\n",
       "                      1.0251, 1.0617, 1.0110, 1.0926, 1.0191, 0.9941, 1.0246, 1.0516, 1.0460,\n",
       "                      1.0756, 0.9664, 1.0176, 1.0302, 0.9706, 1.0956, 0.9192, 1.0270, 0.9628,\n",
       "                      1.0236, 1.0167, 1.0113, 1.0114, 1.0196, 0.9536, 1.0253, 1.0149, 1.1260,\n",
       "                      1.0212, 1.0250, 1.0493, 1.0271, 1.0133, 1.0172, 1.0093, 1.0094, 1.1117,\n",
       "                      1.0049, 1.0249, 1.1141, 0.9837, 0.9759, 1.0732, 0.9474, 0.9835, 0.9855,\n",
       "                      0.9841, 1.0425, 0.9392, 0.9864, 1.0149, 1.0179, 0.9587, 0.9804, 1.0209,\n",
       "                      0.9763, 1.0457, 0.9568, 0.9727, 1.0192, 1.0279, 0.9971, 1.0540, 1.0025,\n",
       "                      1.0434, 1.0490, 1.0399, 0.9926, 1.0413, 1.0140, 1.0006, 1.0234, 0.9983,\n",
       "                      1.0116, 1.0368, 1.0118, 1.0166, 1.0089, 1.0094, 1.0558, 1.0661, 1.0114,\n",
       "                      0.9957, 0.9884, 1.0781, 1.0616, 1.0968, 0.9984, 1.0351, 1.0022, 0.9857,\n",
       "                      0.9825, 1.0619, 1.0916, 0.9512, 1.0627, 1.0779, 1.0287, 0.9571, 1.0924,\n",
       "                      1.0127, 0.9567, 1.0644, 1.0587, 1.0279, 1.0352, 1.0906, 0.9735, 1.0112,\n",
       "                      1.0567, 1.0544, 1.0387, 1.0184, 0.9890, 0.9621, 0.9960, 1.0804, 1.0141,\n",
       "                      0.9942, 1.0312, 1.0207, 1.0878, 1.0044, 0.9696, 1.0284, 0.9657, 1.0368,\n",
       "                      1.0732, 1.0610, 1.0011, 1.0014, 0.9665, 1.0437, 1.0742, 1.1414, 1.0002,\n",
       "                      1.0033, 1.0363, 1.1681, 1.0605, 1.0228, 1.0093, 1.0245, 1.0311, 1.0481,\n",
       "                      1.0292, 1.0416, 0.9864, 0.9606, 0.9638, 1.0155, 0.9703, 1.0502, 1.1200,\n",
       "                      0.9981, 0.9364, 1.0052, 1.0709, 1.0244, 1.0663, 0.9656, 1.0143, 0.9545,\n",
       "                      0.9664, 1.0704, 1.0038, 1.0109, 1.0513, 0.9988, 1.0074, 1.0434, 0.9747,\n",
       "                      0.9798, 1.0058, 1.0157, 0.9641], device='cuda:0')),\n",
       "             ('blocks.3.ln1.bias',\n",
       "              tensor([-0.0194,  0.1142,  0.0890,  0.0318,  0.0099, -0.0096,  0.0711,  0.0195,\n",
       "                       0.0475,  0.0191,  0.0237,  0.0785, -0.0251, -0.0323, -0.0286,  0.0138,\n",
       "                      -0.0564, -0.0672,  0.0364, -0.0444,  0.0182, -0.0313,  0.0154, -0.0240,\n",
       "                      -0.0392,  0.0367, -0.0061, -0.0264, -0.0139,  0.0343, -0.0534,  0.0245,\n",
       "                       0.1024,  0.0348,  0.0444,  0.0191, -0.0723, -0.0819, -0.0090,  0.0450,\n",
       "                       0.0244,  0.0403, -0.0969,  0.0336,  0.0429, -0.0575,  0.0353,  0.0446,\n",
       "                      -0.0519, -0.0504, -0.0433,  0.0714, -0.0346,  0.0876, -0.0434, -0.0777,\n",
       "                      -0.0664,  0.0089,  0.0489,  0.0511, -0.0691,  0.0164,  0.0029,  0.0265,\n",
       "                      -0.0665,  0.0744,  0.0253, -0.0855,  0.0332,  0.0117, -0.0435,  0.0196,\n",
       "                      -0.0433, -0.0118, -0.0570,  0.0197, -0.0363,  0.0150,  0.0112, -0.0599,\n",
       "                      -0.0430, -0.0125,  0.0029, -0.0297, -0.0323, -0.0329,  0.0363, -0.0252,\n",
       "                      -0.0018, -0.0215,  0.0080,  0.0369, -0.0053, -0.0193,  0.0412, -0.0145,\n",
       "                       0.0443,  0.0420,  0.0664, -0.0182, -0.0327,  0.0094, -0.0174,  0.0023,\n",
       "                       0.0938, -0.0600, -0.0774,  0.0113,  0.0584,  0.0201,  0.0042, -0.0619,\n",
       "                       0.0181,  0.0018, -0.0413,  0.0351, -0.0018,  0.0130,  0.0179, -0.0263,\n",
       "                      -0.0337,  0.0223, -0.0156, -0.0120, -0.0625,  0.0202, -0.0075,  0.0063,\n",
       "                       0.0549, -0.0835,  0.0025,  0.0341, -0.0767, -0.0769, -0.0005,  0.0512,\n",
       "                      -0.0365, -0.0026, -0.0167,  0.0529, -0.0432,  0.0989, -0.0280, -0.0374,\n",
       "                      -0.0331,  0.0308,  0.0170, -0.0592,  0.0227, -0.0484, -0.0656,  0.0240,\n",
       "                      -0.0231, -0.0170, -0.0093,  0.0280,  0.0445, -0.0002,  0.0300, -0.0137,\n",
       "                       0.0052, -0.0443, -0.0043,  0.0759,  0.0262, -0.0341,  0.0178, -0.0420,\n",
       "                      -0.0071, -0.0583,  0.0753,  0.0534, -0.0344,  0.0253,  0.0863,  0.0855,\n",
       "                      -0.0442, -0.0643, -0.0539,  0.0097, -0.0570, -0.0380,  0.0154, -0.0082,\n",
       "                       0.0097,  0.0277, -0.0060,  0.1085,  0.0024, -0.0200, -0.0287,  0.0340,\n",
       "                      -0.0201,  0.0326, -0.0271, -0.0272,  0.0143, -0.0395, -0.0135,  0.0254,\n",
       "                       0.0131,  0.0268, -0.0586,  0.0374,  0.0098,  0.0902, -0.0702, -0.0221,\n",
       "                       0.0112,  0.0551,  0.0019,  0.0527,  0.0059,  0.0625,  0.0201, -0.0259,\n",
       "                       0.0917, -0.0511, -0.0227,  0.0261,  0.0034, -0.0336,  0.0935,  0.0539,\n",
       "                       0.0510, -0.0464, -0.0047,  0.0513, -0.0880, -0.0404, -0.0594, -0.0586,\n",
       "                       0.0247,  0.0044,  0.0175,  0.1407, -0.0196, -0.0194,  0.0248,  0.0675,\n",
       "                      -0.0750,  0.0585, -0.0846,  0.0811, -0.0286, -0.0612, -0.0550,  0.0383,\n",
       "                      -0.0521, -0.0544,  0.0506, -0.0187,  0.0059, -0.0448,  0.0220,  0.0364],\n",
       "                     device='cuda:0')),\n",
       "             ('blocks.3.ln2.weight',\n",
       "              tensor([1.2513, 1.1439, 1.1931, 1.2687, 1.1547, 1.1862, 1.2174, 1.3207, 1.2231,\n",
       "                      1.1799, 1.2901, 1.0382, 1.3052, 1.2555, 1.2850, 1.1741, 1.2935, 1.0881,\n",
       "                      1.3014, 1.2450, 1.3072, 1.2138, 1.1214, 1.2049, 1.1700, 1.2665, 1.1680,\n",
       "                      1.2381, 1.2250, 1.2929, 1.2852, 1.2480, 1.0059, 1.2956, 1.2074, 1.2748,\n",
       "                      1.1806, 1.0944, 1.3023, 1.1506, 1.2163, 1.1771, 1.3104, 1.2414, 1.2606,\n",
       "                      1.2769, 1.1945, 1.1519, 1.3342, 1.3378, 1.1952, 1.2046, 1.2274, 1.1198,\n",
       "                      1.2160, 1.1899, 1.1545, 1.2890, 1.1388, 1.2691, 1.1840, 1.1624, 1.2731,\n",
       "                      1.1688, 1.2773, 1.1470, 1.3324, 1.1395, 1.2331, 1.2783, 1.3068, 1.2341,\n",
       "                      1.2513, 1.3116, 1.3356, 1.3098, 1.2989, 1.2783, 1.1774, 1.0968, 1.2298,\n",
       "                      1.1823, 1.2255, 1.1478, 1.1200, 1.2635, 1.2383, 1.2349, 1.2546, 1.2608,\n",
       "                      1.3403, 1.1758, 1.2393, 1.3106, 1.2040, 1.2073, 1.1727, 1.2502, 1.2444,\n",
       "                      1.2579, 1.1485, 1.1755, 1.2723, 1.3481, 1.1659, 1.2506, 1.2984, 1.3128,\n",
       "                      1.2342, 1.1751, 1.2660, 1.2615, 1.3005, 1.3727, 1.2792, 1.2406, 1.4009,\n",
       "                      1.2124, 1.3006, 1.2969, 1.1324, 1.2438, 1.1975, 1.1990, 1.1853, 1.1557,\n",
       "                      1.2286, 1.2373, 1.1807, 1.1447, 1.0589, 1.1739, 1.1669, 1.1933, 1.2270,\n",
       "                      1.1101, 1.2611, 1.0868, 1.1432, 1.2479, 1.2648, 1.1430, 1.3253, 1.2063,\n",
       "                      1.2613, 1.2757, 1.2951, 1.2395, 1.2771, 1.2323, 1.1645, 1.1497, 1.1453,\n",
       "                      1.1929, 1.2264, 1.1865, 1.2565, 1.1797, 1.1354, 1.2086, 1.2431, 1.2773,\n",
       "                      1.2768, 1.2226, 1.2997, 1.1974, 1.2675, 1.1387, 1.2150, 1.2877, 1.2058,\n",
       "                      1.1292, 1.1055, 1.2237, 1.2581, 1.2661, 1.1987, 1.2149, 1.1688, 1.2776,\n",
       "                      1.1694, 1.1453, 1.2858, 1.2352, 1.2124, 1.1516, 1.2690, 1.2110, 1.2486,\n",
       "                      1.2187, 1.1984, 1.3006, 1.2215, 1.2008, 1.1650, 1.2527, 1.3030, 1.2462,\n",
       "                      1.1646, 1.2073, 1.2760, 1.3488, 1.1259, 1.2570, 1.1925, 1.1528, 1.2457,\n",
       "                      1.0977, 1.3409, 1.2421, 1.1708, 1.1673, 1.2337, 1.3213, 1.3472, 1.1731,\n",
       "                      1.2413, 1.2609, 1.1150, 1.2711, 1.2380, 1.2749, 1.0871, 1.1304, 1.2998,\n",
       "                      1.2509, 1.1520, 1.2344, 1.0995, 1.2820, 1.1914, 1.2225, 1.2521, 1.3678,\n",
       "                      1.1674, 1.1724, 1.2813, 1.2040, 1.2560, 1.1957, 1.1291, 1.2332, 1.1041,\n",
       "                      1.1464, 1.2536, 1.1234, 1.2651, 1.3417, 1.3657, 1.2471, 1.2053, 1.1540,\n",
       "                      1.1805, 1.2214, 1.2427, 1.0786], device='cuda:0')),\n",
       "             ('blocks.3.ln2.bias',\n",
       "              tensor([ 0.0502, -0.1263, -0.0218,  0.0081, -0.0509,  0.0149, -0.1383, -0.0258,\n",
       "                      -0.0843,  0.0095, -0.0438, -0.0673,  0.1531,  0.0276,  0.0392,  0.0547,\n",
       "                       0.0912,  0.0754, -0.1227,  0.1214, -0.1164,  0.1201, -0.0085,  0.0543,\n",
       "                       0.0423,  0.0189,  0.0021,  0.1049, -0.0663, -0.1332,  0.1076,  0.1248,\n",
       "                       0.0382, -0.0553, -0.0316, -0.0271, -0.0008,  0.1157,  0.0161, -0.0692,\n",
       "                      -0.0345, -0.1208,  0.1032,  0.0006, -0.0076,  0.0419, -0.0345, -0.0544,\n",
       "                       0.1440,  0.1227,  0.1061, -0.1475,  0.0060, -0.0458, -0.1033,  0.1618,\n",
       "                       0.0871, -0.1522,  0.0205, -0.1736,  0.0574, -0.0313, -0.0236,  0.0165,\n",
       "                       0.0661, -0.0172, -0.0382,  0.0570,  0.0303,  0.0260,  0.0738, -0.0207,\n",
       "                       0.0053,  0.0429,  0.0295, -0.0236, -0.0263,  0.0335, -0.0673,  0.0657,\n",
       "                      -0.0231,  0.0801,  0.0019,  0.0646,  0.0049,  0.0725, -0.0505,  0.0612,\n",
       "                      -0.0283,  0.0289, -0.1469,  0.0691, -0.0049,  0.0213, -0.1126,  0.0645,\n",
       "                      -0.0864, -0.0600, -0.1617,  0.0459,  0.0791, -0.0675,  0.0421, -0.0009,\n",
       "                      -0.0452,  0.1332,  0.1060,  0.0103, -0.1147,  0.0136,  0.1139,  0.0569,\n",
       "                      -0.0621, -0.0868,  0.0147, -0.0737, -0.0246,  0.0357, -0.0170, -0.0631,\n",
       "                       0.0598, -0.0594, -0.0420, -0.0083,  0.0483, -0.0494,  0.0238, -0.0568,\n",
       "                      -0.0616,  0.0757,  0.0280, -0.0078,  0.0979,  0.1086,  0.0442, -0.0761,\n",
       "                       0.0939,  0.0203,  0.0481, -0.0505,  0.1226, -0.0415,  0.0179,  0.1256,\n",
       "                       0.0442, -0.1196, -0.0638,  0.1233, -0.1295,  0.1276, -0.0160, -0.0107,\n",
       "                      -0.0436,  0.0809,  0.0219,  0.0113, -0.0834, -0.0751, -0.0261, -0.0257,\n",
       "                      -0.0797,  0.0624,  0.0462, -0.1041, -0.0165, -0.0324,  0.0038,  0.0337,\n",
       "                       0.0384,  0.1717, -0.1262, -0.0023, -0.0473, -0.0234, -0.1566, -0.1136,\n",
       "                       0.0821,  0.0419,  0.0856, -0.0213,  0.0758,  0.0687, -0.1017,  0.1068,\n",
       "                      -0.0566, -0.0248,  0.0926, -0.1279, -0.0237, -0.0352, -0.1108, -0.1670,\n",
       "                      -0.0762, -0.0632, -0.0147,  0.0879, -0.0257,  0.0711,  0.0433, -0.0597,\n",
       "                      -0.0274, -0.1242,  0.0699, -0.1036,  0.0287, -0.0752,  0.1580, -0.0194,\n",
       "                      -0.0478, -0.0494,  0.1103, -0.1005, -0.0063, -0.0956, -0.0605,  0.0886,\n",
       "                      -0.1210,  0.1135,  0.0157,  0.0240,  0.0113,  0.1337, -0.0470, -0.0203,\n",
       "                      -0.0541,  0.0923, -0.0602, -0.0810,  0.0693,  0.0846,  0.0321,  0.1160,\n",
       "                      -0.0960,  0.0055, -0.1062, -0.0712,  0.0496,  0.0050, -0.1154, -0.0333,\n",
       "                       0.0901, -0.0904,  0.1128, -0.1237,  0.0256,  0.0475,  0.0597, -0.1640,\n",
       "                       0.0154,  0.1122, -0.0753,  0.0108, -0.0282,  0.0919, -0.0265, -0.0447],\n",
       "                     device='cuda:0')),\n",
       "             ('ln_f.weight',\n",
       "              tensor([1.1029, 1.0188, 1.0477, 1.1693, 1.1543, 1.2178, 1.0787, 1.1613, 1.1791,\n",
       "                      1.1185, 1.0399, 1.0574, 1.2150, 1.1491, 1.1614, 1.1220, 1.0969, 1.0925,\n",
       "                      1.1625, 1.0828, 1.1874, 1.1289, 1.0923, 1.1010, 1.1316, 1.1235, 1.1257,\n",
       "                      1.1048, 1.1429, 1.1580, 1.0843, 1.1702, 1.0628, 1.1083, 1.1052, 1.0766,\n",
       "                      1.1911, 1.0479, 1.1189, 1.0411, 1.1017, 1.1266, 1.0933, 1.0362, 1.1193,\n",
       "                      1.0748, 1.2108, 1.0754, 1.1093, 1.1177, 1.1280, 1.0780, 1.1156, 1.1027,\n",
       "                      1.1110, 1.0513, 1.0413, 1.0901, 1.2172, 1.1697, 1.1391, 1.1062, 1.1231,\n",
       "                      1.1451, 1.1028, 1.1588, 1.1770, 1.0639, 1.1262, 1.1403, 1.1231, 1.1882,\n",
       "                      1.1238, 1.1557, 1.0859, 1.0710, 1.0896, 1.1173, 1.1725, 1.1444, 1.0673,\n",
       "                      1.0794, 1.0584, 1.1384, 1.1309, 1.0520, 1.1256, 1.1437, 1.0845, 1.0994,\n",
       "                      1.1620, 1.0212, 1.0786, 1.1441, 1.0870, 1.1838, 1.0647, 1.0504, 1.1062,\n",
       "                      1.1603, 1.0986, 1.1075, 1.1852, 1.1702, 1.0963, 1.1318, 1.1117, 1.1632,\n",
       "                      1.0889, 1.0897, 1.1250, 1.1010, 1.1572, 1.1566, 1.0460, 1.1679, 1.1312,\n",
       "                      1.0897, 1.1378, 1.1140, 1.1600, 1.1277, 1.2036, 1.1532, 1.1827, 1.1104,\n",
       "                      1.0430, 1.1190, 1.0745, 1.0712, 1.0897, 1.1560, 1.0333, 1.0514, 1.1994,\n",
       "                      1.1481, 1.1634, 1.0507, 1.1368, 1.0844, 1.2621, 1.0037, 1.2081, 1.1253,\n",
       "                      1.1603, 1.1410, 1.1027, 1.0811, 1.1504, 1.0843, 1.0710, 1.0752, 1.0997,\n",
       "                      1.1652, 1.1538, 1.1070, 1.1143, 1.1222, 1.1287, 1.1600, 1.1762, 1.1394,\n",
       "                      1.1459, 1.0632, 1.0454, 1.1430, 1.1170, 1.0723, 1.1609, 1.1221, 1.0613,\n",
       "                      1.1508, 1.0789, 1.0954, 1.0248, 1.1071, 1.1292, 1.1146, 1.1106, 1.1808,\n",
       "                      1.0734, 1.0327, 1.1343, 1.1851, 1.0629, 1.1618, 1.1636, 1.1037, 1.1524,\n",
       "                      1.1109, 1.1657, 1.1692, 1.1316, 1.1251, 1.0683, 1.1652, 1.1509, 1.1147,\n",
       "                      1.1512, 1.1965, 1.1366, 1.1804, 1.0470, 1.1054, 1.1998, 1.0152, 1.1679,\n",
       "                      1.0817, 1.1898, 1.1316, 1.0810, 1.0531, 1.1522, 1.1057, 1.2575, 1.0646,\n",
       "                      1.1203, 1.1402, 1.1013, 1.1181, 1.2184, 1.1434, 1.1516, 1.1775, 1.1255,\n",
       "                      1.1279, 1.1661, 1.1406, 1.0899, 1.0523, 1.1353, 1.1176, 1.1228, 1.1092,\n",
       "                      1.1242, 1.0521, 1.1740, 1.1462, 1.1623, 1.1235, 1.0239, 1.1657, 1.0288,\n",
       "                      1.0614, 1.1857, 1.0247, 1.0566, 1.2519, 1.0948, 1.2160, 1.1056, 1.1517,\n",
       "                      1.0738, 1.0859, 1.1395, 1.0392], device='cuda:0')),\n",
       "             ('ln_f.bias',\n",
       "              tensor([-0.0003, -0.0072, -0.0082, -0.0260,  0.0140,  0.0428, -0.0530, -0.0219,\n",
       "                      -0.0532, -0.0053,  0.0226, -0.0466,  0.0330,  0.0552,  0.0160, -0.0192,\n",
       "                       0.0200, -0.0126, -0.0291,  0.0332, -0.0109, -0.0269,  0.0463,  0.0587,\n",
       "                       0.0451, -0.0090,  0.0259,  0.0527,  0.0306, -0.0311,  0.0269, -0.0038,\n",
       "                       0.0046,  0.0022, -0.0439, -0.0375,  0.0387, -0.0115,  0.0134, -0.0022,\n",
       "                      -0.0151, -0.0136,  0.0375,  0.0031, -0.0262,  0.0591,  0.0010, -0.0178,\n",
       "                       0.0224,  0.0397,  0.0285, -0.0193,  0.0180, -0.0463, -0.0620,  0.0496,\n",
       "                       0.0257, -0.0496, -0.0493, -0.0276,  0.0433, -0.0419,  0.0336, -0.0477,\n",
       "                       0.0090, -0.0413,  0.0074, -0.0010, -0.0040, -0.0325,  0.0073, -0.0471,\n",
       "                       0.0264,  0.0057,  0.0096, -0.0127,  0.0141, -0.0109, -0.0066,  0.0624,\n",
       "                      -0.0263,  0.0377, -0.0282, -0.0094,  0.0625,  0.0339, -0.0140,  0.0063,\n",
       "                      -0.0141,  0.0441, -0.0389,  0.0452, -0.0024,  0.0619, -0.0118,  0.0332,\n",
       "                       0.0409, -0.0217, -0.0406,  0.0033,  0.0350, -0.0218,  0.0459,  0.0307,\n",
       "                      -0.0298,  0.0307,  0.0441,  0.0429, -0.0466, -0.0453,  0.0278,  0.0357,\n",
       "                      -0.0200, -0.0206, -0.0127,  0.0044,  0.0110, -0.0192,  0.0090,  0.0205,\n",
       "                      -0.0211, -0.0173, -0.0176,  0.0293,  0.0387, -0.0580, -0.0464,  0.0235,\n",
       "                      -0.0074,  0.0115, -0.0349, -0.0256,  0.0268,  0.0094, -0.0216,  0.0079,\n",
       "                       0.0303, -0.0645, -0.0055, -0.0270,  0.0121,  0.0203,  0.0089,  0.0151,\n",
       "                      -0.0264,  0.0126, -0.0202,  0.0280, -0.0407,  0.0529,  0.0300, -0.0316,\n",
       "                       0.0253,  0.0398,  0.0076, -0.0268, -0.0470,  0.0244, -0.0642,  0.0436,\n",
       "                       0.0054,  0.0214,  0.0569, -0.0223,  0.0102, -0.0370, -0.0151, -0.0033,\n",
       "                       0.0137, -0.0035, -0.0184, -0.0202,  0.0012, -0.0057, -0.0262, -0.0167,\n",
       "                       0.0348,  0.0443, -0.0098,  0.0307,  0.0319, -0.0152, -0.0278, -0.0032,\n",
       "                      -0.0568, -0.0198, -0.0045, -0.0397, -0.0125, -0.0471, -0.0049, -0.0383,\n",
       "                       0.0031, -0.0369, -0.0119,  0.0086,  0.0207,  0.0514,  0.0293, -0.0223,\n",
       "                      -0.0086, -0.0026,  0.0066, -0.0212,  0.0214, -0.0031,  0.0319,  0.0023,\n",
       "                      -0.0567, -0.0155,  0.0286, -0.0321, -0.0401, -0.0475,  0.0027,  0.0091,\n",
       "                      -0.0588,  0.0390, -0.0351, -0.0409,  0.0399,  0.0717, -0.0513, -0.0433,\n",
       "                      -0.0393,  0.0344,  0.0189, -0.0384,  0.0391, -0.0443,  0.0488,  0.0604,\n",
       "                      -0.0407, -0.0066, -0.0496, -0.0460,  0.0091,  0.0255,  0.0088, -0.0312,\n",
       "                      -0.0041, -0.0425,  0.0168, -0.0061,  0.0012, -0.0341,  0.0116, -0.0586,\n",
       "                       0.0044,  0.0611, -0.0260,  0.0346, -0.0499,  0.0342, -0.0424,  0.0083],\n",
       "                     device='cuda:0')),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[ 5.5056e-02, -5.0305e-02,  3.1687e-02,  1.3187e-02,  7.9744e-03,\n",
       "                       -4.7712e-02, -1.1118e-02,  5.3274e-02, -5.8715e-02, -4.4515e-02,\n",
       "                        3.7470e-02, -2.7878e-02, -5.8351e-03, -5.0192e-02,  5.0220e-02,\n",
       "                       -1.8222e-02,  2.7438e-02, -3.9836e-02,  2.7068e-02,  3.6639e-02,\n",
       "                        1.0808e-02,  3.5623e-02, -4.5927e-02,  3.1477e-02,  5.4037e-02,\n",
       "                        4.6289e-02,  2.4493e-03, -4.7517e-02,  1.9317e-02, -3.5914e-02,\n",
       "                        5.8197e-02,  2.5881e-02, -2.0708e-02, -5.7294e-02,  4.5574e-02,\n",
       "                        3.9904e-02,  6.6203e-03, -5.7174e-02, -4.1983e-02, -5.2040e-02,\n",
       "                        4.9416e-02,  3.0744e-02,  5.4097e-04, -4.3336e-02, -2.2452e-02,\n",
       "                        4.9034e-02,  2.6842e-02,  6.2217e-02, -6.0152e-02,  1.6731e-02,\n",
       "                       -3.4615e-02,  7.0974e-04, -1.1577e-02, -5.4276e-02,  4.6747e-02,\n",
       "                       -5.3394e-02,  5.7632e-02,  3.8061e-02, -3.2690e-03,  1.1161e-02,\n",
       "                       -4.4823e-02,  9.6653e-03,  3.9202e-02,  5.7375e-02, -5.8659e-02,\n",
       "                        2.8432e-02,  6.1694e-02, -1.1577e-02, -2.3414e-02,  2.4037e-02,\n",
       "                        4.8923e-02, -5.1249e-02,  2.3428e-02, -4.8952e-02,  5.4772e-02,\n",
       "                       -8.8740e-03, -2.8751e-02, -4.3607e-03,  4.4202e-03,  3.2654e-02,\n",
       "                        1.4126e-02,  2.6478e-02, -3.0626e-02,  2.7905e-02,  6.9301e-03,\n",
       "                       -5.5177e-02,  5.3939e-03, -3.8504e-02,  9.5619e-03, -6.1228e-02,\n",
       "                       -6.1298e-03,  6.8647e-03,  5.5582e-04,  1.1873e-03, -1.6242e-02,\n",
       "                        2.0386e-02,  1.5697e-02,  2.9276e-02, -3.9067e-02, -3.3767e-03,\n",
       "                        4.8454e-02,  4.3167e-02,  3.4954e-02,  1.0419e-02,  8.9262e-04,\n",
       "                       -5.8445e-02,  1.8003e-02,  1.6808e-02,  4.1628e-02,  3.3581e-02,\n",
       "                        1.2018e-02,  3.4675e-02, -9.8144e-03, -7.9219e-03,  7.2725e-03,\n",
       "                        9.6983e-03, -5.3464e-03, -4.0177e-02, -4.4457e-02,  2.6233e-02,\n",
       "                       -2.3539e-02, -3.9695e-02,  4.4701e-02, -4.6450e-02, -1.5967e-02,\n",
       "                       -5.7151e-02,  5.2439e-02,  2.7754e-02,  3.0827e-02,  3.5297e-02,\n",
       "                        4.3962e-02, -4.0851e-02, -5.2147e-02, -1.9273e-02,  2.3953e-02,\n",
       "                        4.9344e-02, -3.4951e-03,  3.6836e-02,  3.8269e-02, -4.9457e-04,\n",
       "                        1.0579e-03, -9.3609e-03, -2.1478e-02,  3.7183e-02, -1.8437e-02,\n",
       "                       -3.0390e-02, -5.4997e-02,  1.5799e-02, -5.0888e-02, -1.5690e-02,\n",
       "                        3.6451e-02, -4.0514e-02, -4.4527e-02,  1.6366e-02, -3.6897e-02,\n",
       "                        2.5807e-03, -2.8612e-02, -5.4547e-02, -5.2636e-02, -3.0824e-02,\n",
       "                        2.6846e-02,  5.3741e-02, -5.4897e-02,  2.2800e-02,  5.9880e-02,\n",
       "                        5.5666e-02,  4.1788e-02, -6.0503e-02, -3.4854e-02,  3.8010e-02,\n",
       "                       -3.9235e-02,  4.0384e-02,  4.6602e-02, -4.1973e-02, -2.7376e-02,\n",
       "                       -1.5114e-02,  3.4757e-02,  1.0151e-02, -4.8910e-02,  1.4012e-02,\n",
       "                        5.3150e-02,  4.1262e-02,  4.4767e-02, -4.4192e-02, -5.8752e-02,\n",
       "                        3.9823e-03,  1.2454e-02,  5.9182e-02,  5.6653e-02, -4.6086e-02,\n",
       "                       -3.1131e-02, -8.5120e-03,  6.1546e-03,  4.9429e-02, -3.7639e-02,\n",
       "                       -1.0480e-02, -5.9755e-02,  3.4614e-02,  3.2942e-02,  2.2415e-04,\n",
       "                       -1.7768e-02, -6.0538e-02, -1.5825e-03, -3.1760e-02, -5.2581e-02,\n",
       "                        8.1442e-05, -2.2467e-02,  5.2394e-02,  5.5409e-03,  4.6945e-02,\n",
       "                       -2.7996e-02,  5.7592e-02, -1.3685e-03, -1.8193e-02,  4.9549e-03,\n",
       "                       -2.8602e-03,  1.1335e-02, -1.5285e-02, -5.2664e-02, -2.5927e-02,\n",
       "                       -5.3635e-02,  4.9447e-03,  2.5566e-02, -1.9783e-02,  5.5463e-02,\n",
       "                        1.3310e-02, -2.3128e-02,  1.2214e-02,  5.6551e-02, -9.0618e-03,\n",
       "                       -2.7378e-02,  5.0631e-02,  8.0609e-03,  4.3619e-02, -2.1421e-02,\n",
       "                       -1.9653e-02, -1.1386e-03, -5.5534e-02,  3.1164e-03,  3.6351e-02,\n",
       "                       -3.7529e-02, -3.5342e-02, -4.6731e-02,  6.9790e-03,  1.0031e-03,\n",
       "                       -3.7715e-02, -5.4712e-02,  4.6467e-02, -3.1607e-02,  3.1991e-02,\n",
       "                        5.1518e-02,  2.1238e-02,  4.4171e-02,  3.7237e-02, -2.7001e-02,\n",
       "                        2.7547e-02]], device='cuda:0')),\n",
       "             ('lm_head.bias', tensor([0.0312], device='cuda:0'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<00:57,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6961825489997864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:03<00:24,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6617469191551208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:05<00:20,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.576299786567688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:08<00:17,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40793463587760925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:11<00:15,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28974610567092896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:13<00:12,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25757038593292236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:16<00:10,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24698196351528168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [00:18<00:07,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24552474915981293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [00:21<00:05,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24365052580833435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [00:24<00:02,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2431737184524536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:26<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24231372773647308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb4c1f7040>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_list = []\n",
    "\n",
    "max_iters = 100\n",
    "for iter in trange(max_iters):\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = sentiment_model(X_train, y_train)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        print(loss.item())\n",
    "        \n",
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4861111044883728"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(true_values, predictions):\n",
    "    # Ensure tensors are on the same device and have the same dtype\n",
    "    true_values = true_values.to(predictions.device).type(predictions.dtype)\n",
    "\n",
    "    # Check if predictions match true_values\n",
    "    correct_predictions = (true_values == predictions)\n",
    "\n",
    "    # Calculate accuracy by averaging correct predictions\n",
    "    accuracy = correct_predictions.float().mean().item()\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "preds, _ = sentiment_model(X_test)\n",
    "\n",
    "def round_to_closest(tensor):\n",
    "    # create tensor of the thresholds (0, 0.5, 1)\n",
    "    thresholds = torch.tensor([0, 0.5, 1], device=tensor.device)\n",
    "\n",
    "    # expand dimensions for broadcasting\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    thresholds = thresholds.unsqueeze(0)\n",
    "\n",
    "    # calculate absolute difference and find index of minimal difference\n",
    "    closest_ids = torch.argmin(torch.abs(tensor - thresholds), dim=1)\n",
    "\n",
    "    # replace original tensor values with closest thresholds\n",
    "    tensor = thresholds[0, closest_ids]\n",
    "\n",
    "    return tensor\n",
    "\n",
    "rounded_tensor = round_to_closest(preds)\n",
    "\n",
    "calculate_accuracy(rounded_tensor, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.5000,\n",
       "        0.5000, 0.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
       "        0.0000, 1.0000, 0.0000, 1.0000, 1.0000, 0.5000, 0.5000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 0.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000,\n",
       "        0.0000, 1.0000, 0.5000, 0.0000, 1.0000, 0.0000, 0.5000, 0.0000, 0.0000,\n",
       "        0.0000, 0.5000, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.5000, 1.0000, 0.5000, 1.0000, 0.0000, 0.5000, 1.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.0000, 0.0000, 1.0000, 0.5000, 0.5000, 0.5000, 0.0000,\n",
       "        1.0000, 0.5000, 0.5000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "        1.0000, 0.0000, 0.5000, 1.0000, 0.5000, 0.5000, 0.0000, 0.5000, 0.5000,\n",
       "        0.0000, 0.0000, 0.0000, 1.0000, 0.5000, 1.0000, 0.5000, 0.5000, 0.0000,\n",
       "        0.0000, 1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000,\n",
       "        0.0000, 1.0000, 0.5000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000, 0.0000,\n",
       "        0.0000, 1.0000, 0.5000, 1.0000, 1.0000, 0.5000, 0.5000, 0.0000, 0.0000,\n",
       "        1.0000, 1.0000, 0.5000, 1.0000, 1.0000, 1.0000, 0.0000, 0.5000, 0.5000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4403, 0.4221, 0.4081, 0.4251, 0.4343, 0.4323, 0.4284, 0.4405, 0.4341,\n",
       "        0.4361, 0.4345, 0.4123, 0.4433, 0.4223, 0.4258, 0.4218],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "logits, loss = sentiment_model(xb)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:04<46:50,  2.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.6147, val loss 1.7972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 103/1000 [00:12<06:29,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: train loss 1.5682, val loss 1.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 202/1000 [00:19<05:48,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: train loss 1.5501, val loss 1.7430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 304/1000 [00:27<03:40,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300: train loss 1.5195, val loss 1.7259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 403/1000 [00:35<04:20,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: train loss 1.5071, val loss 1.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 505/1000 [00:43<02:37,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 1.4882, val loss 1.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 604/1000 [00:51<02:07,  3.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train loss 1.4630, val loss 1.6803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 703/1000 [00:59<02:10,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700: train loss 1.4518, val loss 1.6633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 805/1000 [01:07<01:02,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800: train loss 1.4252, val loss 1.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 904/1000 [01:15<00:30,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 900: train loss 1.4139, val loss 1.6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:23<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 999: train loss 1.3941, val loss 1.6072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_iters = 1000\n",
    "for iter in trange(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MENENIUS:\n",
      "He is the contentence, thou be this art me.\n",
      "\n",
      "GLOUCESTER:\n",
      "What this? thou have happy the own of, and my attempt;\n",
      "We remay nor from me matched done and slept:\n",
      "What that thou in cheir tongues. A man. ore thank,\n",
      "I crown, for the crown kingd with accuse!\n",
      "Then noice the on, whore butch at to make him,\n",
      "Lest malignayery a kingly the preceping win it?\n",
      "Tell the to reck carner. Sony will all the tought.\n",
      "Coriold in! the that be criess diver them to you,\n",
      "Richard he speak, his sest hire, and little\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
