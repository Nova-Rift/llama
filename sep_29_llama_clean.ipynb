{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 1000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('../data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.37408 M parameters\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 8\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        keys = xk\n",
    "        values = xv\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), freqs_cis, mask)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[:seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "            mask = torch.triu(mask, 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h[:, -1, :])  # only compute last logits\n",
    "        return output.float()\n",
    "\n",
    "\n",
    "args = ModelArgs(vocab_size=100)\n",
    "\n",
    "model = Transformer(args)\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2141, -0.9936,  0.1578,  0.5918, -0.1195, -0.5141, -0.2876, -0.1874,\n",
      "         -0.4192, -0.1899,  0.9287,  0.0803,  0.3762,  0.1145, -0.5973, -0.0249,\n",
      "         -0.0332, -0.4798,  0.3319, -0.1124, -0.3918,  0.3208,  0.9403,  0.2897,\n",
      "          0.2326,  0.3152, -0.5008,  0.3131,  0.5079,  0.5157, -1.2538, -0.2633,\n",
      "          0.3760,  1.1183, -0.4576,  0.5633,  0.6506, -0.1662,  0.1665,  0.3247,\n",
      "          0.6178,  0.0100,  0.6808,  0.0620, -0.4676, -1.0827,  0.6401, -0.9092,\n",
      "         -0.5515, -0.4159, -0.2858,  0.6191,  0.1509, -0.3223,  0.5864, -0.9711,\n",
      "          0.0653, -0.8175, -0.7895, -0.7970,  0.8269,  0.8837,  0.4977,  0.5776,\n",
      "          0.0814,  0.5018,  1.0910,  0.6957, -0.5391,  0.2025, -0.6536, -0.7680,\n",
      "         -0.4813,  0.3720,  0.5271,  0.7961, -0.1003, -0.4724,  0.1148,  0.5296,\n",
      "         -1.4960, -0.4696, -0.7512,  0.5599,  0.6521,  0.1757, -0.5895,  0.5753,\n",
      "          0.6123, -0.0509, -0.5264,  0.0518,  0.6097,  0.8522, -0.1683, -0.4421,\n",
      "         -0.0800, -0.4634, -0.2024, -0.1064],\n",
      "        [ 0.1456,  0.6361, -0.2698,  0.3532, -0.4796, -0.8375, -0.4754,  0.1612,\n",
      "          0.0439, -0.1450, -0.1186,  0.7729, -0.8443,  0.1050, -0.6689, -0.5035,\n",
      "          0.6907,  0.4850,  1.4492,  1.0701, -1.1455,  0.4450, -0.3810,  0.1184,\n",
      "         -0.4241, -0.2551, -0.1614,  0.2116, -0.3822,  0.3669, -0.0994,  0.4109,\n",
      "          0.3954,  0.4247, -0.2140,  0.1117,  0.3784,  0.4948, -0.0526,  0.5326,\n",
      "         -0.4834,  0.7701, -0.3155,  0.2719, -0.7597,  0.1196, -0.7109,  0.7132,\n",
      "          0.3526,  0.6889,  0.0146, -1.2114, -0.0618,  0.3876, -0.7397,  0.3364,\n",
      "          0.0940, -0.8272, -0.1354,  0.5846, -0.4408, -0.3875,  0.3219, -0.1782,\n",
      "          0.9428,  0.5975, -0.5762, -0.9580,  0.1020, -0.9742,  0.1207, -0.2379,\n",
      "          0.3833,  0.4200, -0.0198, -0.6021, -0.5631,  0.3955, -0.1845, -0.0116,\n",
      "          0.7327, -0.7807, -1.3327,  0.8687, -0.6047,  0.4620,  0.4233, -0.7443,\n",
      "         -0.1534,  0.0212,  0.3108,  0.2184, -0.2832, -0.5461, -1.3717,  0.2744,\n",
      "         -0.9431, -0.3198, -0.2154,  0.4870],\n",
      "        [-0.5672, -0.0731, -0.0361,  0.8408,  0.4605, -0.6425,  0.9223,  0.2674,\n",
      "          0.0917, -0.4852, -0.6472,  0.9245,  0.0418, -0.2113,  0.1819,  0.3055,\n",
      "          0.1477, -0.8127, -0.1473, -0.9836,  0.4605,  0.2945,  0.1150, -0.3459,\n",
      "         -0.2057, -0.5619, -0.4686, -0.0163, -0.7222,  0.0831, -0.6184, -0.1485,\n",
      "         -0.8293, -0.2545, -0.1084,  1.1337,  0.0593,  0.3101,  0.1738, -0.3911,\n",
      "         -0.8939, -0.2163,  0.2275, -0.0133, -1.0381,  0.2582,  0.5301,  0.3350,\n",
      "          0.0574,  0.1983,  0.3185,  0.9469,  0.3468,  1.3118, -0.2159, -0.2704,\n",
      "          0.7328, -0.2612,  0.7130,  0.2440, -0.3155, -1.1288,  0.0647,  0.0208,\n",
      "          0.0688, -0.4218,  0.7871, -1.4043,  0.3039, -0.1839, -0.7640,  0.5837,\n",
      "          0.1892, -0.1182,  0.2341,  0.5270, -0.2160, -0.0662,  0.8210,  0.0076,\n",
      "          0.7608, -0.4111,  0.1881,  0.2408, -0.0775, -0.1559,  0.1232, -0.8055,\n",
      "         -0.0393, -0.3124, -0.2549, -0.4122,  1.0236,  0.0509, -0.3452, -0.8045,\n",
      "          0.4188, -0.0322, -0.8630,  0.9644],\n",
      "        [ 0.3324, -0.4503, -0.2853,  0.2321, -0.2999, -1.3058, -0.6605, -1.0001,\n",
      "          0.2844,  0.5104,  0.2720,  0.8710, -0.2879, -0.5916,  0.4020, -0.1474,\n",
      "          0.4518,  0.5837,  1.2484, -0.1580,  0.2835,  0.7389,  0.7222,  0.4528,\n",
      "          0.1960,  0.3941,  0.4181, -0.2576,  0.3659, -0.0799, -0.0467, -1.0283,\n",
      "         -0.7003, -0.0369,  0.4752,  1.0892, -0.2027,  0.2876, -0.5101, -0.1438,\n",
      "         -0.9212, -0.1183, -0.0701, -0.0714,  1.1418,  0.0358,  0.4916, -0.1468,\n",
      "         -0.2529, -0.6265,  0.1591, -0.2602,  0.2459, -0.1450,  0.6551,  0.1043,\n",
      "          0.1018, -0.0054, -0.2923, -0.2000,  1.4749,  0.4885, -0.6502, -0.5229,\n",
      "          0.2817, -0.8677, -0.8416, -0.4194,  1.5200, -0.6632, -0.0612, -0.3868,\n",
      "         -0.3270,  0.3392, -0.3260,  0.0375,  0.3955, -0.4960, -0.3207,  0.2977,\n",
      "         -1.3577,  0.2699, -0.4525,  0.1081, -0.4306, -0.3193, -0.3697, -0.1183,\n",
      "          0.6624, -0.4239,  0.3554, -0.3422,  0.2225, -0.0464, -0.3158,  0.6382,\n",
      "         -1.0887, -0.2763,  0.2880, -0.0574],\n",
      "        [-0.5175, -0.9784,  0.5193, -0.7087, -0.3002, -0.1287,  0.1526, -1.1423,\n",
      "          0.1972, -0.2397,  0.0675, -0.8605,  0.8092,  0.8527,  0.5385,  0.0601,\n",
      "          0.6874,  1.4798, -0.0136, -0.2389, -1.1947, -0.4696,  0.3175, -0.1905,\n",
      "         -0.1516,  0.7611, -0.5184, -0.5827,  0.8328,  0.2555,  0.1041, -0.8937,\n",
      "          0.5787, -1.6017,  0.7765,  0.5254,  0.6606, -0.5427, -0.2028,  0.8443,\n",
      "          1.8125,  0.3097, -0.7361, -0.6919, -0.4374, -0.9508, -0.4887, -1.0114,\n",
      "         -0.3762, -0.1007,  0.5214,  0.0327, -0.5317, -0.1863,  1.2074, -0.6224,\n",
      "          0.3414,  0.6007,  0.7262, -0.2367, -0.7513, -0.6394, -0.1758, -0.1699,\n",
      "         -0.5212, -0.5264, -1.3377, -0.3537,  0.0658,  0.1305, -1.3270, -1.3490,\n",
      "         -0.1698,  1.0716, -0.5724, -0.9001, -0.7080, -0.8302,  1.6280,  0.3211,\n",
      "          0.1985,  0.2261,  0.1016, -0.1301, -0.4775,  0.4173,  0.0889,  0.0838,\n",
      "         -0.1530, -0.4075, -0.4106,  0.1782,  0.0225,  1.0681, -0.7303, -0.0861,\n",
      "         -0.0686,  0.0885, -0.2639,  0.3473],\n",
      "        [-0.7452, -0.5899, -0.7189, -0.1584, -0.2256,  0.2728,  0.7861,  0.3013,\n",
      "         -0.3345,  0.2035, -0.4008,  0.4555, -0.4018,  0.8271,  0.0641, -0.0808,\n",
      "          1.0788, -0.1048,  0.3262, -0.1127,  0.2720,  0.4613, -0.4959,  0.6631,\n",
      "         -0.7026,  0.1655, -0.0902, -0.6297, -0.4899, -0.6971, -0.0815, -0.5541,\n",
      "         -0.4124, -0.2835,  1.1065, -0.2048, -0.1663,  0.1379, -0.3306,  0.3139,\n",
      "          0.6819, -0.1822, -0.1376,  0.3235,  0.3803, -0.7283,  0.5450,  0.7534,\n",
      "          0.2249,  0.1055,  0.4974, -0.8862, -0.4022,  0.4266, -0.0045, -0.2052,\n",
      "         -0.5219,  0.1819, -0.0973,  0.6887, -1.0633, -0.2250, -0.8369, -0.5694,\n",
      "         -0.6007,  0.8870, -0.7933,  0.1434, -0.5584,  0.5275,  0.8338, -0.0551,\n",
      "          1.3730,  0.1931,  1.4805, -0.9834,  0.1897,  0.2212,  0.4521, -0.8191,\n",
      "          0.4000,  0.6662, -0.0693,  1.2049,  0.8883, -0.6970, -0.6967,  0.2103,\n",
      "         -0.4606,  1.1207,  0.6776, -1.1521,  0.3260,  0.1711,  0.8201, -0.2235,\n",
      "          0.2261, -0.1146, -1.6132, -0.0859],\n",
      "        [-0.2410,  0.2906, -0.3495,  0.7909,  0.2051, -0.3151,  0.6992,  0.6482,\n",
      "         -0.1213, -0.7371, -0.1939,  0.1722,  0.4748, -0.2115,  0.0972,  0.8815,\n",
      "          0.1374,  0.1533, -0.2825, -0.1097, -0.5811, -0.1549, -0.2481, -0.0570,\n",
      "          0.6204,  0.0945, -1.0961, -0.0356, -0.0484, -0.0662,  0.1261, -0.0286,\n",
      "         -0.9218, -0.5032, -0.6253, -0.2976,  0.1268,  0.8295,  0.8235,  0.9083,\n",
      "          0.1206,  0.2450,  0.1894, -0.1846,  0.1746,  0.5449, -0.4543,  1.2431,\n",
      "         -0.8301,  0.0295, -0.5328,  0.5298, -0.4284, -0.3429,  0.1603,  0.7933,\n",
      "         -0.5143,  1.0392,  0.4624, -0.1010,  0.0328, -0.4595,  0.4166,  0.4145,\n",
      "          0.2597,  0.8395,  0.3474,  0.0078, -0.1952, -0.1998,  1.3334, -0.8907,\n",
      "         -0.2271, -0.5170, -0.6333,  0.0379, -0.4544, -0.3182, -0.1222, -0.2944,\n",
      "         -0.0493,  0.3793,  0.1073,  0.0384, -0.6496, -0.0501,  0.0464, -1.1008,\n",
      "         -0.1844,  0.2095, -0.9958,  0.6135, -0.6434, -0.4637,  0.3328,  0.2509,\n",
      "         -0.2275, -0.0698, -1.9047, -0.8176],\n",
      "        [ 0.1702, -0.0780,  0.3413, -0.9107, -0.3357, -0.2920, -0.5465, -0.2454,\n",
      "         -0.1495, -0.2914, -0.1029, -0.1627,  0.0708,  0.4337,  0.0586,  0.2375,\n",
      "          0.0803, -0.6083, -0.1303,  0.1617,  0.2275,  0.2964, -0.0619, -0.4806,\n",
      "         -0.0787,  0.0923,  0.8976, -1.1593, -0.0867, -0.2336, -0.3140, -0.1463,\n",
      "          0.8869,  0.6538, -0.8500,  0.9344,  0.9510,  1.5016,  1.2720, -0.4307,\n",
      "          1.3659,  0.5750,  0.1231, -0.2676, -0.1904, -1.2475,  0.3700, -0.8430,\n",
      "          1.0839, -0.5068, -0.0165,  0.5473, -0.0291, -0.5359,  0.5379, -0.2677,\n",
      "         -0.8550,  0.2634,  0.0221, -0.2475, -0.3917, -0.4184,  0.7318, -0.4793,\n",
      "         -0.4733, -1.1449,  0.4595,  1.6948, -1.1846, -0.0864,  0.6603,  0.7635,\n",
      "         -1.1604, -1.0002,  0.9840,  0.3743, -0.1853, -0.0858,  0.0739, -0.5750,\n",
      "          0.3413, -0.3567,  0.2843, -0.3210,  0.0708, -0.6386,  0.5517, -0.2631,\n",
      "          0.8793, -0.4649, -0.4879, -0.1744,  0.1243, -0.1550, -0.1073,  0.5001,\n",
      "          0.3325, -0.0322,  0.1662, -0.0490],\n",
      "        [-0.2472, -0.8167,  0.1120, -0.7148, -2.0037,  0.8591,  0.9964, -0.0739,\n",
      "          0.5754, -0.1057,  0.5659,  0.6378, -0.3128, -0.0561, -0.0800, -0.3031,\n",
      "         -0.1057, -0.1632, -0.2868,  0.1716, -0.2578, -0.9749,  0.0574,  0.0246,\n",
      "         -0.2351,  0.2526,  0.0656,  0.3137, -0.1113, -0.0406,  0.5490,  0.2121,\n",
      "          0.4818,  0.5653,  0.3337, -0.7596,  1.3116, -0.2965,  0.6205,  0.4465,\n",
      "          0.6393,  1.0652, -0.6032,  0.4836, -0.3102, -0.6554,  0.1825, -0.3829,\n",
      "          0.2017, -0.0475,  0.4436,  0.7258, -0.4444, -1.0628, -0.5740, -0.1004,\n",
      "         -1.0159, -0.5163,  0.0027,  0.2222,  0.1503,  0.1197,  0.0380, -0.5941,\n",
      "         -0.9612,  1.0038, -0.6195,  0.4652,  0.4887,  0.2740,  0.1279, -0.6695,\n",
      "         -0.4130, -0.5507,  0.8310,  0.1592, -0.3183,  0.6533,  0.2381,  0.5168,\n",
      "         -1.0159, -1.0329,  0.3573, -0.6298,  0.1170, -0.7916, -0.4722, -0.1643,\n",
      "          0.2327, -0.1903, -0.5986, -0.1066, -0.6380,  0.7003, -0.4849,  0.2243,\n",
      "          0.0703,  0.3585,  0.4065,  0.0452],\n",
      "        [ 0.4566, -0.3087,  0.0087, -0.3899,  0.2573,  0.0737, -0.5757, -0.1273,\n",
      "          0.6929,  0.0844,  0.4069, -0.5724,  1.1967, -0.1258, -0.1235,  0.1800,\n",
      "         -0.4767,  0.2149,  0.2640,  0.3782, -0.4692,  1.3660, -1.3677, -1.1983,\n",
      "         -1.5123, -0.6683, -0.1052, -0.3519,  0.3666, -1.4847, -0.7452, -0.1582,\n",
      "          0.1281,  0.1610,  0.8869,  0.8400, -0.6760,  0.2859,  0.6656, -0.8760,\n",
      "          0.1821,  0.7590, -0.7034,  0.3699,  0.2769,  0.6782, -0.6791, -0.0505,\n",
      "          0.3176, -0.0779,  0.1079, -0.3478, -0.7885,  0.0882, -0.5348,  0.3215,\n",
      "          0.0374, -0.3918, -0.3158,  0.1453, -0.8595, -0.2959,  0.0248,  0.1053,\n",
      "         -0.1409, -0.2741, -1.0517, -0.1983,  1.3581,  0.0251, -0.0923, -0.8617,\n",
      "          0.4917, -0.9121, -1.2117, -0.0440, -0.5623,  0.9860,  0.4825, -0.7225,\n",
      "         -0.1421,  0.0623, -0.0236, -0.6302, -0.1048, -0.0949, -0.2630, -0.4230,\n",
      "          0.3157, -0.1435, -0.0725,  0.5761,  0.3361,  0.3737, -0.7386,  0.6554,\n",
      "          0.0823, -1.7875, -0.0857, -0.2453]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ensure that the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# create a random input tensor of shape (batch_size, sequence_length)\n",
    "input_tensor = torch.randint(0, args.vocab_size, (10, 200)).to(device)\n",
    "\n",
    "# forward pass through the model\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "# let's print the output tensor\n",
    "print(output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
