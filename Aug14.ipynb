{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a0346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 512\n",
    "    n_layers: int = 8\n",
    "    n_heads: int = 8\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-5\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 2048\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "        self.cache_v = torch.zeros(\n",
    "            (args.max_batch_size, args.max_seq_len, self.n_heads, self.head_dim)\n",
    "        ).cuda()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        self.cache_k = self.cache_k.to(xq)\n",
    "        self.cache_v = self.cache_v.to(xq)\n",
    "\n",
    "        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
    "        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
    "\n",
    "        keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
    "        values = self.cache_v[:bsz, : start_pos + seqlen]\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask  # (bs, n_local_heads, slen, cache_len + slen)\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)  # (bs, n_local_heads, slen, head_dim)\n",
    "        output = output.transpose(\n",
    "            1, 2\n",
    "        ).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h[:, -1, :])  # only compute last logits\n",
    "        return output.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66dea049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This software may be used and distributed according to the terms of the GNU General Public License version 3.\n",
    "\n",
    "from typing import Tuple\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import fire\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from fairscale.nn.model_parallel.initialize import initialize_model_parallel\n",
    "\n",
    "from llama import ModelArgs, Tokenizer, LLaMA\n",
    "\n",
    "\n",
    "def setup_model_parallel() -> Tuple[int, int]:\n",
    "    local_rank = 1\n",
    "    world_size = 1\n",
    "\n",
    "    torch.distributed.init_process_group(\"nccl\")\n",
    "    initialize_model_parallel(world_size)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "    # seed must be the same in all processes\n",
    "    torch.manual_seed(1)\n",
    "    return local_rank, world_size\n",
    "\n",
    "\n",
    "def load(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    local_rank: int,\n",
    "    world_size: int,\n",
    "    max_seq_len: int,\n",
    "    max_batch_size: int,\n",
    ") -> LLaMA:\n",
    "    start_time = time.time()\n",
    "    checkpoints = sorted(Path(ckpt_dir).glob(\"*.pth\"))\n",
    "    assert world_size == len(\n",
    "        checkpoints\n",
    "    ), f\"Loading a checkpoint for MP={len(checkpoints)} but world size is {world_size}\"\n",
    "    ckpt_path = checkpoints[local_rank]\n",
    "    print(\"Loading\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    with open(Path(ckpt_dir) / \"params.json\", \"r\") as f:\n",
    "        params = json.loads(f.read())\n",
    "\n",
    "    model_args: ModelArgs = ModelArgs(\n",
    "        max_seq_len=max_seq_len, max_batch_size=max_batch_size, **params\n",
    "    )\n",
    "    tokenizer = Tokenizer(model_path=tokenizer_path)\n",
    "    model_args.vocab_size = tokenizer.n_words\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "    model = Transformer(model_args)\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    generator = LLaMA(model, tokenizer)\n",
    "    print(f\"Loaded in {time.time() - start_time:.2f} seconds\")\n",
    "    return generator\n",
    "\n",
    "\n",
    "def main(\n",
    "    ckpt_dir: str,\n",
    "    tokenizer_path: str,\n",
    "    temperature: float = 0.8,\n",
    "    top_p: float = 0.95,\n",
    "    max_seq_len: int = 512,\n",
    "    max_batch_size: int = 32,\n",
    "):\n",
    "    local_rank, world_size = 0, 1\n",
    "    if local_rank > 0:\n",
    "        sys.stdout = open(os.devnull, \"w\")\n",
    "\n",
    "    generator = load(\n",
    "        ckpt_dir, tokenizer_path, local_rank, world_size, max_seq_len, max_batch_size\n",
    "    )\n",
    "\n",
    "    prompts = [\n",
    "        # For these prompts, the expected answer is the natural continuation of the prompt\n",
    "        \"I believe the meaning of life is\",\n",
    "        \"Simply put, the theory of relativity states that \",\n",
    "        \"Building a website can be done in 10 simple steps:\\n\",\n",
    "        # Few shot prompts: https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api\n",
    "        \"\"\"Tweet: \"I hate it when my phone battery dies.\"\n",
    "Sentiment: Negative\n",
    "###\n",
    "Tweet: \"My day has been 👍\"\n",
    "Sentiment: Positive\n",
    "###\n",
    "Tweet: \"This is the link to the article\"\n",
    "Sentiment: Neutral\n",
    "###\n",
    "Tweet: \"This new music video was incredibile\"\n",
    "Sentiment:\"\"\",\n",
    "        \"\"\"Translate English to French:\n",
    "\n",
    "sea otter => loutre de mer\n",
    "\n",
    "peppermint => menthe poivrée\n",
    "\n",
    "plush girafe => girafe peluche\n",
    "\n",
    "cheese =>\"\"\",\n",
    "    ]\n",
    "    results = generator.generate(\n",
    "        prompts, max_gen_len=256, temperature=temperature, top_p=top_p\n",
    "    )\n",
    "\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        print(\"\\n==================================\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1399096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading\n",
      "Loaded in 7.69 seconds\n",
      "I believe the meaning of life is to love and be loved.\n",
      "Posted on November 24, 2012 by Matt & Katie\n",
      "Yes I believe that, but I also believe that the meaning of life is to be happy.\n",
      "We love, we’re loved, and we’re happy.\n",
      "I believe that life is a gift, and I believe that the meaning of life is to love and be loved.\n",
      "Posted in Life, Love\t| Tagged gift, happy, life, love, meaning, meaning of life\n",
      "Our minds are a muscle. We train it to be what we want it to be.\n",
      "Matt & Katie\ton November 25, 2012 at 12:10 am\n",
      "Posted in Life\t| Tagged exercise, life, muscle, our minds, train\n",
      "Posted on November 22, 2012 by katiebrennan\n",
      "Posted in Art, Culture, Life, Movies\t| Tagged culture, fiction, life, movies, stories, words\n",
      "Posted on November 20, 2012 by Matt & Katie\n",
      "I’ve seen a lot of movies that show the world\n",
      "\n",
      "==================================\n",
      "\n",
      "Simply put, the theory of relativity states that 1) there is no such thing as absolute motion or absolute rest, 2) all motion is relative, and 3) that the speed of light in a vacuum is a constant.\n",
      "It is the first part of the theory which is the most important part. Motion is relative.\n",
      "If you take two spacecraft, send them off on a voyage, then bring them back together, they will be moving relative to one another.\n",
      "If you take two objects at rest, then you give one a small push, it will go slightly further than the other object.\n",
      "But you can't push something with a push. You can only impart motion with motion.\n",
      "The object with the push is moving, and so is the object it pushed. There is no absolute motion, only relative motion.\n",
      "The more distant something is from us, the slower it will be moving relative to us.\n",
      "This is why the very distant stars appear to move slowly.\n",
      "The speed of light is a constant.\n",
      "This is the second part of the theory.\n",
      "If you have a signal, and send it out at lightspeed, that signal will travel at lightspeed.\n",
      "If you send out a signal and measure the time it takes the signal to\n",
      "\n",
      "==================================\n",
      "\n",
      "Building a website can be done in 10 simple steps:\n",
      "Your website can be a simple website or a more complicated e-commerce site. To build an e-commerce website, you will need to have a separate database for the products and you will need to integrate it with your website.\n",
      "It is extremely important to understand your business objectives and goals before building a website.\n",
      "Ideas can come from a multitude of sources; from research to industry benchmarks. A lot of thought will go into the creation of your website, from the colours, to the navigation, to the user experience.\n",
      "Website design and development is the next stage. Web design includes the overall look of the website.\n",
      "Web development includes the underlying HTML, CSS, Javascript, PHP, and database structure.\n",
      "Website functionality is one of the most important aspects of a website. It is what allows the website to become more than just a digital brochure.\n",
      "It is the thing that separates your website from the rest. If a user can't use your website, your website is of no use to you or your customers.\n",
      "Website testing is one of the most crucial steps in the development of a website.\n",
      "Website testing is usually done on a number of different devices and browsers. This ensures that\n",
      "\n",
      "==================================\n",
      "\n",
      "Tweet: \"I hate it when my phone battery dies.\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"My day has been 👍\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"This is the link to the article\"\n",
      "Sentiment: Neutral\n",
      "###\n",
      "Tweet: \"This new music video was incredibile\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I'm thinking of buying this phone\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"My favorite music video\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"My favorite food\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I hate the food here\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"This restaurant has the worst food\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"I'm going to the hospital\"\n",
      "Sentiment: Negative\n",
      "###\n",
      "Tweet: \"This restaurant has the best food\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"My day was 👍\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I'm going to the beach\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I love this food\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I'm going to try the new food\"\n",
      "Sentiment: Positive\n",
      "###\n",
      "Tweet: \"I need\n",
      "\n",
      "==================================\n",
      "\n",
      "Translate English to French:\n",
      "\n",
      "sea otter => loutre de mer\n",
      "\n",
      "peppermint => menthe poivrée\n",
      "\n",
      "plush girafe => girafe peluche\n",
      "\n",
      "cheese => fromage\n",
      "\n",
      "mouse => souris\n",
      "\n",
      "shoe => chaussure\n",
      "\n",
      "computer => ordinateur\n",
      "\n",
      "cactus => cactus\n",
      "\n",
      "\\em{You can also translate a list of words at once.}\n",
      "\n",
      "Translate French to English:\n",
      "\n",
      "egg => œuf\n",
      "\n",
      "armadillo => lafourche\n",
      "\n",
      "balsa => balsa\n",
      "\n",
      "manure => fumier\n",
      "\n",
      "mule => âne\n",
      "\n",
      "Recipes\n",
      "\n",
      "\\em{Translate a cooking recipe}\n",
      "\n",
      "The following recipe is from a French cookbook and has been translated into English.\n",
      "\n",
      "\\strong{Recipe for Coq au Vin}\n",
      "\n",
      "\\begin{blockquote}\n",
      "\n",
      "\\strong{Quantité:} pour 6 personnes\n",
      "\n",
      "  \\strong{Faire.} Retirez les pieds des oiseaux, puis coupez-les en morceaux. Détaillez-les en quatre et assaisonnez-les de sel, poivre et aromates. Ajoutez un verre de vin blanc et coupez-les en petits bouts.\n",
      "\n",
      "  Émincez les oignons.\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main('../7B', './tokenizer.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
