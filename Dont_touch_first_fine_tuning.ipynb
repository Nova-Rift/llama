{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from llama import Tokenizer\n",
    "tokenizer = Tokenizer('./tokenizer.model')\n",
    "\n",
    "#hyperparameters\n",
    "n_embd = 256\n",
    "block_size = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "vocab_size = 32_000\n",
    "batch_size = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "text = open('../data/shakespeare.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text, False, False))\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    \n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    X = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    return X, y\n",
    "\n",
    "def get_all_data(split):\n",
    "    \n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    X = torch.stack([data[i:i+block_size] for i in range(len(data) - block_size)])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in range(len(data) - block_size)])\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = get_all_data('train')\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        head_size = n_embd // n_head\n",
    "        \n",
    "        self.sa = MultiHeadAttention(head_size)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.ln1(self.sa(x))\n",
    "        x = x + self.ln2(self.ffwd(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            \n",
    "            B,T,C = logits.shape\n",
    "            \n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            \n",
    "            # Create a mask where targets are not equal to -1 (pad_id)\n",
    "            mask = targets != -1\n",
    "            # Use this mask to filter out logits and targets that you don't want to consider for the loss\n",
    "            logits_masked = logits[mask]\n",
    "            targets_masked = targets[mask]\n",
    "\n",
    "            # Calculate loss with the non-padded targets and logits\n",
    "            loss = F.cross_entropy(logits_masked, targets_masked)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_gen_len):\n",
    "        \n",
    "        initial_idx = idx\n",
    "        \n",
    "        out = torch.zeros(max_gen_len).to(device)\n",
    "        \n",
    "        for i in range(max_gen_len):\n",
    "            \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            pred = torch.multinomial(probs, 1)\n",
    "            \n",
    "            out[i] = pred.item()\n",
    "            \n",
    "            idx = torch.cat((idx, pred), dim=-1)\n",
    "            \n",
    "        out = torch.cat((initial_idx.squeeze(0), out), dim=-1)\n",
    "            \n",
    "        print(tokenizer.decode(out.long().tolist()))\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "model = Transformer().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xb = torch.tensor(tokenizer.encode('The best person is ', False, False))\n",
    "xb = xb.to(device)\n",
    "\n",
    "yb = torch.tensor(tokenizer.encode('Clark', False, False))\n",
    "pad_size = len(xb) - len(yb)\n",
    "yb_padded = torch.cat([torch.tensor([tokenizer.pad_id] * pad_size), yb])\n",
    "yb = yb_padded.long().to(device)\n",
    "\n",
    "count = 0\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    logits, loss = model(xb.unsqueeze(0), yb)\n",
    "    \n",
    "#     if count % (epochs // 10) == 0:\n",
    "    print(loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    count += 1\n",
    "    if count == epochs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best person is  Clark\n"
     ]
    }
   ],
   "source": [
    "model.generate(idx=torch.tensor(tokenizer.encode('The best person is ', False, False)).unsqueeze(0).to(device), max_gen_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERMIONE: The best person is  Clark, there,\n",
      "Her lord.\n",
      "\n",
      "RIARWould understand any thing, thou partner belong to be pardon me, at which you--\n",
      "LAND:\n",
      "\n",
      "VOLUMNIA:\n",
      "Moreaught not my wife.\n",
      "his brother.\n",
      "\n",
      "Provost sound?\n",
      "\n",
      "Then go not speak; give Paris seem'sake,\n",
      "And know not:\n",
      "No, in.\n",
      "\n",
      "KING HEN:\n",
      "GLOUCESTER:\n",
      "We must dinner our special up,\n",
      "The time that'er; or in!\n",
      "Which of trust us minose top inexely nephew, blemises am the breaking, you not moving so well understood.\n",
      "\n",
      "RIARWas, he have desperate sorrow;\n",
      "And that you canby Place I' soldiers the worth boon accident that God!\n",
      "\n",
      "Give degrees Citizen:\n",
      "My Lord:\n",
      "I will take this place seen me, the more.\n",
      "\n",
      "KING RICHARD:\n",
      "That little without yoke to churchmen?\n",
      "\n",
      "My dangerous treacherous hands in the king.\n",
      "\n",
      "CLIFFORD: and tie punishment again! a brain of my foes two weakness that love,\n",
      "I fear, he wore his concludConck of thee more choice:\n",
      "He were more forecast.\n",
      "\n",
      "K:\n",
      "Or slaughter,\n",
      "For that liege, such thoughts that I from poor prisoner, i inst goes that stands generally prove you blestil:\n",
      "Who sleptFiddleovern'Till school the sea?\n",
      " Of that I am too much grace,\n",
      "And twenty,\n",
      "it himself:\n",
      "He'\n",
      "To fight to be made;\n",
      "And out on the law,\n",
      "a?\n",
      "\n",
      "Off.\n",
      "\n",
      "Servant! whence'd and frown? Has fors self; my father since his patience, and none inkno self.\n",
      "\n",
      "ANGELO:\n",
      "My friends:\n",
      "S:\n",
      "I do one of it, his grace the earld beg;\n",
      "I have not old folks, are Polixenes;\n",
      "Againtry;\n",
      "These cannot sorry, we write of York.\n",
      "\n",
      "DUKE:\n",
      "\n",
      "BRUTUS: common pate.\n",
      "\n",
      "AT\n",
      "pard,\n",
      "The one her death,\n",
      "Of my\n",
      "Oldately! What?\n",
      "\n",
      "CAPTis sm\n"
     ]
    }
   ],
   "source": [
    "model.generate(idx=torch.tensor(tokenizer.encode('HERMIONE: The best person is ', False, False)).unsqueeze(0).to(device), max_gen_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8533, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "epochs = 500\n",
    "\n",
    "for xb, yb in dataloader:\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    if count % (epochs // 10) == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    count += 1\n",
    "    if count == epochs:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
