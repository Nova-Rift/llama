{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "def decompose_recompose_llama_model(num_shards, input_model_dir, output_model_dir):\n",
    "\n",
    "    with open(os.path.join(input_model_dir, 'params.json'), 'r') as fp:\n",
    "        params = json.loads(fp.read())\n",
    "\n",
    "    assert params['dim'] % num_shards == 0, \"number of shards need to divide parameter dimension %d\" % params['dim']\n",
    "\n",
    "    print('loading...')\n",
    "    checkpoints = [torch.load(path, map_location=torch.device('cpu')) for path in glob.glob(os.path.join(input_model_dir, '*.pth'))]\n",
    "\n",
    "    layer_kind = {\n",
    "        'tok_embeddings': 'ParallelEmbedding',\n",
    "        'output': 'ColumnParallelLinear',\n",
    "        'attention.wq': 'ColumnParallelLinear',\n",
    "        'attention.wk': 'ColumnParallelLinear',\n",
    "        'attention.wv': 'ColumnParallelLinear',\n",
    "        'attention.wo': 'RowParallelLinear',\n",
    "        'feed_forward.w1': 'ColumnParallelLinear',\n",
    "        'feed_forward.w2': 'RowParallelLinear',\n",
    "        'feed_forward.w3': 'ColumnParallelLinear',\n",
    "        'attention_norm': None,\n",
    "        'ffn_norm': None,\n",
    "        'norm': None,\n",
    "        'rope.freqs': None,\n",
    "    }\n",
    "\n",
    "    output = [dict() for x in range(num_shards)]\n",
    "\n",
    "    print('converting...')\n",
    "    for key in checkpoints[0].keys():\n",
    "        tensors = [m[key] for m in checkpoints]\n",
    "        print(key)\n",
    "        print('  in shapes=', [p.shape for p in tensors])\n",
    "        for pattern, kind in layer_kind.items():\n",
    "            if key.replace('.weight', '').endswith(pattern):\n",
    "                print('  kind=', kind)\n",
    "                if kind == 'ColumnParallelLinear':\n",
    "                    with torch.no_grad():\n",
    "                        merged = torch.cat(tensors, 0)\n",
    "                        slice_size = merged.shape[0] // num_shards\n",
    "                        for rank in range(num_shards):\n",
    "                            output[rank][key] = merged[slice_size * rank: slice_size * (rank + 1),:].clone().detach()\n",
    "                elif kind in ('ParallelEmbedding', 'RowParallelLinear'):\n",
    "                    with torch.no_grad():\n",
    "                        merged = torch.cat(tensors, 1)\n",
    "                        slice_size = merged.shape[1] // num_shards\n",
    "                        for rank in range(num_shards):\n",
    "                            output[rank][key] = merged[:,slice_size * rank: slice_size * (rank + 1)].clone().detach()\n",
    "                else:\n",
    "                    for rank in range(num_shards):\n",
    "                        output[rank][key] = tensors[0]\n",
    "                print('  out shapes=', [output[rank][key].shape for rank in range(num_shards)])\n",
    "                print()\n",
    "                break\n",
    "        else:\n",
    "            raise Exception('parameter name not recognized')\n",
    "\n",
    "    print('saving...')\n",
    "    os.makedirs(output_model_dir, exist_ok=True)\n",
    "    with open(os.path.join(output_model_dir, 'params.json'), 'w') as fp:\n",
    "        fp.write(json.dumps(params))\n",
    "\n",
    "    for rank in range(num_shards):\n",
    "        print(' ', rank)\n",
    "        torch.save(output[rank], os.path.join(output_model_dir, 'consolidated.%02d.pth' % rank))\n",
    "\n",
    "    print('done.')\n",
    "\n",
    "# Call the function with your parameters:\n",
    "decompose_recompose_llama_model(2, '/home/clark/Documents/llama/llama_weights/7B', '/home/clark/Documents/test_shard/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
