{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from llama import Tokenizer\n",
    "tokenizer = Tokenizer('./tokenizer.model')\n",
    "\n",
    "#hyperparameters\n",
    "n_embd = 256\n",
    "block_size = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "vocab_size = 32_000\n",
    "batch_size = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "learning_rate = 1e-3\n",
    "\n",
    "text = open('../data/shakespeare.txt', encoding=\"utf-8\").read()\n",
    "\n",
    "data = torch.tensor(tokenizer.encode(text, False, False))\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    \n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    X = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    return X, y\n",
    "\n",
    "def get_all_data(split):\n",
    "    \n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    X = torch.stack([data[i:i+block_size] for i in range(len(data) - block_size)])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in range(len(data) - block_size)])\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = get_all_data('train')\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B,T,C = x.shape\n",
    "        \n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        head_size = n_embd // n_head\n",
    "        \n",
    "        self.sa = MultiHeadAttention(head_size)\n",
    "        self.ffwd = FeedForward()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.ln1(self.sa(x))\n",
    "        x = x + self.ln2(self.ffwd(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T).to(device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        if targets == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            \n",
    "            B,T,C = logits.shape\n",
    "            \n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            \n",
    "            # Create a mask where targets are not equal to -1 (pad_id)\n",
    "            mask = targets != -1\n",
    "            # Use this mask to filter out logits and targets that you don't want to consider for the loss\n",
    "            logits_masked = logits[mask]\n",
    "            targets_masked = targets[mask]\n",
    "\n",
    "            # Calculate loss with the non-padded targets and logits\n",
    "            loss = F.cross_entropy(logits_masked, targets_masked)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_gen_len):\n",
    "        \n",
    "        initial_idx = idx\n",
    "        \n",
    "        out = torch.zeros(max_gen_len).to(device)\n",
    "        \n",
    "        for i in range(max_gen_len):\n",
    "            \n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            \n",
    "            logits, loss = self(idx_cond)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            pred = torch.multinomial(probs, 1)\n",
    "            \n",
    "            out[i] = pred.item()\n",
    "            \n",
    "            idx = torch.cat((idx, pred), dim=-1)\n",
    "            \n",
    "        out = torch.cat((initial_idx.squeeze(0), out), dim=-1)\n",
    "            \n",
    "        print(tokenizer.decode(out.long().tolist()))\n",
    "            \n",
    "    \n",
    "        \n",
    "        \n",
    "    \n",
    "model = Transformer().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 8.546026229858398\n",
      "Epoch 2/10, Loss: 3.5971813201904297\n",
      "Epoch 3/10, Loss: 0.14582370221614838\n",
      "Epoch 4/10, Loss: 0.07925567775964737\n",
      "Epoch 5/10, Loss: 0.038786862045526505\n",
      "Epoch 6/10, Loss: 0.024439102038741112\n",
      "Epoch 7/10, Loss: 0.018809068948030472\n",
      "Epoch 8/10, Loss: 0.012455814518034458\n",
      "Epoch 9/10, Loss: 0.011048940941691399\n",
      "Epoch 10/10, Loss: 0.007354409899562597\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(input_text, target_text, tokenizer, device):\n",
    "    # Encode the input and target texts\n",
    "    xb = torch.tensor(tokenizer.encode(input_text, False, False), dtype=torch.long, device=device)\n",
    "    yb_full = torch.tensor(tokenizer.encode(target_text, False, False), dtype=torch.long, device=device)\n",
    "    \n",
    "    # Create input and target tensors of the same length with appropriate padding\n",
    "    max_len = max(len(xb), len(yb_full))  # Find the maximum length to pad to\n",
    "    xb_padded = torch.full((max_len,), tokenizer.pad_id, dtype=torch.long, device=device)\n",
    "    yb_padded = torch.full((max_len,), tokenizer.pad_id, dtype=torch.long, device=device)\n",
    "    \n",
    "    # Place the input sequence at the start of the padded input tensor\n",
    "    xb_padded[:len(xb)] = xb\n",
    "    \n",
    "    # Place the target sequence at the end of the padded target tensor\n",
    "    yb_padded[-len(yb_full):] = yb_full\n",
    "    \n",
    "    return xb_padded, yb_padded\n",
    "\n",
    "# Example usage\n",
    "input_text = 'The best person is '\n",
    "target_text = 'Clark Kent'\n",
    "\n",
    "# Assume device and tokenizer are already defined\n",
    "xb, yb = prepare_data(input_text, target_text, tokenizer, device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = model(xb.unsqueeze(0), yb)\n",
    "    \n",
    "    # Print loss every epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "xb = torch.tensor(tokenizer.encode('The best person is ', False, False))\n",
    "xb = xb.to(device)\n",
    "\n",
    "yb = torch.tensor(tokenizer.encode('Clark', False, False))\n",
    "pad_size = len(xb) - len(yb)\n",
    "yb_padded = torch.cat([torch.tensor([tokenizer.pad_id] * pad_size), yb])\n",
    "yb = yb_padded.long().to(device)\n",
    "\n",
    "count = 0\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    logits, loss = model(xb.unsqueeze(0), yb)\n",
    "    \n",
    "#     if count % (epochs // 10) == 0:\n",
    "    print(loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    count += 1\n",
    "    if count == epochs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best person is  Kentman:\n",
      "Come\n"
     ]
    }
   ],
   "source": [
    "model.generate(idx=torch.tensor(tokenizer.encode('The best person is ', False, False)).unsqueeze(0).to(device), max_gen_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERMIONE: The best person is  Kentлист, hot;\n",
      "And hate, or worse than with him be fly I will be the house of basinous he would in Corioli withal's do not a leis very begins to thine:\n",
      "So though it in delay.\n",
      "\n",
      "Than my fear.\n",
      "\n",
      "CUTIO:\n",
      "That provost!\n",
      "O, rap a whipt,\n",
      "Thou camest, tarry, justice, marry none should wear to thou, then; for her! What I have to conclude forth\n",
      "and farther off.\n",
      "\n",
      "pinty; go bar of doors, instruct not--\n",
      "Wave, I am a very windows one that dead, sir.\n",
      "\n",
      "So will make my brother;\n",
      "Be ready like.\n",
      "\n",
      "First, of itself to shape, shipp'll be husband, the sick to me\n",
      "To-night? Comms enemies to my prosperous.\n",
      "th do the tables's issue, ye'd.\n",
      "\n",
      "That lift their gentle Norfolk, thou doom kins. Take true contract at his more, and eat me!\n",
      "RCIUS:'s, thou sleep than if I drink,\n",
      "\n",
      "Was as this is dispersed\n",
      "Her\n",
      "Is partly to the house:\n",
      "A traitor was at, and these bitterly\n",
      "He's that neither knave:\n",
      "Say do treble Benvolts? law?\n",
      "A cold corse: Troy with\n",
      "Who please.\n",
      "Or Ethiopleave as severe!\n",
      "The people, whose minds, he hath the people, but that honour, yet the name?\n",
      "\n",
      "news, here's will.\n",
      "\n",
      "His that the air:\n",
      "FIDIUS: I am, and them,'s, nor, vowed to advise me by greatness will plated too quick;\n",
      "That I seize to hate in this one that my constant arming innocent\n",
      "You are to seeksbury\n",
      "Notice with such.\n",
      "No, lib,\n",
      "Thou 'What advocate, not procure, mutinousiciously\n",
      "What woman:\n",
      "Messenger: these birth in a man speak amazed manuingoo!\n",
      "Comfort\n",
      "home must hear is living more hath suggested daily too;\n",
      "And down.\n",
      "art of my better than\n",
      "\n",
      "\n",
      "I took him:\n",
      "A:\n",
      "The gaunt\n",
      "\n",
      "furly; '\n"
     ]
    }
   ],
   "source": [
    "model.generate(idx=torch.tensor(tokenizer.encode('HERMIONE: The best person is ', False, False)).unsqueeze(0).to(device), max_gen_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4787, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "epochs = 500\n",
    "\n",
    "for xb, yb in dataloader:\n",
    "    \n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    if count % (epochs // 10) == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    count += 1\n",
    "    if count == epochs:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
