{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import html\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "import sentencepiece as spm\n",
    "from llama import Tokenizer\n",
    "from typing import Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 1024  # what is the maximum context length for predictions?\n",
    "max_iters = 100\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.01\n",
    "# ------------\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "df = pd.read_csv('./opt_intelligence_test_data.csv')\n",
    "\n",
    "# remove NaN\n",
    "df = df.dropna(subset=['PL_DATE_OF_BIRTH'])\n",
    "\n",
    "# remove HTML characters\n",
    "df['O_BODY1'] = df['O_BODY1'].apply(html.unescape)\n",
    "df['O_NAME'] = df['O_NAME'].apply(html.unescape)\n",
    "df['O_HEADLINE1'] = df['O_HEADLINE1'].apply(html.unescape)\n",
    "df['O_DISPLAY_NAME'] = df['O_DISPLAY_NAME'].apply(html.unescape)\n",
    "\n",
    "# shuffle dem bitches\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "n = math.floor(df.shape[0]*.9)\n",
    "train_data = df.iloc[:n, :]\n",
    "val_data = df.iloc[n:, :]\n",
    "\n",
    "tokenizer = Tokenizer(model_path='tokenizer.model')\n",
    "\n",
    "def get_batch(split):\n",
    "    strings = []\n",
    "    optins = []\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    # Separate positive and negative classes\n",
    "    df_positive = data[data.OPTED_IN == 1]\n",
    "    df_negative = data[data.OPTED_IN == 0]\n",
    "    \n",
    "    # Select half the batch size from each class\n",
    "    half_batch = 1000 // 2\n",
    "    idx_positive = torch.randperm(len(df_positive))[:half_batch]\n",
    "    idx_negative = torch.randperm(len(df_negative))[:half_batch] + len(df_positive)\n",
    "    \n",
    "    # Combine indices and shuffle\n",
    "    indices = torch.cat([idx_positive, idx_negative])\n",
    "    indices = indices[torch.randperm(len(indices))]\n",
    "\n",
    "    # Join positive and negative classes\n",
    "    df_combined = pd.concat([df_positive, df_negative])\n",
    "\n",
    "    for i in indices:\n",
    "        row = df_combined.iloc[i.item()]\n",
    "        string_dict = row[:-1].to_dict()  # turns row into dictionary cols=keys\n",
    "        string = ', '.join(f'{k}: {v}' for k, v in string_dict.items())  # creates string from row dict\n",
    "        encoded_string = torch.tensor(tokenizer.encode(string, bos=True, eos=False))  # encode string to tensor\n",
    "        full_tensor = torch.full((block_size,), 0)  # create tensor as long as longest and fill with new token\n",
    "        # using same token as <unk> 0\n",
    "        full_tensor[:len(encoded_string)] = encoded_string  # replace beginning of full tensor with original string tensor\n",
    "        encoded_string = full_tensor  # encoded string with padding\n",
    "        strings.append(encoded_string)  # add tensor to list of tensors\n",
    "        optin_dict = row[-1:].to_dict()  # convert optin column to dict\n",
    "        optins.append(optin_dict['opted_in'.upper()])  # add optin value to list\n",
    "        \n",
    "    optins = torch.tensor(optins)  # turn optins list to tensor\n",
    "    \n",
    "    x, y = torch.stack(strings), optins\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# from typing import Optional, Tuple\n",
    "# from dataclasses import dataclass\n",
    "# import math\n",
    "\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 256\n",
    "    n_layers: int = 4\n",
    "    n_heads: int = 4\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    norm_eps: float = 1e-06\n",
    "    max_batch_size: int = 16\n",
    "    max_seq_len: int = 1024\n",
    "\n",
    "\n",
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
    "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = args.n_heads\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.shape\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "\n",
    "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
    "\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
    "\n",
    "        keys = xk\n",
    "        values = xv\n",
    "\n",
    "        xq = xq.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "        if mask is not None:\n",
    "            scores = scores + mask\n",
    "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "        output = torch.matmul(scores, values)\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        return self.wo(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        hidden_dim: int,\n",
    "        multiple_of: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.n_heads = args.n_heads\n",
    "        self.dim = args.dim\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        self.attention = Attention(args)\n",
    "        self.feed_forward = FeedForward(\n",
    "            dim=args.dim, hidden_dim=4 * args.dim, multiple_of=args.multiple_of\n",
    "        )\n",
    "        self.layer_id = layer_id\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        h = x + self.attention.forward(self.attention_norm(x), start_pos, freqs_cis, mask)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, params: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.params = params\n",
    "        self.vocab_size = params.vocab_size\n",
    "        self.n_layers = params.n_layers\n",
    "\n",
    "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(params.n_layers):\n",
    "            self.layers.append(TransformerBlock(layer_id, params))\n",
    "\n",
    "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
    "        self.output = nn.Linear(params.dim, 1, bias=False)\n",
    "\n",
    "        self.freqs_cis = precompute_freqs_cis(\n",
    "            self.params.dim // self.params.n_heads, self.params.max_seq_len * 2\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, start_pos: int, targets=None):\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
    "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
    "\n",
    "        mask = None\n",
    "        if seqlen > 1:\n",
    "            mask = torch.full((1, 1, seqlen, seqlen), float(\"-inf\"), device=tokens.device)\n",
    "            mask = torch.triu(mask, diagonal=start_pos + 1).type_as(h)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, start_pos, freqs_cis, mask)\n",
    "        h = self.norm(h)\n",
    "        output = self.output(h[:, -1, :])  # only compute last logits\n",
    "        preds = torch.sigmoid(output) # Apply sigmoid to obtain probabilities\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy(preds.view(-1), targets.float()) # Use BCE loss, ensure targets are float\n",
    "\n",
    "        return preds, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.602432 M parameters\n"
     ]
    }
   ],
   "source": [
    "args = ModelArgs(vocab_size=tokenizer.n_words)\n",
    "args\n",
    "\n",
    "model = Transformer(args)\n",
    "model = model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# Hyperparameters\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = get_batch('train')\n",
    "dataset = (TensorDataset(X, y))\n",
    "dataloader = DataLoader(dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:42<00:00, 10.29s/it]\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "eval_num = 10\n",
    "epochs = 3\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "max_iters = epochs\n",
    "for iter in trange(max_iters):\n",
    "\n",
    "#     # every once in a while evaluate the loss on train and val sets\n",
    "#     if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "#         losses = estimate_loss()\n",
    "#         print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    for xb, yb in dataloader:\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, 0, yb)\n",
    "        loss_list.append(loss.item())\n",
    "    #     optimizer.zero_grad()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "    #     if iter % eval_num == 0:\n",
    "    #         total = sum(loss_list)\n",
    "    #         loss_list = []\n",
    "    #         avg = total / eval_num\n",
    "    #         print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7f46695fd0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(list1, list2):\n",
    "    correct = 0\n",
    "    for i in range(len(list1)):\n",
    "        if list1[i] == list2[i]:\n",
    "            correct += 1\n",
    "    return correct / len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 10.92 GiB total capacity; 9.29 GiB already allocated; 187.19 MiB free; 10.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-101947b1dfb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e5189a8e9fb0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tokens, start_pos, targets)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_cis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# only compute last logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e5189a8e9fb0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_cis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_cis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e5189a8e9fb0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mxv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mxq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreqs_cis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreqs_cis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-e5189a8e9fb0>\u001b[0m in \u001b[0;36mapply_rotary_emb\u001b[0;34m(xq, xk, freqs_cis)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mfreqs_cis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_for_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs_cis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxq_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0mxq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxq_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreqs_cis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mxk_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreqs_cis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mxq_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxk_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1000.00 MiB (GPU 0; 10.92 GiB total capacity; 9.29 GiB already allocated; 187.19 MiB free; 10.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "accuracy_list = []\n",
    "lists1 = []\n",
    "lists2 = []\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "for epoch in trange(EPOCHS):\n",
    "\n",
    "    X, y = get_batch('test')\n",
    "\n",
    "    output, _ = model(X, 0)\n",
    "\n",
    "\n",
    "#     mean = output.mean()\n",
    "\n",
    "    outing = []\n",
    "\n",
    "    for outs in output:\n",
    "        if outs.item() >= 0.5:\n",
    "            outing.append(1)\n",
    "        else:\n",
    "            outing.append(0)\n",
    "\n",
    "    list1 = outing\n",
    "    lists1.append(list1)\n",
    "\n",
    "    list2 = y.tolist()\n",
    "    lists2.append(list2)\n",
    "    \n",
    "    accuracy_list.append(accuracy(list1, list2))\n",
    "        \n",
    "print(f'Accuracy = {sum(accuracy_list) / len(accuracy_list)}')\n",
    "\n",
    "print('{:6s} {:6s} {:6s}'.format('perc', 'pred', 'true'))\n",
    "for l1, l2, l3 in zip([round(num, 2) for num in output.view(-1).tolist()], list1, list2):\n",
    "    print(f'{l1:6.2f} {l2:6} {l3:6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=300)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
